[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Misc Resources",
    "section": "",
    "text": "I love to share my knowledge with others, so I try to give talks whenever possible. This also led me to create AI Learners, a non-profit Spanish-speaking community in which we offer talks and workshops on topics such as Machine Learning, Deep Learning, Natural Language Processing and Computer Vision for people of all levels of knowledge.\n\nCo-authored\nPapers (among others)\n\nZephyr: Direct Distillation of LM Alignment\nFederated benchmarking of medical artificial intelligence with MedPerf\nAfroDigits: A Community-Driven Spoken Digit Dataset for African Languages\nEvaluate & Evaluation on the Hub: Better Best Practices for Data and Model Measurement\n\nBlogs (not exhaustive)\n\nMixture of Experts Explained\nWelcome Mixtral - a SOTA Mixture of Experts on Hugging Face\nCode Llama: Llama 2 learns to code\nThe Falcon has landed in the Hugging Face ecosystem\nLlama 2 is here - get it on Hugging Face\n\n\n\nWorkshops\nIf you want to invite me to give a workshop/conference/talk, feel free to reach out to omar at huggingface.co. My aim is to do less talks and focus my energy in very key topics and high-quality content. Here are some of my previous talks (mix of English and Spanish):\n\n\n\nName\nDate\nAudience\nLocation\n\n\n\n\nML Demos (OS Startup Accelerator with Meta)\nApril, 2024\nMeta\nParis\n\n\nOpen Source ML\nMar, 2024\nT3chFest\nMadrid\n\n\nOpen Source ML - from pretrained models to production\nNov, 2023\nAI Learners\nMexico City\n\n\nIntroduction to transformers\nNov, 2023\nUniversities networks\nMexico City\n\n\nOpen Source ML - from pretrained models to production\nNov, 2023\nBBVA\nMexico City\n\n\nInterview with Platzi\nNov, 2023\nPlatzi\nOnline\n\n\nOpen Source ML - from pretrained models to production\nJune, 2023\nINTERFACE 2023\nOnline\n\n\nLLMs in Prod Conference Lightning talk\nJune, 2023\nLLMs in Prod Conference\n\n\n\nColloquium about NLP trends\nApril, 2023\nUZH\nZurich\n\n\nCollaborative Machine Learning for the future of AI\nFeb, 2023\nColumbia University\nNYC\n\n\nCollaborative Machine Learning for the future of AI\nFeb, 2023\nIBM\nNY\n\n\nMachine Learning Open Source\nFeb, 2023\nITAM University\nOnline\n\n\nHands-on Workshop with Gradio\nSept, 2022\nRIIAA\nOnline\n\n\nCraft amazing Machine Learning demos\nSept, 2022\nDataFest Armenia\nYerevan\n\n\nHow to craft awesome Machine Learning Demos with Python\nAug, 2022\nGoogle ML Bootcamp\nOnline\n\n\nHow to craft awesome Machine Learning Demos with Python\nJuly, 2022\nEuroPython\nDublin\n\n\nStories of Open Source Machine Learning\nJune, 2022\nBAAI\nOnline\n\n\nanel on Multilinguality in multimodal research and open data\nApril, 2022\nWiki-M3L at ICLR\nOnline\n\n\nOpen data Keynote\nApril, 2022\nWiki-M3L at ICLR\nOnline\n\n\nOpen Machine Learning\nApril, 2022\nQCon\nLondon\n\n\nands on Workshop with Hugging Face\nMarch 2022\nUniversity of Groningen\nOnline\n\n\nands on Workshop with Hugging Face\nMarch 2022\nSorbonne Universiy\nOnline\n\n\nands on Workshop with Hugging Face\nFeb, 2022\nAI.Slovakia Radio\nOnline\n\n\nands on Workshop with Hugging Face\nFeb 2022\nIITG.ia\nOnline\n\n\nHow to build ML in an open and collaborative way\nFeb, 2022\nAI Sweden NLP Seminar Series\nOnline\n\n\nBuilding Machine Learning demos with Python\nOct, 2021\nPyCon Chile\nOnline\n\n\nBuilding Machine Learning demos with Python\nOct, 2021\nPyCon Argentina\nOnline\n\n\nBuilding Machine Learning demos with Python\nOct, 2021\nPyCon Sweden\nOnline\n\n\nBuilding Machine Learning demos with Python\nOct, 2021\nPyconZa\nOnline\n\n\nNLP from Zero to Hero, Building NLP demos with Spaces\nOct, 2021\nSpainAI\nOnline\n\n\nPodcast: Democratizing AI\nSept, 2021\nSaturdaysAI\nOnline\n\n\nPodcast: In the frontier of the development of AI\nSept, 2021\nLATAM Minds\nOnline\n\n\nNLP from Zero to Hero, Word Embeddings\nJuly, 2021\nSomos NLP\nOnline\n\n\nContributing to the Open Source ML Ecosystem\nJuly, 2021\nTalent Land Mexico\nOnline\n\n\nOpen Source Ecosystem of ML and NLP\nJune, 2021\nData Science Research Peru\nOnline\n\n\nDifferentiable 3D Graphics\nAug, 2020\nRIIAA\nOnline\n\n\nConversation about AI\nJuly, 2020\nELIA\nOnline\n\n\nML in production two days workshop\nOct, 2019\nEOI Business School\nMadrid\n\n\nIntroduction to Machine Learning and the regulation efforts\nApril, 2019\nUN Women Colombia\nBogota\n\n\nEnd to end Machine Learning\nMarch, 2019\nData Science Research Peru\nLima\n\n\nWorld Models\nMarch, 2019\nData Science Research Peru\nLima\n\n\nIntroduction to TensorFlow\nMarch, 2019\nAI Learners\nOnline\n\n\nGenerative Adversarial Networks\n2018\nAI Learners\nOnline\n\n\nNeural Networks\n2018\nAI Learners\nMexico City\n\n\nIntroduction to Deep Learning\n2018\nMVS Radio\nMexico City\n\n\nDeep Learning in a real setting\n2018\nMVS Radio\nMexico City\n\n\nWorld Models, can a program learn from its dreams?\n2018\nAI Learners\nMexico City\n\n\nIntroduction to Deep Learning and Neural Networks\n2018\nAI Learners\nMexico City\n\n\nIntroduction to Deep Learning\n2018\nHackRobots\nMexico City\n\n\n\n\n\nSuccesses brag-list\n\nSpontaneously proposing and organizing a ‚Äúsmall meetup‚Äù in Paris during ICCV that ended up with over 2000 people (Yann approves)\nVery involved in the release of Falcon 180, Llama 2, Code Llama, NASA x IBM, Mixtral, and XTTS, among others.\nEnabled members from different teams to do impactful and exciting things, such as launching a course with Andrew Ng.\nLaunching the HF Discord Server which has nicely grown over the years (you can join at [hf.co/join/discord(hf.co/join/discord)])\nCollaborated with the Google Colab team for a couple of features, such as notebooks on HF!\nGoing from 20k repos to over a million repositories on Hugging Face. Repos go brrr üî•\nLaunch of Spaces Docker templates (collaborated with Docker, Argilla, Elixir, LabelStudio, AimStack, Shiny, ZenML, Anaconda, and Giskard).\n\n\n\nCourses taught (very old)\nI should update these one day\n\nMachine Learning for economists (Mexico City, 2018)\n\nTaught a small group of economists Machine Learning basics and how to apply it for practical economy problem.\nTopics: Machine Learning, linear regression, classification, regularization, how to evaluate learning algorithms, deep learning, clustering, reinforcement learning, and recurrent neural networks.\n\nPractical Deep Learning (Mexico City, 2018)\n\nGave a three months course at the ITESM (Tecnol√≥gico de Monterrey) to CS students about practical Deep Learning.\nTopics: Introduction to Deep Learning, backpropagation, convolutional neural networks, recurrent neural networks, LSTMs, embeddings and word2vec, Reinforcement Learning, world models, and an introduction to TensorFlow.\n\nImmersive Web Development (Mexico City, 2017)\n\nThroughout two semesters, I taught an introduction to web development course to CS students.\nTopics: HTML5 & CSS3, designing their own framework, bootstrap and polymer, JavaScript, JQuery, APIs, Node, Selenium, and front end optimization."
  },
  {
    "objectID": "blog/posts/sentence_embeddings2/index.html",
    "href": "blog/posts/sentence_embeddings2/index.html",
    "title": "Sentence Embeddings. Cross-encoders and Re-ranking",
    "section": "",
    "text": "This series aims to demystify embeddings and show you how to use them in your projects. The first blog post taught you how to use and scale up open-source embedding models, pick an existing model, current evaluation methods, and the state of the ecosystem. This second blog post will dive deeper into embeddings and explain the differences between bi-encoders and cross-encoders. Then, we‚Äôll dive into retrieving and re-ranking: we‚Äôll build a tool to answer questions about 400 AI papers. We‚Äôll briefly discuss about two different papers at the end. Enjoy!\nYou can either read the content here or execute it in Google Colab by clicking the badge at the top of the page. Let‚Äôs dive into embeddings!"
  },
  {
    "objectID": "blog/posts/sentence_embeddings2/index.html#tldr",
    "href": "blog/posts/sentence_embeddings2/index.html#tldr",
    "title": "Sentence Embeddings. Cross-encoders and Re-ranking",
    "section": "TL;DR",
    "text": "TL;DR\nSentence Transformers supports two types of models: Bi-encoders and Cross-encoders. Bi-encoders are faster and more scalable, but cross-encoders are more accurate. Although both tackle similar high-level tasks, when to use one versus the other is quite different. Bi-encoders are better for search, and cross-encoders are better for classification and high-accuracy ranking. Let‚Äôs dive into the details!"
  },
  {
    "objectID": "blog/posts/sentence_embeddings2/index.html#intro",
    "href": "blog/posts/sentence_embeddings2/index.html#intro",
    "title": "Sentence Embeddings. Cross-encoders and Re-ranking",
    "section": "Intro",
    "text": "Intro\nAll the models we saw in the previous blog post were bi-encoders. Bi-encoders are models that encode the input text into a fixed-length vector. When you compute the similarity between two sentences, we usually encode the two sentences into two vectors and then compute the similarity between the two vectors (e.g., by using cosine similarity). We train bi-encoders to optimize the increase in the similarity between the query and relevant sentences and decrease the similarity between the query and the other sentences. This is why bi-encoders are better suited for search. As the previous blog post showed, bi-encoders are fast and easily scalable. If multiple sentences are provided, the bi-encoder will encode each sentence independently. This means that the sentence embeddings are independent of each other. This is a good thing for search, as we can encode millions of sentences in parallel. However, this also means that the bi-encoder doesn‚Äôt know anything about the relationship between the sentences.\nWhen we use cross-encoders, we do something different. Cross-encoders encode the two sentences simultaneously and then output a classification score. The figure below shows the high-level differences\n\nWhy would you use one versus the other? Cross-encoders are slower and more memory intensive but also much more accurate. A cross-encoder is an excellent choice to compare a few dozen sentences. If you want to compare hundreds of thousands of sentences, a bi-encoder is a better choice, as otherwise a cross-encoder could take multiple hours. What if you care about accuracy and want to compare thousands of sentences efficiently? This is a typical case when you want to retrieve information. In those cases, an option is first to use a bi-encoder to reduce the number of candidates (i.e., get the top 20 most relevant examples) and then use a cross-encoder to get the final result. This is called re-ranking and is a common technique in information retrieval; we‚Äôll learn more about it later in this blog post!\nGiven that the cross-encoder is more accurate, it‚Äôs also a good option for tasks where subtle differences matter, such as medical or legal documents where a slight difference in wording can change the sentence‚Äôs meaning."
  },
  {
    "objectID": "blog/posts/sentence_embeddings2/index.html#cross-encoders",
    "href": "blog/posts/sentence_embeddings2/index.html#cross-encoders",
    "title": "Sentence Embeddings. Cross-encoders and Re-ranking",
    "section": "Cross-encoders",
    "text": "Cross-encoders\nAs mentioned, cross-encoders encode two texts simultaneously and then output a classification label. The cross-encoder first generates a single embedding that captures representations and their relationships. Compared to bi-encoder-generated embeddings (which are independent of each other), cross-encoder embeddings are dependent on each other. This is why cross-encoders are better suited for classification, and their quality is higher: they can capture the relationship between the two sentences! On the flip side, cross-encoders are slow if you need to compare thousands of sentences since they need to encode all the sentence pairs.\nLet‚Äôs say you have four sentences, and you need to compare all the possible pairs:\n\nA bi-encoder would need to encode each sentence independently, so it would need to encode four sentences.\nA cross-encoder would need to encode all the possible pairs, so it would need to encode six sentences (AB, AC, AD, BC, BD, CD).\n\nLet‚Äôs scale this. Let‚Äôs say you have 100,000 sentences, and you need to compare all the possible pairs:\n\nA bi-encoder would encode 100,000 sentences.\nA cross-encoder would encode 4,999,950,000 pairs! (Using the combinations formula: n! / (r!(n-r)!), where n=100,000 and r=2). No wonder they don‚Äôt scale well!\n\nHence, it makes sense they are slower!\n\n\n\n\n\n\nNote\n\n\n\nAlthough cross-encoders have an intermediate embedding before the classification layer, it is not used for similarity search. This is because the cross-encoder is trained to optimize the classification loss, not the similarity loss. Hence, the embedding is specific to the classification task and not the similarity task.\n\n\nThey can be used for different tasks. For example, for passage retrieval (given a question and a passage, is the passage relevant to the question?). Let‚Äôs look at a quick code snippet with a small cross-encoder model trained for this:\n\n!pip install sentence_transformers datasets\n\n\nfrom sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder('cross-encoder/ms-marco-TinyBERT-L-2-v2', max_length=512)\nscores = model.predict([('How many people live in Berlin?', 'Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.'), \n                        ('How many people live in Berlin?', 'Berlin is well known for its museums.')])\nscores\n\narray([ 7.152365 , -6.2870445], dtype=float32)\n\n\nAnother use case, more similar to what we did with bi-encoders, is to use cross-encoders for semantic similarity. For example, given two sentences, are they semantically similar? Although this is the same task we solved with bi-encoders, remember that cross-encoders are more accurate but slower.\n\nmodel = CrossEncoder('cross-encoder/stsb-TinyBERT-L-4')\nscores = model.predict([(\"The weather today is beautiful\", \"It's raining!\"), \n                        (\"The weather today is beautiful\", \"Today is a sunny day\")])\nscores\n\narray([0.46552283, 0.6350213 ], dtype=float32)"
  },
  {
    "objectID": "blog/posts/sentence_embeddings2/index.html#retrieve-and-re-rank",
    "href": "blog/posts/sentence_embeddings2/index.html#retrieve-and-re-rank",
    "title": "Sentence Embeddings. Cross-encoders and Re-ranking",
    "section": "Retrieve and re-rank",
    "text": "Retrieve and re-rank\nNow that we have learned about the differences between cross-encoders and bi-encoders, let‚Äôs see how we can use them in practice by doing a two-stage retrieval and re-ranking system. This is a common technique in information retrieval, where you first retrieve the most relevant documents and then re-rank them using a more accurate model. This is a good option for comparing thousands of sentences efficiently and caring about accuracy.\nSuppose you have a corpus of 100,000 sentences and want to find the most relevant sentences to a given query. The first step is to use a bi-encoder to retrieve many candidates (to ensure recall). Then, you use a cross-encoder to re-rank the candidates and get the final result with high precision. This is a high-level overview of how the system would look like\n\nLet‚Äôs try our luck by implementing a paper search system! We‚Äôll use a AI Arxiv Dataset in an excellent tutorial from Pinecone about rerankers. The goal is to be able to ask AI questions and get relevant paper sections to answer the questions.\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"jamescalam/ai-arxiv-chunked\")\ndataset[\"train\"]\n\nFound cached dataset json (/home/osanseviero/.cache/huggingface/datasets/jamescalam___json/jamescalam--ai-arxiv-chunked-0d76bdc6812ffd50/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n\n\n\n\n\nDataset({\n    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n    num_rows: 41584\n})\n\n\nIf you look at the dataset, it‚Äôs a chunked dataset of 400 Arxiv papers. Chunked means that sections are split into chunks/pieces of fewer tokens to make things more manageable for the model. Here is a sample:\n\ndataset[\"train\"][0]\n\n{'doi': '1910.01108',\n 'chunk-id': '0',\n 'chunk': 'DistilBERT, a distilled version of BERT: smaller,\\nfaster, cheaper and lighter\\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\\nHugging Face\\n{victor,lysandre,julien,thomas}@huggingface.co\\nAbstract\\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be Ô¨Ånetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-speciÔ¨Åc\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its',\n 'id': '1910.01108',\n 'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',\n 'summary': 'As Transfer Learning from large-scale pre-trained models becomes more\\nprevalent in Natural Language Processing (NLP), operating these large models in\\non-the-edge and/or under constrained computational training or inference\\nbudgets remains challenging. In this work, we propose a method to pre-train a\\nsmaller general-purpose language representation model, called DistilBERT, which\\ncan then be fine-tuned with good performances on a wide range of tasks like its\\nlarger counterparts. While most prior work investigated the use of distillation\\nfor building task-specific models, we leverage knowledge distillation during\\nthe pre-training phase and show that it is possible to reduce the size of a\\nBERT model by 40%, while retaining 97% of its language understanding\\ncapabilities and being 60% faster. To leverage the inductive biases learned by\\nlarger models during pre-training, we introduce a triple loss combining\\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\\nfor on-device computations in a proof-of-concept experiment and a comparative\\non-device study.',\n 'source': 'http://arxiv.org/pdf/1910.01108',\n 'authors': ['Victor Sanh',\n  'Lysandre Debut',\n  'Julien Chaumond',\n  'Thomas Wolf'],\n 'categories': ['cs.CL'],\n 'comment': 'February 2020 - Revision: fix bug in evaluation metrics, updated\\n  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at\\n  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing\\n  - NeurIPS 2019',\n 'journal_ref': None,\n 'primary_category': 'cs.CL',\n 'published': '20191002',\n 'updated': '20200301',\n 'references': [{'id': '1910.01108'}]}\n\n\nLet‚Äôs get all the chunks, which we‚Äôll encode:\n\nchunks = dataset[\"train\"][\"chunk\"] \nlen(chunks)\n\n41584\n\n\nNow, we‚Äôll use a bi-encoder to encode all the chunks into embeddings. We‚Äôll truncate long passages to 512 tokens. Note that short context is one of the downsides of many embedding models! We‚Äôll specifically use the multi-qa-MiniLM-L6-cos-v1 model, which is a small-sized model trained to encoder questions and passages into a similar embedding space. This model is a bi-encoder, so it‚Äôs fast and scalable.\nEmbedding all the 40,000+ passages takes around 30 seconds on my not-particularly special computer. Please note that we only need to generate the embeddings of the passages once, as we can save them to disk and load them later. In a production setting, you can save the embeddings to a database and load from there.\n\nfrom sentence_transformers import SentenceTransformer\n\nbi_encoder = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\nbi_encoder.max_seq_length = 256\n\ncorpus_embeddings = bi_encoder.encode(chunks, convert_to_tensor=True, show_progress_bar=True)\n\n\n\n\nAwesome! Now, let‚Äôs provide a question and search for the relevant passage. To do this, we need to encode the question and then compute the similarity between the question and all the passages. Let‚Äôs do this and look at the top hits!\n\nfrom sentence_transformers import util\n\nquery = \"what is rlhf?\"\ntop_k = 25 # how many chunks to retrieve\nquery_embedding = bi_encoder.encode(query, convert_to_tensor=True).cuda()\n\nhits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)[0]\nhits\n\n[{'corpus_id': 14679, 'score': 0.6097552180290222},\n {'corpus_id': 17387, 'score': 0.5659530162811279},\n {'corpus_id': 39564, 'score': 0.5590510368347168},\n {'corpus_id': 14725, 'score': 0.5585878491401672},\n {'corpus_id': 5628, 'score': 0.5296251773834229},\n {'corpus_id': 14802, 'score': 0.5075011253356934},\n {'corpus_id': 9761, 'score': 0.49943411350250244},\n {'corpus_id': 14716, 'score': 0.4931946098804474},\n {'corpus_id': 9763, 'score': 0.49280521273612976},\n {'corpus_id': 20638, 'score': 0.4884325861930847},\n {'corpus_id': 20653, 'score': 0.4873950183391571},\n {'corpus_id': 9755, 'score': 0.48562008142471313},\n {'corpus_id': 14806, 'score': 0.4792214035987854},\n {'corpus_id': 14805, 'score': 0.475425660610199},\n {'corpus_id': 20652, 'score': 0.4740477204322815},\n {'corpus_id': 20711, 'score': 0.4703512489795685},\n {'corpus_id': 20632, 'score': 0.4695567488670349},\n {'corpus_id': 14750, 'score': 0.46810320019721985},\n {'corpus_id': 14749, 'score': 0.46809980273246765},\n {'corpus_id': 35209, 'score': 0.46695172786712646},\n {'corpus_id': 14671, 'score': 0.46657535433769226},\n {'corpus_id': 14821, 'score': 0.4637290835380554},\n {'corpus_id': 14751, 'score': 0.4585301876068115},\n {'corpus_id': 14815, 'score': 0.45775431394577026},\n {'corpus_id': 35250, 'score': 0.4569615125656128}]\n\n\n\n#Let's store the IDs for later\nretrieval_corpus_ids = [hit['corpus_id'] for hit in hits]\n\n# Now let's print the top 3 results\nfor i, hit in enumerate(hits[:3]):\n    sample = dataset[\"train\"][hit[\"corpus_id\"]]\n    print(f\"Top {i+1} passage with score {hit['score']} from {sample['source']}:\")\n    print(sample[\"chunk\"])\n    print(\"\\n\")\n\nTop 1 passage with score 0.6097552180290222 from http://arxiv.org/pdf/2204.05862:\nlearning from human feedback, which we improve on a roughly weekly cadence. See Section 2.3.\n4This means that our helpfulness dataset goes ‚Äòup‚Äô in desirability during the conversation, while our harmlessness\ndataset goes ‚Äòdown‚Äô in desirability. We chose the latter to thoroughly explore bad behavior, but it is likely not ideal\nfor teaching good behavior. We believe this difference in our data distributions creates subtle problems for RLHF, and\nsuggest that others who want to use RLHF to train safer models consider the analysis in Section 4.4.\n5\n1071081091010\nNumber of Parameters0.20.30.40.50.6Mean Eval Acc\nMean Zero-Shot Accuracy\nPlain Language Model\nRLHF\n1071081091010\nNumber of Parameters0.20.30.40.50.60.7Mean Eval Acc\nMean Few-Shot Accuracy\nPlain Language Model\nRLHFFigure 3 RLHF model performance on zero-shot and few-shot NLP tasks. For each model size, we plot\nthe mean accuracy on MMMLU, Lambada, HellaSwag, OpenBookQA, ARC-Easy, ARC-Challenge, and\nTriviaQA. On zero-shot tasks, RLHF training for helpfulness and harmlessness hurts performance for small\n\n\nTop 2 passage with score 0.5659530162811279 from http://arxiv.org/pdf/2302.07842:\npreferences and values which are diÔ¨Écult to capture by hard- coded reward functions.\nRLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\nranking two model generations for the same prompt. This data is then collected to learn a reward model\nthat predicts a scalar reward given any generated text. The r eward captures human preferences when\njudging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\nalgorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\npre-trained via self-supervised learning. However, for mo re complex tasks, the model‚Äôs generations may not\nbe good enough. In such cases, RLHF is typically applied afte r an initial supervised Ô¨Åne-tuning phase using\na small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\nOuyang et al. ,2022;Stiennon et al. ,2020).\nA successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\n(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing\n\n\nTop 3 passage with score 0.5590510368347168 from http://arxiv.org/pdf/2307.09288:\n31\n5 Discussion\nHere, we discuss the interesting properties we have observed with RLHF (Section 5.1). We then discuss the\nlimitations of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc (Section 5.2). Lastly, we present our strategy for responsibly releasing these\nmodels (Section 5.3).\n5.1 Learnings and Observations\nOur tuning process revealed several interesting results, such as L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc ‚Äôs abilities to temporally\norganize its knowledge, or to call APIs for external tools.\nSFT (Mix)\nSFT (Annotation)\nRLHF (V1)\n0.0 0.2 0.4 0.6 0.8 1.0\nReward Model ScoreRLHF (V2)\nFigure 20: Distribution shift for progressive versions of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , from SFT models towards RLHF.\nBeyond Human Supervision. At the outset of the project, many among us expressed a preference for\n\n\n\n\nGreat! We got the most similar chunks according to the high-recall but low-precision bi-encoder.\nNow, let‚Äôs re-rank by using a higher-accuracy cross-encoder model. We‚Äôll use the cross-encoder/ms-marco-MiniLM-L-6-v2 model. This model was trained with the MS MARCO Passage Retrieval dataset, a large dataset with real search questions and their relevant text passages. That makes the model quite suitable for making predictions using questions and passages.\nWe‚Äôll use the same question and the top 10 chunks we got from the bi-encoder. Let‚Äôs see the results! Recall that cross-encoders expect pairs, so we‚Äôll create pairs of the question and each chunk.\n\nfrom sentence_transformers import  CrossEncoder\ncross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\ncross_inp = [[query, chunks[hit['corpus_id']]] for hit in hits]\ncross_scores = cross_encoder.predict(cross_inp)\ncross_scores\n\narray([ 1.2227577 ,  5.048051  ,  1.2897239 ,  2.205767  ,  4.4136825 ,\n        1.2272772 ,  2.5638275 ,  0.81847703,  2.35553   ,  5.590804  ,\n        1.3877895 ,  2.9497519 ,  1.6762824 ,  0.7211323 ,  0.16303705,\n        1.3640019 ,  2.3106787 ,  1.5849439 ,  2.9696884 , -1.1079378 ,\n        0.7681126 ,  1.5945492 ,  2.2869687 ,  3.5448399 ,  2.056368  ],\n      dtype=float32)\n\n\nLet‚Äôs add a new value with the cross-score and sort by it!\n\nfor idx in range(len(cross_scores)):\n    hits[idx]['cross-score'] = cross_scores[idx]\nhits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\nmsmarco_l6_corpus_ids = [hit['corpus_id'] for hit in hits] # save for later\n\nhits\n\n[{'corpus_id': 20638, 'score': 0.4884325861930847, 'cross-score': 5.590804},\n {'corpus_id': 17387, 'score': 0.5659530162811279, 'cross-score': 5.048051},\n {'corpus_id': 5628, 'score': 0.5296251773834229, 'cross-score': 4.4136825},\n {'corpus_id': 14815, 'score': 0.45775431394577026, 'cross-score': 3.5448399},\n {'corpus_id': 14749, 'score': 0.46809980273246765, 'cross-score': 2.9696884},\n {'corpus_id': 9755, 'score': 0.48562008142471313, 'cross-score': 2.9497519},\n {'corpus_id': 9761, 'score': 0.49943411350250244, 'cross-score': 2.5638275},\n {'corpus_id': 9763, 'score': 0.49280521273612976, 'cross-score': 2.35553},\n {'corpus_id': 20632, 'score': 0.4695567488670349, 'cross-score': 2.3106787},\n {'corpus_id': 14751, 'score': 0.4585301876068115, 'cross-score': 2.2869687},\n {'corpus_id': 14725, 'score': 0.5585878491401672, 'cross-score': 2.205767},\n {'corpus_id': 35250, 'score': 0.4569615125656128, 'cross-score': 2.056368},\n {'corpus_id': 14806, 'score': 0.4792214035987854, 'cross-score': 1.6762824},\n {'corpus_id': 14821, 'score': 0.4637290835380554, 'cross-score': 1.5945492},\n {'corpus_id': 14750, 'score': 0.46810320019721985, 'cross-score': 1.5849439},\n {'corpus_id': 20653, 'score': 0.4873950183391571, 'cross-score': 1.3877895},\n {'corpus_id': 20711, 'score': 0.4703512489795685, 'cross-score': 1.3640019},\n {'corpus_id': 39564, 'score': 0.5590510368347168, 'cross-score': 1.2897239},\n {'corpus_id': 14802, 'score': 0.5075011253356934, 'cross-score': 1.2272772},\n {'corpus_id': 14679, 'score': 0.6097552180290222, 'cross-score': 1.2227577},\n {'corpus_id': 14716, 'score': 0.4931946098804474, 'cross-score': 0.81847703},\n {'corpus_id': 14671, 'score': 0.46657535433769226, 'cross-score': 0.7681126},\n {'corpus_id': 14805, 'score': 0.475425660610199, 'cross-score': 0.7211323},\n {'corpus_id': 20652, 'score': 0.4740477204322815, 'cross-score': 0.16303705},\n {'corpus_id': 35209, 'score': 0.46695172786712646, 'cross-score': -1.1079378}]\n\n\nAs you can see above, the cross-encoder does not agree as much with the bi-encoder. Surprisingly, some of the top cross-encoder results (14815 and 14749) have the lowest bi-encoder scores. This makes sense - bi-encoders compare the similitude of the question and the documents in the embedding space, while cross-encoders consider the relationship between the question and the document.\n\nfor i, hit in enumerate(hits[:3]):\n    sample = dataset[\"train\"][hit[\"corpus_id\"]]\n    print(f\"Top {i+1} passage with score {hit['cross-score']} from {sample['source']}:\")\n    print(sample[\"chunk\"])\n    print(\"\\n\")\n\nTop 1 passage with score 0.9668010473251343 from http://arxiv.org/pdf/2204.05862:\nStackoverflow Good Answer vs. Bad Answer Loss Difference\nPython FT\nPython FT + RLHF(b)Difference in mean log-prob between good and bad\nanswers to Stack OverÔ¨Çow questions.\nFigure 37 Analysis of RLHF on language modeling for good and bad Stack OverÔ¨Çow answers, over many\nmodel sizes, ranging from 13M to 52B parameters. Compared to the baseline model (a pre-trained LM\nÔ¨Ånetuned on Python code), the RLHF model is more capable of distinguishing quality (right) , but is worse\nat language modeling (left) .\nthe RLHF models obtain worse loss. This is most likely due to optimizing a different objective rather than\npure language modeling.\nB.8 Further Analysis of RLHF on Code-Model Snapshots\nAs discussed in Section 5.3, RLHF improves performance of base code models on code evals. In this appendix, we compare that with simply prompting the base code model with a sample of prompts designed to\nelicit helpfulness, harmlessness, and honesty, which we refer to as ‚ÄòHHH‚Äô prompts. In particular, they contain\na couple of coding examples. Below is a description of what this prompt looks like:\nBelow are a series of dialogues between various people and an AI assistant. The AI tries to be helpful,\n\n\nTop 2 passage with score 0.9574587345123291 from http://arxiv.org/pdf/2302.07459:\nWe examine the inÔ¨Çuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an\nincreasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of\nthese models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,\nprevious work shows that the amount of RLHF training can signiÔ¨Åcantly change metrics on a wide range of\npersonality, political preference, and harm evaluations for a given model size [41]. As a result, it is important\nto control for the amount of RLHF training in the analysis of our experiments.\n3.2 Experiments\n3.2.1 Overview\nWe test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping\nand discrimination. Stereotyping involves the use of generalizations about groups in ways that are often\nharmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ\n[40] (¬ß3.2.2) and Windogender [49] (¬ß3.2.3). For discrimination, we focus on whether models make disparate\ndecisions about individuals based on protected characteristics that should have no relevance to the outcome.5\nTo measure discrimination, we construct a new benchmark to test for the impact of race in a law school course\n\n\nTop 3 passage with score 0.9408788084983826 from http://arxiv.org/pdf/2302.07842:\npreferences and values which are diÔ¨Écult to capture by hard- coded reward functions.\nRLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\nranking two model generations for the same prompt. This data is then collected to learn a reward model\nthat predicts a scalar reward given any generated text. The r eward captures human preferences when\njudging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\nalgorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\npre-trained via self-supervised learning. However, for mo re complex tasks, the model‚Äôs generations may not\nbe good enough. In such cases, RLHF is typically applied afte r an initial supervised Ô¨Åne-tuning phase using\na small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\nOuyang et al. ,2022;Stiennon et al. ,2020).\nA successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\n(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing\n\n\n\n\nNice! The results seem relevant to the query. What can we do to improve the results?\nHere we used cross-encoder/ms-marco-MiniLM-L-6-v2, which is‚Ä¶well..it‚Äôs three years old and it‚Äôs tiny! It was one of the best re-ranking models some years ago.\nTo pick a model, I suggest going to the MTEB leaderboard, clicking reranking, and selecting a good model that meets your requirements. The average column is a good proxy for general quality, but you might be particularly interested in a dataset (e.g., MSMarco in the retrieval tab).\nNote that some older models, such as MiniLM, are not there. Additionally, not all of these models are cross-encoders, so it‚Äôs always important to experiment if adding the second-stage, slower re-ranker is worth it. Here are some that are interesting:\n\nE5 Mistral 7B Instruct (Dec 2023): This is a decoder-based embedder (not an encoder-based one as we learned before!). This means the model is massive for most applications (it has 7B params, which is two orders of magnitude higher than MiniLM!). This one is interesting because of the new trend of using decoder models rather than encoders, which could enable working with longer contexts. Here is the paper.\nBAAI Reranker (Sep 2023): A high-quality re-ranking model with a decent size (278M parameters). Let‚Äôs get the results with this and compare!\n\n\n# Same code as before, just different model\ncross_encoder = CrossEncoder('BAAI/bge-reranker-base')\n\ncross_inp = [[query, chunks[hit['corpus_id']]] for hit in hits]\ncross_scores = cross_encoder.predict(cross_inp)\n\nfor idx in range(len(cross_scores)):\n    hits[idx]['cross-score'] = cross_scores[idx]\n\nhits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\nbge_corpus_ids = [hit['corpus_id'] for hit in hits]\nfor i, hit in enumerate(hits[:3]):\n    sample = dataset[\"train\"][hit[\"corpus_id\"]]\n    print(f\"Top {i+1} passage with score {hit['cross-score']} from {sample['source']}:\")\n    print(sample[\"chunk\"])\n    print(\"\\n\")\n\nTop 1 passage with score 0.9668010473251343 from http://arxiv.org/pdf/2204.05862:\nStackoverflow Good Answer vs. Bad Answer Loss Difference\nPython FT\nPython FT + RLHF(b)Difference in mean log-prob between good and bad\nanswers to Stack OverÔ¨Çow questions.\nFigure 37 Analysis of RLHF on language modeling for good and bad Stack OverÔ¨Çow answers, over many\nmodel sizes, ranging from 13M to 52B parameters. Compared to the baseline model (a pre-trained LM\nÔ¨Ånetuned on Python code), the RLHF model is more capable of distinguishing quality (right) , but is worse\nat language modeling (left) .\nthe RLHF models obtain worse loss. This is most likely due to optimizing a different objective rather than\npure language modeling.\nB.8 Further Analysis of RLHF on Code-Model Snapshots\nAs discussed in Section 5.3, RLHF improves performance of base code models on code evals. In this appendix, we compare that with simply prompting the base code model with a sample of prompts designed to\nelicit helpfulness, harmlessness, and honesty, which we refer to as ‚ÄòHHH‚Äô prompts. In particular, they contain\na couple of coding examples. Below is a description of what this prompt looks like:\nBelow are a series of dialogues between various people and an AI assistant. The AI tries to be helpful,\n\n\nTop 2 passage with score 0.9574587345123291 from http://arxiv.org/pdf/2302.07459:\nWe examine the inÔ¨Çuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an\nincreasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of\nthese models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,\nprevious work shows that the amount of RLHF training can signiÔ¨Åcantly change metrics on a wide range of\npersonality, political preference, and harm evaluations for a given model size [41]. As a result, it is important\nto control for the amount of RLHF training in the analysis of our experiments.\n3.2 Experiments\n3.2.1 Overview\nWe test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping\nand discrimination. Stereotyping involves the use of generalizations about groups in ways that are often\nharmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ\n[40] (¬ß3.2.2) and Windogender [49] (¬ß3.2.3). For discrimination, we focus on whether models make disparate\ndecisions about individuals based on protected characteristics that should have no relevance to the outcome.5\nTo measure discrimination, we construct a new benchmark to test for the impact of race in a law school course\n\n\nTop 3 passage with score 0.9408788084983826 from http://arxiv.org/pdf/2302.07842:\npreferences and values which are diÔ¨Écult to capture by hard- coded reward functions.\nRLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\nranking two model generations for the same prompt. This data is then collected to learn a reward model\nthat predicts a scalar reward given any generated text. The r eward captures human preferences when\njudging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\nalgorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\npre-trained via self-supervised learning. However, for mo re complex tasks, the model‚Äôs generations may not\nbe good enough. In such cases, RLHF is typically applied afte r an initial supervised Ô¨Åne-tuning phase using\na small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\nOuyang et al. ,2022;Stiennon et al. ,2020).\nA successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\n(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing\n\n\n\n\nLet‚Äôs compare the ranking of the three models:\n\nfor i in range(25):\n    print(f\"Top {i+1} passage. Bi-encoder {retrieval_corpus_ids[i]}, Cross-encoder (MS Marco) {msmarco_l6_corpus_ids[i]}, BGE {bge_corpus_ids[i]}\")\n\nTop 1 passage. Bi-encoder 14679, Cross-encoder (MS Marco) 20638, BGE 14815\nTop 2 passage. Bi-encoder 17387, Cross-encoder (MS Marco) 17387, BGE 20638\nTop 3 passage. Bi-encoder 39564, Cross-encoder (MS Marco) 5628, BGE 17387\nTop 4 passage. Bi-encoder 14725, Cross-encoder (MS Marco) 14815, BGE 14679\nTop 5 passage. Bi-encoder 5628, Cross-encoder (MS Marco) 14749, BGE 9761\nTop 6 passage. Bi-encoder 14802, Cross-encoder (MS Marco) 9755, BGE 39564\nTop 7 passage. Bi-encoder 9761, Cross-encoder (MS Marco) 9761, BGE 20632\nTop 8 passage. Bi-encoder 14716, Cross-encoder (MS Marco) 9763, BGE 14725\nTop 9 passage. Bi-encoder 9763, Cross-encoder (MS Marco) 20632, BGE 9763\nTop 10 passage. Bi-encoder 20638, Cross-encoder (MS Marco) 14751, BGE 14750\nTop 11 passage. Bi-encoder 20653, Cross-encoder (MS Marco) 14725, BGE 14805\nTop 12 passage. Bi-encoder 9755, Cross-encoder (MS Marco) 35250, BGE 9755\nTop 13 passage. Bi-encoder 14806, Cross-encoder (MS Marco) 14806, BGE 14821\nTop 14 passage. Bi-encoder 14805, Cross-encoder (MS Marco) 14821, BGE 14802\nTop 15 passage. Bi-encoder 20652, Cross-encoder (MS Marco) 14750, BGE 14749\nTop 16 passage. Bi-encoder 20711, Cross-encoder (MS Marco) 20653, BGE 5628\nTop 17 passage. Bi-encoder 20632, Cross-encoder (MS Marco) 20711, BGE 14751\nTop 18 passage. Bi-encoder 14750, Cross-encoder (MS Marco) 39564, BGE 14716\nTop 19 passage. Bi-encoder 14749, Cross-encoder (MS Marco) 14802, BGE 14806\nTop 20 passage. Bi-encoder 35209, Cross-encoder (MS Marco) 14679, BGE 20711\nTop 21 passage. Bi-encoder 14671, Cross-encoder (MS Marco) 14716, BGE 20652\nTop 22 passage. Bi-encoder 14821, Cross-encoder (MS Marco) 14671, BGE 14671\nTop 23 passage. Bi-encoder 14751, Cross-encoder (MS Marco) 14805, BGE 20653\nTop 24 passage. Bi-encoder 14815, Cross-encoder (MS Marco) 20652, BGE 35209\nTop 25 passage. Bi-encoder 35250, Cross-encoder (MS Marco) 35209, BGE 35250\n\n\nInteresting, we get very different results! Let‚Äôs briefly look into some of them.\n\n\n\n\n\n\nNote\n\n\n\nI suggest doing something like dataset[\"train\"][20638][\"chunk\"] to print a particular result. Here is a quick summary of the results.\n\n\nThe bi-encoder is good at getting some results related to RLHF, but it‚Äôs struggling to get good, precise passages responding to what RLHF is. I looked at the top 5 results for each model. From looking at the passages, 17387 and 20638 are the only passages that really answer the question. Although the three models agree that 17387 is highly relevant, it‚Äôs interesting that the bi-encoder ranks 20638 lowly, while the two cross-encoders rank it highly. You can find them here.\n\n\n\n\n\n\n\n\n\n\nCorpus ID\nRelevant text or summary\nBi-encoder pos (from top 10)\nMSMarco pos\nBGE pos\n\n\n\n\n14679\nDiscusses implications and applications of RLHF but no definition.\n1\n20\n4\n\n\n17387\nDescribes the process of RLHF in detail and applications\n2\n2\n3\n\n\n39564\nThis chunk is messy and is more of a discussion section intro than an answer\n3\n18\n6\n\n\n14725\nCharacteristics about RLHF but no definition of what it is\n4\n11\n8\n\n\n20638\n‚Äúincreasingly popular technique for reducing harmful behaviors in large language models‚Äù\n10\n1\n2\n\n\n5628\nDiscusses the reward modeling (a component) but does not define RLHF\n5\n3\n16\n\n\n14815\nDiscusses RLHF but does not define it\n24\n4\n1\n\n\n14749\nDiscusses impact of RLHF but it has no definition\n19\n5\n15\n\n\n9761\nDiscusses the reward modeling (a component) but does not define RLHF\n7\n7\n5\n\n\n\nReranking is a frequent feature in libraries; llamaindex allows you to use a VectorIndexRetriever to retrieve and a LLMRerank to rerank (see tutorial), Cohere offers a Rerank Endpoint and qdrant supports similar functionality. However, as you saw above, it‚Äôs relatively simple to implement yourself. If you have a high-quality bi-encoder model, you can use it to rerank and benefit from its speed.\n\n\n\n\n\n\nLLMs as rerankers\n\n\n\nSome people use a generative LLM as a reranker. For example, OpenAI‚Äôs Coobook has an example in which they use GPT-3 as a reranker by building a prompt asking the model to determine if a document is relevant for the document. Although this shows the impressive capabilities of an LLM, it‚Äôs usually not the best option for the task, as it will likely have worse quality, be more expensive, and be slower than a cross-encoder.\nExperiment and see what works best for your data. Using LLMs as rerankers can sometimes be helpful if your documents have very long contexts (for which bert-based models struggle)."
  },
  {
    "objectID": "blog/posts/sentence_embeddings2/index.html#aside-specter2",
    "href": "blog/posts/sentence_embeddings2/index.html#aside-specter2",
    "title": "Sentence Embeddings. Cross-encoders and Re-ranking",
    "section": "Aside: SPECTER2",
    "text": "Aside: SPECTER2\nIf you‚Äôre particularly excited about embeddings for scientific tasks, I suggest looking at SPECTER2 from AllenAI, a family of models that generate embeddings for scientific papers. These models can be used to do things such as predicting links, looking for nearest papers, find candidate papers for a given query, classify papers using the embeddings as features, and more!\nThe base model was trained on scirepeval, a dataset of millions of triples of scientific paper citations. After being trained, the authors fine-tuned the model using adapters, a library for parameter-efficient fine-tuning (don‚Äôt worry if you don‚Äôt know what this is). The authors attached a small neural network, called an adapter, to the base model. This adapter is trained to perform a specific task, but training for a specific task requires much fewer data than training the whole model. Because of these differences, one needs to use transformers and adapters to run inference, e.g.¬†by doing something like\nmodel = AutoAdapterModel.from_pretrained('allenai/specter2_base')\nmodel.load_adapter(\"allenai/specter2\", source=\"hf\", load_as=\"proximity\", set_active=True)\nI recommend reading the model card to learn more about the model and its usage. You can also read the paper for more details."
  },
  {
    "objectID": "blog/posts/sentence_embeddings2/index.html#aside-augmented-sbert",
    "href": "blog/posts/sentence_embeddings2/index.html#aside-augmented-sbert",
    "title": "Sentence Embeddings. Cross-encoders and Re-ranking",
    "section": "Aside: Augmented SBERT",
    "text": "Aside: Augmented SBERT\nAugmented SBERT is a technique for collecting data to improve bi-encoders. Pre-training and fine-tuning bi-encoders require lots of data, so the authors suggested using cross-encoders to label a large set of input pairs and add that to the training data. For example, if you have very little labeled data, you can train a cross-encoder and then label unlabeled pairs, which can be used to train a bi-encoder.\nHow do you generate the pairs? We can use random combinations of sentences and then label them using the cross-encoder. This would lead to mostly negative pairs and skew the label distribution. To avoid this, the authors explored different techniques:\n\nWith Kernel Density Estimation (KDE), the goal is to have similar label distributions between a small, golden dataset and the augmentation dataset. This is achieved by dropping some negative pairs. Of course, this will be inefficient as you‚Äôll need to generate many pairs to get a few positive ones.\nBM25 is an algorithm used in search engines based on overlap (e.g., word frequency, length of document, etc.). Based on this, the authors get the top-k similar sentences to retrieve the k most similar sentences, and then, a cross-encoder is used to label them. This is efficient but will only be able to capture semantic similarity if there is little overlap between the sentences.\nSemantic Search Sampling trains a bi-encoder on the golden data and then used to sample other similar pairs.\nBM25 + Semantic Search Sampling combines the two previous methods. This helps find lexical and semantically similar sentences.\n\nThere are nice figures and example scripts to do this in the Sentence Transformers docs.\n\n\n\nAugmented SBERT - the image is from the original paper"
  },
  {
    "objectID": "blog/posts/sentence_embeddings2/index.html#conclusion",
    "href": "blog/posts/sentence_embeddings2/index.html#conclusion",
    "title": "Sentence Embeddings. Cross-encoders and Re-ranking",
    "section": "Conclusion",
    "text": "Conclusion\nThat was fun! We just learned to do one of the most common sentence embedding tasks: retrieve and rerank! We learned about the differences between bi-encoders and cross-encoders and when to use one versus the other. We also learned about some techniques to improve bi-encoders, such as augmented SBERT.\nDon‚Äôt hesitate to change the code and play with it! If you like this blog post, don‚Äôt hesitate to leave a GitHub Star or share it, that‚Äôs always appreciated and motivating!"
  },
  {
    "objectID": "blog/posts/sentence_embeddings2/index.html#knowledge-check",
    "href": "blog/posts/sentence_embeddings2/index.html#knowledge-check",
    "title": "Sentence Embeddings. Cross-encoders and Re-ranking",
    "section": "Knowledge Check",
    "text": "Knowledge Check\n\nWhat is the difference between bi-encoders and cross-encoders?\nExplain the different steps of reranking.\nHow many embeddings would we need to generate to compare 30,000 sentences using a bi-encoder? How many times would we run inference with a cross-encoder?\nWhat are some techniques to improve bi-encoders?\n\nNow, you have solid foundations to implement your search system. As a follow-up, I suggest implementing a similar retrieve and rerank system with a different dataset. Explore how changing both retrieval and reranking models impact your results."
  },
  {
    "objectID": "blog/posts/sentence_embeddings/index.html",
    "href": "blog/posts/sentence_embeddings/index.html",
    "title": "Sentence Embeddings. Introduction to Sentence Embeddings",
    "section": "",
    "text": "This series aims to demystify embeddings and show you how to use them in your projects. This first blog post will teach you how to use and scale up open-source embedding models. We‚Äôll look into the criteria for picking an existing model, current evaluation methods, and the state of the ecosystem. We‚Äôll look into three exciting applications:\nYou can either read the content here or execute it in Google Colab by clicking the badge at the top of the page. Let‚Äôs dive into embeddings!"
  },
  {
    "objectID": "blog/posts/sentence_embeddings/index.html#the-tldr",
    "href": "blog/posts/sentence_embeddings/index.html#the-tldr",
    "title": "Sentence Embeddings. Introduction to Sentence Embeddings",
    "section": "The TL;DR",
    "text": "The TL;DR\nYou keep reading about ‚Äúembeddings this‚Äù and ‚Äúembeddings that‚Äù, but you might still not know exactly what they are. You are not alone! Even if you have a vague idea of what embeddings are, you might use them through a black-box API without really understanding what‚Äôs going on under the hood. This is a problem because the current state of open-source embedding models is very strong - they are pretty easy to deploy, small (and hence cheap to host), and outperform many closed-source models.\nAn embedding represents information as a vector of numbers (think of it as a list!). For example, we can obtain the embedding of a word, a sentence, a document, an image, an audio file, etc. Given the sentence ‚ÄúToday is a sunny day‚Äù, we can obtain its embedding, which would be a vector of a specific size, such as 384 numbers (such vector could look like [0.32, 0.42, 0.15, ‚Ä¶, 0.72]). What is interesting is that the embeddings capture the semantic meaning of the information. For example, embedding the sentence ‚ÄúToday is a sunny day‚Äù will be very similar to that of the sentence ‚ÄúThe weather is nice today‚Äù. Even if the words are different, the meaning is similar, and the embeddings will reflect that.\n\n\n\n\n\n\nIf you‚Äôre not sure what words such as ‚Äúvector‚Äù, ‚Äúsemantic similarity‚Äù, the vector size, or ‚Äúpretrained‚Äù mean, don‚Äôt worry! We‚Äôll explain them in the following sections. Focus on the high-level understanding first.\n\n\n\nSo, this vector captures the semantic meaning of the information, making it easier to compare to each other. For example, we can use embeddings to find similar questions in Quora or StackOverflow, search code, find similar images, etc. Let‚Äôs look into some code!\nWe‚Äôll use Sentence Transformers, an open-source library that makes it easy to use pre-trained embedding models. In particular, ST allows us to turn sentences into embeddings quickly. Let‚Äôs run an example and then discuss how it works under the hood.\nLet‚Äôs begin by installing the library:\n\n!pip install sentence_transformers\n\nThe second step is to load an existing model. We‚Äôll start using all-MiniLM-L6-v2. It‚Äôs not the best open-source embedding model, but it‚Äôs quite popular and very small (23 million parameters), which means we can get started with it very quickly.\n\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n\nNow that we loaded a model, let‚Äôs use it to encode some sentences. We can use the encode method to obtain the embeddings of a list of sentences. Let‚Äôs try it out!\n\nfrom sentence_transformers import util\n\nsentences = [\"The weather today is beautiful\", \"It's raining!\", \"Dogs are awesome\"]\nembeddings = model.encode(sentences)\nembeddings.shape\n\n(3, 384)\n\n\nall-MiniLM-L6-v2 creates embeddings of 384 values. We obtain three embeddings, one for each sentence. Think of embeddings as a ‚Äúdatabase‚Äù of embeddings. Given a new sentence, how can we find the most similar sentence? We can use the util.pytorch_cos_sim method to compute the cosine similarity (we‚Äôll talk more about it soon) between the new sentence embedding and all the embeddings in the database. The cosine similarity is a number between 0 and 1 that indicates how similar two embeddings are. A value of 1 means that the embeddings are identical, while 0 means that the embeddings are entirely different. Let‚Äôs try it out!\n\nfirst_embedding = model.encode(\"Today is a sunny day\")\nfor embedding, sentence in zip(embeddings, sentences):\n    similarity = util.pytorch_cos_sim(first_embedding, embedding)\n    print(similarity, sentence)\n\ntensor([[0.7344]]) The weather today is beautiful\ntensor([[0.4180]]) It's raining!\ntensor([[0.1060]]) Dogs are awesome\n\n\nWhat can we interpret of this? Although ‚Äútoday is a sunny day‚Äù and ‚Äúthe weather today is beautiful‚Äù don‚Äôt have the same words, the embeddings can capture some semantic meaning, so the cosine similarity is relatively high. On the other hand, ‚ÄúDogs are awesome‚Äù, although true, has nothing to do with the weather or today; hence, the cosine similarity is very low.\nTo expand on this idea of similar embeddings, let‚Äôs look into how they could be used in a product. Imagine that U.S. Social Security would like to allow users to write Medicare-related questions in an input field. This topic is very sensitive, and we likely don‚Äôt want a model to hallucinate with something unrelated! Instead, we can leverage a database of questions (in this case, there‚Äôs an existing Medicare FAQ). The process is similar to the above‚Äù\n\nWe have a corpus (collection) of questions and answers.\nWe compute the embeddings of all the questions.\nGiven a new question, we compute its embedding.\nWe compute the cosine similarity between the new question embedding and all the embeddings in the database.\nWe return the most similar question (which is associated with the most similar embedding).\n\nSteps 1 and 2 can be done offline (that is, we compute the embeddings only once and store them). The rest of the steps can be done at search time (each time a user asks a question). Let‚Äôs see what this would look like in code.\n\n\n\nRepresentation of embeddings in two dimensions\n\n\nLet‚Äôs first create our map of frequently asked questions.\n\n# Data from https://faq.ssa.gov/en-US/topic/?id=CAT-01092\n\nfaq = {\n    \"How do I get a replacement Medicare card?\": \"If your Medicare card was lost, stolen, or destroyed, you can request a replacement online at Medicare.gov.\",\n    \"How do I sign up for Medicare?\": \"If you already get Social Security benefits, you do not need to sign up for Medicare. We will automatically enroll you in Original Medicare (Part A and Part B) when you become eligible. We will mail you the information a few months before you become eligible.\",\n    \"What are Medicare late enrollment penalties?\": \"In most cases, if you don‚Äôt sign up for Medicare when you‚Äôre first eligible, you may have to pay a higher monthly premium. Find more information at https://faq.ssa.gov/en-us/Topic/article/KA-02995\",\n    \"Will my Medicare premiums be higher because of my higher income?\": \"Some people with higher income may pay a larger percentage of their monthly Medicare Part B and prescription drug costs based on their income. We call the additional amount the income-related monthly adjustment amount.\",\n    \"What is Medicare and who can get it?\": \"Medicare is a health insurance program for people age 65 or older. Some younger people are eligible for Medicare including people with disabilities, permanent kidney failure and amyotrophic lateral sclerosis (Lou Gehrig‚Äôs disease or ALS). Medicare helps with the cost of health care, but it does not cover all medical expenses or the cost of most long-term care.\",\n}\n\nOnce again, we use the encode method to obtain the embeddings of all the questions.\n\ncorpus_embeddings = model.encode(list(faq.keys()))\nprint(corpus_embeddings.shape)\n\n(5, 384)\n\n\nOnce a user asks a question, we obtain its embedding. We usually refer to this embedding as the query embedding.\n\nuser_question = \"Do I need to pay more after a raise?\"\nquery_embedding = model.encode(user_question)\nquery_embedding.shape\n\n(384,)\n\n\nWe can now compute the similarity between the corpus embeddings and the query embedding. We could have a loop and use util.pytorch.cos_sim as we did before, but Sentence Transformers provides an even friendlier method called semantic_search that does all the work for us. It returns the top-k most similar embeddings and their similarity score. Let‚Äôs try it out!\n\nsimilarities = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)\nsimilarities\n\n[[{'corpus_id': 3, 'score': 0.35796287655830383},\n  {'corpus_id': 2, 'score': 0.2787758708000183},\n  {'corpus_id': 1, 'score': 0.15840476751327515}]]\n\n\nLet‚Äôs now look at which questions and answers this corresponds to:\n\nfor i, result in enumerate(similarities[0]):\n    corpus_id = result[\"corpus_id\"]\n    score = result[\"score\"]\n    print(f\"Top {i+1} question (p={score}): {list(faq.keys())[corpus_id]}\")\n    print(f\"Answer: {list(faq.values())[corpus_id]}\")\n\nTop 1 question (p=0.35796287655830383): Will my Medicare premiums be higher because of my higher income?\nAnswer: Some people with higher income may pay a larger percentage of their monthly Medicare Part B and prescription drug costs based on their income. We call the additional amount the income-related monthly adjustment amount.\nTop 2 question (p=0.2787758708000183): What are Medicare late enrollment penalties?\nAnswer: In most cases, if you don‚Äôt sign up for Medicare when you‚Äôre first eligible, you may have to pay a higher monthly premium. Find more information at https://faq.ssa.gov/en-us/Topic/article/KA-02995\nTop 3 question (p=0.15840476751327515): How do I sign up for Medicare?\nAnswer: If you already get Social Security benefits, you do not need to sign up for Medicare. We will automatically enroll you in Original Medicare (Part A and Part B) when you become eligible. We will mail you the information a few months before you become eligible.\n\n\nGreat, so given the question ‚ÄúDo I need to pay more after a raise?‚Äù, we know that the most similar question is ‚ÄúWill my Medicare premiums be higher because of my higher income?‚Äù and hence we can return the provided answer. In practice, you would likely have thousands to millions of embeddings, but this was a simple yet powerful example of how embeddings can be used to find similar questions.\nNow that we better understand what embeddings are and how they can be used, let‚Äôs do a deeper dive into them!"
  },
  {
    "objectID": "blog/posts/sentence_embeddings/index.html#from-word-embeddings-to-sentence-embeddings",
    "href": "blog/posts/sentence_embeddings/index.html#from-word-embeddings-to-sentence-embeddings",
    "title": "Sentence Embeddings. Introduction to Sentence Embeddings",
    "section": "From word embeddings to sentence embeddings",
    "text": "From word embeddings to sentence embeddings\n\nWord2Vec and GloVe\nIt‚Äôs time to take a step back and learn more about embeddings and why they are needed. Neural networks, such as BERT, are not able to process words directly; they need numbers. And the way to provide words is to represent them as vectors, also called word embeddings.\nIn the traditional setup, you define a vocabulary (which words are allowed), and then each word in this vocabulary has an assigned embedding. Words not in the vocabulary are mapped to a special token, usually called  (a standard placeholder for words not found during training). For example, let‚Äôs say we have a vocabulary of three words, and we assign each word a vector of size five. We could have the following embeddings:\n\n\n\nWord\nEmbedding\n\n\n\n\nking\n[0.15, 0.2, 0.2, 0.3, 0.5]\n\n\nqueen\n[0.12, 0.1, 0.19, 0.3, 0.47]\n\n\npotato\n[0.13, 0.4, 0.1, 0.15, 0.01]\n\n\n&lt;UNK&gt;\n[0.01, 0.02, 0.01, 0.4, 0.11]\n\n\n\nThe embedding I wrote above are numbers that I wrote somewhat randomly. In practice, the embeddings are learned. This is the main idea of methods such as Word2Vec and GloVe. They learn the embeddings of the words in a corpus in such a way that words that appear in similar contexts have similar embeddings. For example, the embeddings of ‚Äúking‚Äù and ‚Äúqueen‚Äù are similar because they appear in similar contexts.\n\n\n\nWord embeddings\n\n\nSome open-source libraries, such as Gensim and fastText, allow you to obtain pre-trained Word2Vec and GloVe embeddings quickly. In the good ol‚Äô days of NLP (2013), people used these models to compute word embeddings, which were helpful as inputs to other models. For example, you can compute the word embeddings of each word in a sentence and then pass that as input to a sci-kit learn classifier to classify the sentiment of the sentence.\nGlove and Word2Vec have fixed representations. Once they are trained, each word is assigned a fixed vector representation, regardless of their context (so ‚Äúbank‚Äù in ‚Äúriver bank‚Äù and ‚Äúsavings bank‚Äù would have the same embedding). Word2vec and GloVe will struggle with words that have multiple meanings.\n\n\n\nThe good ol‚Äô days of NLP\n\n\n\n\n\n\n\n\nUnderstanding the details of word2vec and GloVe is unnecessary to understand the rest of the blog post and sentence embeddings, so I‚Äôll skip them. I recommend reading this chapter from the excellent interactive NLP course if you‚Äôre interested.\nAs a TL;DR\n\nWord2Vec is trained by passing a very large corpus and training a shallow neural network to predict the surrounding words. Later alternatives predict the center word given the surrounding words.\nGloVe is trained by looking at the co-occurrence matrix of words (how often words appear together within a certain distance) and then using that matrix to obtain the embeddings.\n\nWord2Vec and GloVe are trained with objectives that ensure that words appearing in similar contexts have similar embeddings.\n\n\n\n\n\nWord Embeddings with Transformers\nMore recently, with the advent of transformers, we have new ways to compute embeddings. The embedding is also learned, but instead of training an embedding model and then another model for the specific task, transformers learn useful embeddings in the context of their task. For example, BERT, a popular transformer model, learns word embeddings in the context of masked language modeling (predicting which word to fill in the blank) and next sentence prediction (whether sentence B follows sentence A).\nTransformers are state-of-the-art in many NLP tasks and can capture contextual information that word2vec and GloVe cannot capture, thanks to a mechanism called attention. Attention allows the model to weigh other words‚Äô importance and capture contextual information. For example, in the sentence ‚ÄúI went to the bank to deposit money‚Äù, the word ‚Äúbank‚Äù is ambiguous. Is it a river bank or a savings bank? The model can use the word ‚Äúdeposit‚Äù to understand that it‚Äôs a savings bank. These are contextualized embeddings - their word embedding can differ based on their surrounding words.\nOk‚Ä¶we talked a lot about word embeddings; time to run some code. Let‚Äôs use a pre-trained transformer model, bert-base-uncased, and obtain some word embeddings. We‚Äôll use the transformers library for this. Let‚Äôs begin by loading the model and its tokenizer\n\nfrom transformers import AutoModel, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\n\nWe haven‚Äôt talked about tokenization so far. Until now, we‚Äôve assumed we split data into words. When using transformers, we divided text into tokens. For example, the word ‚Äúbanking‚Äù could be split into two tokens, ‚Äúbank‚Äù and ‚Äúing‚Äù. The tokenizer is responsible for breaking the data into tokens, and the way it splits the data is model-specific and is a deterministic learning process, which means that the same word will always be split into the same tokens. Let‚Äôs see what this looks like in code:\n\ntext = \"The king and the queen are happy.\"\ntokenizer.tokenize(text, add_special_tokens=True)\n\n['[CLS]', 'the', 'king', 'and', 'the', 'queen', 'are', 'happy', '.', '[SEP]']\n\n\nAlright, in this example, each word was a token! (this is not always the case, as we‚Äôll soon see). But we also see two things that might be unexpected: [CLS] and [SEP]. These are special tokens added to the sentence‚Äôs beginning and end. These are used because BERT was trained with that format. One of BERT‚Äôs training objectives is next-sentence prediction, which means that it was trained to predict whether two sentences are consecutive. The [CLS] token represents the entire sentence, and the [SEP] token separates sentences. This will be interesting later when we talk about sentence embeddings.\nLet‚Äôs now obtain the embeddings of each token.\n\nencoded_input = tokenizer(text, return_tensors=\"pt\")\noutput = model(**encoded_input)\noutput[\"last_hidden_state\"].shape\n\ntorch.Size([1, 10, 768])\n\n\nGreat! BERT is giving us an embedding of 768 values for each token. Each of these tokens has semantic information - they capture the meaning of the word in the context of the sentence. Let‚Äôs see if the embedding corresponding to the word ‚Äúking‚Äù in this context is similar to the one in ‚Äúqueen‚Äù.\n\nking_embedding = output[\"last_hidden_state\"][0][2]  # 2 is the position of king\nqueen_embedding = output[\"last_hidden_state\"][0][5]  # 5 is the position of queen\nprint(f\"Shape of embedding {king_embedding.shape}\")\nprint(\n    f\"Similarity between king and queen embedding {util.pytorch_cos_sim(king_embedding, queen_embedding)[0][0]}\"\n)\n\nShape of embedding torch.Size([768])\nSimilarity between king and queen embedding 0.7920711040496826\n\n\nOk, it seems they are quite similar in this context! Let‚Äôs now look at the word ‚Äúhappy‚Äù.\n\nhappy_embedding = output.last_hidden_state[0][7]  # happy\nutil.pytorch_cos_sim(king_embedding, happy_embedding)\n\ntensor([[0.5239]], grad_fn=&lt;MmBackward0&gt;)\n\n\nThis makes sense; the queen embedding is more similar to the king than the happy embedding.\nLet‚Äôs now look at how the same word can have different values depending on the context:\n\ntext = \"The angry and unhappy king\"\nencoded_input = tokenizer(text, return_tensors=\"pt\")\noutput = model(**encoded_input)\noutput[\"last_hidden_state\"].shape\n\ntorch.Size([1, 7, 768])\n\n\n\ntokenizer.tokenize(text, add_special_tokens=True)\n\n['[CLS]', 'the', 'angry', 'and', 'unhappy', 'king', '[SEP]']\n\n\n\nking_embedding_2 = output[\"last_hidden_state\"][0][5]\nutil.pytorch_cos_sim(king_embedding, king_embedding_2)\n\ntensor([[0.5740]], grad_fn=&lt;MmBackward0&gt;)\n\n\nWow! Although both embeddings seem to correspond to the ‚Äúking‚Äù embedding, they are pretty different in the vector space. What is going on? Remember that these are contextual embeddings. The context of the first sentence is quite positive, while the second sentence is quite negative. Hence, the embeddings are different.\nPreviously, we discussed how the tokenizer might split a word into multiple tokens. A valid question is how we would obtain the word embedding in such a case. Let‚Äôs look at an example with the long word ‚Äútokenization.‚Äù\n\ntokenizer.tokenize(\"tokenization\")\n\n['token', '##ization']\n\n\nThe word ‚Äútokenization‚Äù was split into two tokens, but we care about the embedding of ‚Äútokenization‚Äù! What can we do? We can do a pooling strategy in which we obtain the embedding of each token and then average them to obtain the word embedding. Let‚Äôs try it out!\nAs before, we get started by tokenizing the test and running the token IDs through the model.\n\ntext = \"this is about tokenization\"\n\nencoded_input = tokenizer(text, return_tensors=\"pt\")\noutput = model(**encoded_input)\n\nLet‚Äôs look at the tokenization of the sentence:\n\ntokenizer.tokenize(text, add_special_tokens=True)\n\n['[CLS]', 'this', 'is', 'about', 'token', '##ization', '[SEP]']\n\n\nSo we want to pool the embeddings of the tokens 4 and 5 by averaging them. Let‚Äôs first obtain the embeddings of the tokens.\n\nword_token_indices = [4, 5]\nword_embeddings = output[\"last_hidden_state\"][0, word_token_indices]\nword_embeddings.shape\n\ntorch.Size([2, 768])\n\n\nAnd now let‚Äôs average them using torch.mean.\n\nimport torch\n\ntorch.mean(word_embeddings, dim=0).shape\n\ntorch.Size([768])\n\n\nLet‚Äôs wrap all of it in a function so we can easily use it later.\n\ndef get_word_embedding(text, word):\n    # Encode the text and do a forward pass through the model to get the hidden states\n    encoded_input = tokenizer(text, return_tensors=\"pt\")\n    with torch.no_grad():  # We don't need gradients for embedding extraction\n        output = model(**encoded_input)\n\n    # Find the indices for the word\n    word_ids = tokenizer.encode(\n        word, add_special_tokens=False\n    )  # No special tokens anymore\n    word_token_indices = [\n        i\n        for i, token_id in enumerate(encoded_input[\"input_ids\"][0])\n        if token_id in word_ids\n    ]\n\n    # Pool the embeddings for the word\n    word_embeddings = output[\"last_hidden_state\"][0, word_token_indices]\n    return torch.mean(word_embeddings, dim=0)\n\nExample 1. Similarity between king and queen embeddings in the context of both being angry.\n\nutil.pytorch_cos_sim(\n    get_word_embedding(\"The king is angry\", \"king\"),\n    get_word_embedding(\"The queen is angry\", \"queen\"),\n)\n\ntensor([[0.8564]])\n\n\nExample 2. Similarity between king and queen embeddings in the context of the king being happy and the queen angry. Notice how they are less similar than in the previous example.\n\nutil.pytorch_cos_sim(\n    get_word_embedding(\"The king is happy\", \"king\"),\n    get_word_embedding(\"The queen is angry\", \"queen\"),\n)\n\ntensor([[0.8273]])\n\n\nExample 3. Similarity between king embeddings in two very different contexts. Even if they are the same word, the different context of the word makes the embeddings very different.\n\n# This is same as before\nutil.pytorch_cos_sim(\n    get_word_embedding(\"The king and the queen are happy.\", \"king\"),\n    get_word_embedding(\"The angry and unhappy king\", \"king\"),\n)\n\ntensor([[0.5740]])\n\n\nExample 4. Similarity between a word that has two different meanings. The word ‚Äúbank‚Äù is ambiguous, it can be a river bank or a savings bank. The embeddings are different depending on the context.\n\nutil.pytorch_cos_sim(\n    get_word_embedding(\"The river bank\", \"bank\"),\n    get_word_embedding(\"The savings bank\", \"bank\"),\n)\n\ntensor([[0.7587]])\n\n\nI hope this gave an idea about what word embeddings are. Now that we understand word embeddings let‚Äôs look into sentence embeddings!\n\n\nSentence Embeddings\nJust as word embeddings are vector representations of words, sentence embeddings are vector representations of a sentence. We can also compute embeddings of paragraphs and documents! Let‚Äôs look into it.\nThere are three approaches we can take: [CLS] pooling, max pooling and mean pooling.\n\nMean pooling means averaging all the word embeddings of the sentence.\nMax pooling means taking the maximum value of each dimension of the word embeddings.\n[CLS] pooling means using the embedding corresponding to the [CLS] token as the sentence embedding. Let‚Äôs look deeper into this last one, which is the least intuitive.\n\n\n[CLS] Pooling\nAs we saw before, BERT adds a special token [CLS] at the beginning of the sentence. This token is used to represent the entire sentence. For example, when someone wants to fine-tune a BERT model to perform text classification, a common approach is to add a linear layer on top of the [CLS] embedding. The idea is that the [CLS] token will capture the meaning of the entire sentence.\n\n\n\nThe hidden state/embedding corresponding to the CLS token can be used to fine-tune a classification model.\n\n\nWe can take the same approach and use the embedding of the [CLS] token as the sentence embedding. Let‚Äôs see how this works in code. We‚Äôll use the same sentence as before.\n\nencoded_input = tokenizer(\"This is an example sentence\", return_tensors=\"pt\")\nmodel_output = model(**encoded_input)\nsentence_embedding = model_output[\"last_hidden_state\"][:, 0, :]\nsentence_embedding.shape\n\ntorch.Size([1, 768])\n\n\nGreat! We obtained the model output‚Äôs first embedding, corresponding to the [CLS] token. Let‚Äôs wrap this code into a function.\n\ndef cls_pooling(model_output):\n    return model_output[\"last_hidden_state\"][:, 0, :]\n\n\ndef get_sentence_embedding(text):\n    encoded_input = tokenizer(text, return_tensors=\"pt\")\n    with torch.no_grad():\n        model_output = model(**encoded_input)\n    return cls_pooling(model_output)\n\n\nembeddings = [get_sentence_embedding(sentence) for sentence in sentences]\nquery_embedding = get_sentence_embedding(\"Today is a sunny day\")\nfor embedding, sentence in zip(embeddings, sentences):\n    similarity = util.pytorch_cos_sim(query_embedding, embedding)\n    print(similarity, sentence)\n\ntensor([[0.9261]]) The weather today is beautiful\ntensor([[0.8903]]) It's raining!\ntensor([[0.9317]]) Dogs are awesome\n\n\nHmm‚Ä¶something looks off here ü§î One would have expected this to work out of the box.\nWell, it turns out BERT has an additional trick. As mentioned before, when BERT was trained, the CLS token was used to predict whether two sentences were consecutive. To do so, BERT processes the [CLS]-corresponding embedding and passes it through a linear layer and a tanh activation function (see code here). The idea is that the linear layer and the tanh activation function will learn a better representation of the [CLS] token. This is the pooler component of the BERT model and is used to obtain the model_output.pooler_output.\n\n\n\n\n\n\nThis might sound confusing, so let‚Äôs repeat what‚Äôs happening here.\n\nBERT outputs the embeddings of each token.\nThe first embedding corresponds to the [CLS] token.\nThe [CLS] token is processed through a linear layer and a tanh activation function to obtain the pooler_output.\n\nDuring training, the pooler_output is used to predict whether two sentences are consecutive (one of the pre-training tasks of BERT). This makes processing the [CLS] token more meaningful than the raw [CLS] embedding.\n\n\n\nTo show that there is no magic going on here, we can either pass the list of word embeddings to model.pooler or simply get the pooler_output from the model output. Let‚Äôs try it out!\n\nmodel.pooler(model_output[\"last_hidden_state\"])[0][:10]\n\ntensor([-0.9302, -0.4884, -0.4387,  0.8024,  0.3668, -0.3349,  0.9438,  0.3593,\n        -0.3216, -1.0000], grad_fn=&lt;SliceBackward0&gt;)\n\n\n\nmodel_output[\"pooler_output\"][0][:10]\n\ntensor([-0.9302, -0.4884, -0.4387,  0.8024,  0.3668, -0.3349,  0.9438,  0.3593,\n        -0.3216, -1.0000], grad_fn=&lt;SliceBackward0&gt;)\n\n\nYay! As you can see, the first ten elements of the embedding are identical! Let‚Äôs now re-compute the distances using this new embedding technique:\n\ndef cls_pooling(model_output):\n    return model.pooler(model_output[\"last_hidden_state\"])  # we changed this\n\n\n# This stays the same\nembeddings = [get_sentence_embedding(sentence) for sentence in sentences]\nquery_embedding = get_sentence_embedding(\"Today is a sunny day\")\nfor embedding, sentence in zip(embeddings, sentences):\n    similarity = util.pytorch_cos_sim(query_embedding, embedding)\n    print(similarity, sentence)\n\ntensor([[0.9673]], grad_fn=&lt;MmBackward0&gt;) The weather today is beautiful\ntensor([[0.9029]], grad_fn=&lt;MmBackward0&gt;) It's raining!\ntensor([[0.8930]], grad_fn=&lt;MmBackward0&gt;) Dogs are awesome\n\n\nMuch, much better! We just obtained the closest sentences to ‚ÄúToday is a sunny day‚Äù."
  },
  {
    "objectID": "blog/posts/sentence_embeddings/index.html#sentence-transformers",
    "href": "blog/posts/sentence_embeddings/index.html#sentence-transformers",
    "title": "Sentence Embeddings. Introduction to Sentence Embeddings",
    "section": "Sentence Transformers",
    "text": "Sentence Transformers\n\nUsing the transformers library\nThis yields some decent results, but in practice, this was not much better than using Word2Vec or GloVe word embeddings and averaging them. The reason is that the [CLS] token is not trained to be a good sentence embedding. It‚Äôs trained to be a good sentence embedding for next-sentence prediction!\nIntroducing ü•Åü•Åü•Å Sentence Transformers! Sentence Sentence Transformers (also known as SBERT) have a special training technique focusing on yielding high-quality sentence embeddings. Just as in the TL;DR section of this blog post, let‚Äôs use the all-MiniLM-L6-v2 model. In the beginning, we used the sentence-transformers library, which is a high-level wrapper library around transformers. Let‚Äôs try to go the hard way first! The process is as follows:\n\nWe tokenize the input sentence.\nWe process the tokens through the model.\nWe calculate the mean of the token embeddings.\nWe normalize the embeddings to ensure the embedding vector has a unit length.\n\nJust as before, we can load the model and the tokenizer, tokenize the sentence and pass it to the model\n\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\nmodel = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\nencoded_input = tokenizer(\"Today is a sunny day\", return_tensors=\"pt\")\nmodel_output = model(**encoded_input)\n\nWhat we‚Äôve done until now is very similar to what we did before, except that we are using a different model. The next step is to do pooling. While previously we did [CLS] pooling, sentence transformers usually use mean or max pooling. Let‚Äôs try it out!\n\ntoken_embeddings = model_output[\"last_hidden_state\"]\ntoken_embeddings.shape\n\ntorch.Size([1, 7, 384])\n\n\nNote how, with this model, each embedding is smaller (384 values rather than 768). We can now compute the mean of the embeddings to obtain the sentence embedding.\n\nmean_embedding = torch.mean(token_embeddings, dim=1)\nmean_embedding.shape\n\ntorch.Size([1, 384])\n\n\nThe last step is to perform normalization. Normalization ensures that the embedding vector has a unit length, which means its length (or magnitude) is 1.\n\n\n\n\n\n\nWhat is normalization?\n\n\n\nTo understand why we do normalization, revisiting some vector math is helpful. For a vector v with components (v1, v2, ‚Ä¶, vn), it‚Äôs length is defined as\n\\[\n\\| \\mathbf{v} \\| = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2}\n\\]\nWhen normalizing a vector, we scale the values so that the vector length is 1. This is done by dividing each vector element by the vector‚Äôs magnitude.\n\\[\n\\mathbf{u} = \\frac{\\mathbf{v}}{\\| \\mathbf{v} \\|}\n\\]\n\n\nThis is particularly helpful when we want to compare vectors. For example, if we want to compute the cosine similarity between two vectors, we usually compare their direction rather than their magnitude. Normalizing the vectors ensures that each vector contributes equally to the similarity. We‚Äôll talk more about embedding comparisons soon! Let‚Äôs try it out!\n\n\n\n\n\n\nNote\n\n\n\nActually, we are using cosine similarity to compute the similarity between embeddings. As we‚Äôll see later in the blog post, the magnitude of the embeddings is not relevant when computing the cosine similarity, but it‚Äôs still a good think to normalize them in case we want to experiment with other ways to measure distances.\n\n\n\nimport torch.nn.functional as F\n\nnormalized_embedding = F.normalize(mean_embedding)\nnormalized_embedding.shape\n\ntorch.Size([1, 384])\n\n\nLet‚Äôs wrap this in a function!\n\ndef mean_pooling(model_output):\n    return torch.mean(model_output[\"last_hidden_state\"], dim=1)\n\n\ndef get_sentence_embedding(text):\n    encoded_input = tokenizer(text, return_tensors=\"pt\")\n    with torch.no_grad():\n        model_output = model(**encoded_input)\n    sentence_embeddings = mean_pooling(model_output)\n    return F.normalize(sentence_embeddings)\n\n\nget_sentence_embedding(\"Today is a sunny day\")[0][:5]\n\ntensor([-0.0926,  0.5913,  0.5535,  0.4214,  0.2129])\n\n\nIn practice, you‚Äôll likely be encoding batches of sentences, so we need to make some changes\n\nModify the tokenization so we apply truncation (cutting the sentence if it‚Äôs longer than the maximum length) and padding (adding [PAD] tokens to the end of the sentence).\nModify the pooling so we take the attention mask into account. The attention mask is a vector of 0s and 1s that indicates which tokens are real and which are padding. We want to ignore the padding tokens when computing the mean!\n\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[\"last_hidden_state\"]\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\n\n# This now receives a list of sentences\ndef get_sentence_embedding(sentences):\n    encoded_input = tokenizer(\n        sentences, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    with torch.no_grad():\n        model_output = model(**encoded_input)\n    sentence_embeddings = mean_pooling(model_output, encoded_input[\"attention_mask\"])\n    return F.normalize(sentence_embeddings)\n\n\nquery_embedding = get_sentence_embedding(\"Today is a sunny day\")[0]\nquery_embedding[:5]\n\ntensor([-0.0163,  0.1041,  0.0974,  0.0742,  0.0375])\n\n\nWe got the same result, great! Let‚Äôs now repeat our search example from before.\n\nembeddings = [get_sentence_embedding(sentence) for sentence in sentences]\nfor embedding, sentence in zip(embeddings, sentences):\n    similarity = util.pytorch_cos_sim(query_embedding, embedding)\n    print(similarity, sentence)\n\ntensor([[0.7344]]) The weather today is beautiful\ntensor([[0.4180]]) It's raining!\ntensor([[0.1060]]) Dogs are awesome\n\n\nNice! Compared to the vanilla BERT [CLS]-pooled embeddings, the sentence transformer embeddings are more meaningful and have a larger difference between the unrelated vectors!\n\n\n\n\n\n\nWhen to use each pooling strategy? It depends on the task.\n\n[CLS] pooling is usually used when the transformer model has been fine-tuned on a specific downstream task that makes the [CLS] token very useful.\nMean pooling is usually more effective on models that have not been fine-tuned on a downstream task. It ensures that all parts of the sentence are represented equally in the embedding and can work for long sentences where the influence of all tokens should be captured.\nMax pooling can be useful to capture the most important features in a sentence. This can be very useful if particular keywords are very informative, but it might miss the subtler context.\n\nIn practice, a pooling method will be stored with the model, and you won‚Äôt have to worry about it. If there‚Äôs no method specified, mean pooling is usually a good default.\n\n\n\n\n\nUsing the sentence-transformers library\nThis was relatively easy, but the sentence-transformers library makes it even easier for us to do all of this! Here is the same code as in the TL;DR section.\n\nfrom sentence_transformers import SentenceTransformer\n\n# We load the model\nmodel = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n\nquery_embedding = model.encode(\"Today is a sunny day\")\nembeddings = model.encode(sentences)\n\nfor embedding, sentence in zip(embeddings, sentences):\n    similarity = util.pytorch_cos_sim(query_embedding, embedding)\n    print(similarity, sentence)\n\ntensor([[0.7344]]) The weather today is beautiful\ntensor([[0.4180]]) It's raining!\ntensor([[0.1060]]) Dogs are awesome\n\n\nThis is quite powerful! If you had to implement a feature to identify duplicate questions without using ML, you would likely have to implement a lexical search system (which looks at exact matches of the input question), a fuzzy search system (which looks at approximate matches of the input question), or a statistical search system (which looks at the frequency of words in the input question).\nWith embeddings, we can easily find similar questions without implementing any of these systems and having excellent results!\nThe following image is a good example of how embeddings can be used to find code that would answer a user‚Äôs question.\n\n\n\nImage of code search\n\n\n\n\nEmbedding dimensions\nAs you saw before, the model we used, all-MiniLM-L6-v2, generates sentence embeddings of 384 values. This is a hyperparameter of the model and can be changed. The larger the embedding size, the more information the embedding can capture. However, larger embeddings are more expensive to compute and store.\nThe embeddings of popular open-source models go from 384 to 1024. The best current model, as of the time of writing, has embedding dimensions of 4096 values, but the model is much larger (7 billion parameters) compared to other models. In the closed-sourced world, Cohere has APIs that go from 384 to 4096 dimensions, OpenAI has embeddings of 1536, and so on. Embedding dimension is a trade-off. If you use very large embeddings, you will potentially get better results, but you will also have to pay more for hosting and inference. If you use vector databases, you will also have to pay more for storage.\n\n\nSequence length\nOne of the limitations of transformer models is that they have a maximum sequence length. This means that they can only process a certain number of tokens. For example, BERT has a maximum context length of 512 tokens. This means that if you want to encode a sentence with more than 512 tokens, you will have to find ways to work around this limitation. For example, you could split the sentence into multiple sentences of 512 tokens and then average the embeddings. This is not ideal because the model will not be able to capture the context of the entire sentence.\nThis is not a problem for most use cases, but it can be a problem for long documents. For example, if you want to encode a 1000-word document, you will have to split it into multiple sentences of 512 tokens. This is not ideal because the model will not be able to capture the context of the entire document. Another approach can be to first generate a summary of the text and then encode the summary. This is a good approach if you want to encode long documents, but will require a good summarization model that might be too slow. Alternatively, you might know if a specific part of the document is good (such as abstracts, introductions, conclusions, etc.) and only encode that part if that‚Äôs the most meaningful part for your task."
  },
  {
    "objectID": "blog/posts/sentence_embeddings/index.html#application-1.-finding-most-similar-quora-duplicate",
    "href": "blog/posts/sentence_embeddings/index.html#application-1.-finding-most-similar-quora-duplicate",
    "title": "Sentence Embeddings. Introduction to Sentence Embeddings",
    "section": "Application 1. Finding most similar Quora duplicate",
    "text": "Application 1. Finding most similar Quora duplicate\nWe‚Äôre going to use the open-source Quora dataset, which contains 400,000 pairs of questions from Quora. We will not train a model (yet!) and rather just use the embeddings to find similar questions given a new question. Let‚Äôs get started!\nOur first step will be to load the data - to do this, we‚Äôll use the datasets library.\n\n!pip install datasets\n\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"quora\")[\"train\"]\ndataset\n\nDataset({\n    features: ['questions', 'is_duplicate'],\n    num_rows: 404290\n})\n\n\nTo take a quick look at the data within the Dataset object, we can convert it to a Pandas DataFrame and look at the first rows.\n\ndataset.to_pandas().head()\n\n\n\n\n\n\n\n\nquestions\nis_duplicate\n\n\n\n\n0\n{'id': [1, 2], 'text': ['What is the step by s...\nFalse\n\n\n1\n{'id': [3, 4], 'text': ['What is the story of ...\nFalse\n\n\n2\n{'id': [5, 6], 'text': ['How can I increase th...\nFalse\n\n\n3\n{'id': [7, 8], 'text': ['Why am I mentally ver...\nFalse\n\n\n4\n{'id': [9, 10], 'text': ['Which one dissolve i...\nFalse\n\n\n\n\n\n\n\nOk, so each sample is a dictionary. We do not care about the is_duplicate column here. Our goal is to find if any question in this dataset is similar to a new question. Let‚Äôs process the dataset so we only have a list of questions.\n\ncorpus_questions = []\nfor d in dataset:\n    corpus_questions.append(d[\"questions\"][\"text\"][0])\n    corpus_questions.append(d[\"questions\"][\"text\"][1])\ncorpus_questions = list(set(corpus_questions))  # Remove duplicates\nlen(corpus_questions)\n\n537362\n\n\nThe next step is to embed all the questions. We‚Äôll use the sentence-transformers library for this. We‚Äôll use the quora-distilbert-multilingual model, which is a model trained for 100 languages and is trained specifically for Quora-style questions. This is a larger model, and hence will be slightly slower. It will also generate larger embeddings of 768 values.\nTo get some quick results without having to wait five minutes for the model to process all the questions, we‚Äôll only process the first 100000 questions. In practice, you would process all the questions or shuffle the questions and process a random subset of them when experimenting.\n\nmodel = SentenceTransformer(\"quora-distilbert-multilingual\")\nquestions_to_embed = 100000\ncorpus_embeddings = model.encode(\n    corpus_questions[:questions_to_embed],\n    show_progress_bar=True,\n    convert_to_tensor=True,\n)\n\n\n\n\n\ncorpus_embeddings.shape\n\ntorch.Size([100000, 768])\n\n\nWe just obtained 100,000 embddings in 20 seconds, even when this Sentence Transformer model is not tiny and I‚Äôm running this on my GPU-Poor computer. Unlike generative models, which are autoregressive and usually much slower, BERT-based models are super fast!\nLet‚Äôs now write a function that searches the corpus for the most similar question.\n\nimport time\n\n\ndef search(query):\n    start_time = time.time()\n    query_embedding = model.encode(query, convert_to_tensor=True)\n    results = util.semantic_search(query_embedding, corpus_embeddings)\n    end_time = time.time()\n\n    print(\"Results (after {:.3f} seconds):\".format(end_time - start_time))\n    # We look at top 5 results\n    for result in results[0][:5]:\n        print(\n            \"{:.3f}\\t{}\".format(result[\"score\"], corpus_questions[result[\"corpus_id\"]])\n        )\n\n\nsearch(\"How can I learn Python online?\")\n\nResults (after 0.612 seconds):\n0.982   What is the best online resource to learn Python?\n0.980   Where I should learn Python?\n0.980   What's the best way to learn Python?\n0.980   How do I learn Python in easy way?\n0.979   How do I learn Python systematically?\n\n\nLet‚Äôs try in Spanish!\n\nsearch(\"Como puedo aprender Python online?\")\n\nResults (after 0.016 seconds):\n0.980   What are the best websites to learn Python?\n0.980   How can I start learning the developing of websites using Python?\n0.979   How do I learn Python in easy way?\n0.976   How can I learn Python faster and effectively?\n0.976   How can I learn advanced Python?\n\n\nIt seems to be working quite well! Note that although our model can process queries in other languages, such as Spanish in the example above, the embeddings were generated for English questions. This means that the model will not be able to find similar questions in other languages."
  },
  {
    "objectID": "blog/posts/sentence_embeddings/index.html#distance-between-embeddings",
    "href": "blog/posts/sentence_embeddings/index.html#distance-between-embeddings",
    "title": "Sentence Embeddings. Introduction to Sentence Embeddings",
    "section": "Distance between embeddings",
    "text": "Distance between embeddings\n\nCosine similarity\nUntil now we‚Äôve been computing the cosine similarity between embeddings. This is a number between 0 and 1 that indicates how similar two embeddings are. A value of 1 means that the embeddings are identical, while 0 means that the embeddings are entirely different. So far we‚Äôve used it as a black-box, so let‚Äôs look into it a bit more.\nThe cosine similarity allows us to compare how similar two vectors are regardless of their magnitude. For example, if we have two vectors, [1, 2, 3] and [2, 4, 6], they are very similar in terms of direction, but their magnitude is different. The cosine similarity will be close to 1, indicating that they are very similar.\n\na = torch.FloatTensor([1, 2, 3])\nb = torch.FloatTensor([2, 3, 4])\nutil.cos_sim(a, b)\n\ntensor([[0.9926]])\n\n\nLet‚Äôs plot both vectors. As you can see, they are very similar in terms of direction, but their magnitude is different.\n\na\n\ntensor([1., 2., 3.])\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nV = np.array([a.tolist(), b.tolist()])\norigin = np.array([[0, 0], [0, 0]])  # origin point\n\nplt.quiver(*origin, V[:, 0], V[:, 1], color=[\"r\", \"b\", \"g\"], scale=10)\nplt.show()\n\n\n\n\n\n\n\n\nLet‚Äôs dive into its math. Cosine similarity is defined as the dot product of the vectors divided by the product of their magnitudes:\n\\[\n\\text{cosine similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|}\n\\]\nWe already discussed magnitudes at the beginning of the blog post. We need to compute the square root of the sum of the squares of a vector component\n\\[\n\\|\\mathbf{A}\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14}\n\\]\n\\[\n\\|\\mathbf{B}\\| = \\sqrt{2^2 + 3^2 + 4^2} = \\sqrt{29}\n\\]\nWe also need to compute the dot product of the vectors. The dot product is defined as the sum of the products of the corresponding vector components\n\\[\n\\mathbf{A} \\cdot \\mathbf{B} = \\sum_{i=1}^{n} A_i B_i\n\\]\nIn this case, the dot product for A and B would look as follows\n\\[\n\\mathbf{A} \\cdot \\mathbf{B} = 1 \\times 2 + 2 \\times 3 + 3 \\times 4 = 2 + 6 + 12 = 20\n\\]\nFinally, we can compute the cosine similarity by doing\n\\[\n\\text{cosine similarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{20}{\\sqrt{14} \\sqrt{29}} = 0.992583\n\\]\nwhich matches our result above.\n\n\n\n\n\n\nNote\n\n\n\nCan you think of two vectors with cosine similarity of 1? Think of vectors with same direction but different magnitude.\n\n\n\n\nDot product\nCosine similarity does not take magnitude into account, but there might be use cases where the magnitude is meaningful. In those cases, dot product is a better metric. This means that longer or more verbose sentences with similar content could have a higher similarity score than shorter sentences with similar content due to their magnitude.\nThe dot product is defined as the sum of the products of the corresponding vector components (it‚Äôs what we did before!)\n\\[\n\\mathbf{A} \\cdot \\mathbf{B} = \\sum_{i=1}^{n} A_i B_i\n\\]\nIf you look at the cosine similarity formula, if you assume the vectors are normalized (that is, their magnitude is 1), the cosine similarity is equivalent to the dot product. This means that the cosine similarity is a normalized dot product.\nLet‚Äôs create a new vector, [4, 6, 8]. This vector has the same direction as [2, 3, 4], but it‚Äôs twice as long. Let‚Äôs compute the dot product of [1, 2, 3] with [2, 3, 4] and [4, 6, 8].\n\nc = torch.FloatTensor([4, 6, 8])\n\nprint(f\"Cosine Similarity between a and b: {util.cos_sim(a, b)}\")\nprint(f\"Cosine Similarity between a and c: {util.cos_sim(a, c)}\")\n\nprint(f\"Dot product between a and b: {torch.dot(a, b)}\")\nprint(f\"Dot product between a and c: {torch.dot(a, c)}\")\n\nCosine Similarity between a and b: tensor([[0.9926]])\nCosine Similarity between a and c: tensor([[0.9926]])\nDot product between a and b: 20.0\nDot product between a and c: 40.0\n\n\nThis makes sense! As b and c have the same angle, the cosine similarity is the same between a and b and a and c.¬†However, the dot product is higher for a and c because c is longer than b.\n\nV = np.array([a.tolist(), b.tolist(), c.tolist()])\norigin = np.array([[0, 0, 0], [0, 0, 0]])  # origin point\n\nplt.quiver(*origin, V[:, 0], V[:, 1], color=[\"r\", \"b\", \"g\"], scale=20)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEuclidean Distance\nThe Euclidean Distance is the distance between two vectors by measuring a straight line between them. Just as the dot product, the Euclidean distance takes magnitude into account. I won‚Äôt dive too much into interpreting both metrics, but the main idea is that the Dot Product measures how much one vector extends into the direction of another vector, while the Euclidean Distance measures the straight-line distance between two vectors. It is defined as the square root of the sum of the squared differences between the vector components. It‚Äôs defined as\n\\[\n\\text{Euclidean Distance}(\\mathbf{A}, \\mathbf{B}) = \\sqrt{\\sum_{i=1}^{n} (A_i - B_i)^2}\n\\]\nIn practice, you can use the Squared Euclidean (L2-Squared)\n\\[\n\\text{Squared Euclidean}(\\mathbf{A}, \\mathbf{B}) = \\sum_{i=1}^{n} (A_i - B_i)^2\n\\]\n\n\nPicking a score function\nWe just learned about dot-product, cosine similarity, and euclidean distance. When to use which?\nIt depends on the model! Some models will be trained in a way that they produce normalized embeddings. In this case, dot-product, cosine similarity and euclidean distance will all produce the same results.\nOther models are not trained in a way that they produce normalized embeddings - they are tuned for dot-product. In this case, dot-product will be the best function to find the closest items in a vector space. Even then, if the magnitude is not important, we can normalize as we did in the previous sections. You can use different distance functions depending on your use case. Models with normalized embeddings will prefer shorter sentences, while models with non-normalized embeddings will prefer longer sentences. This is because the magnitude of the embeddings will be larger for longer sentences.\n\n\n\nDistance function\nValues\nWhen to use\n\n\n\n\nCosine similarity\n[-1, 1]\nWhen the magnitude is not important\n\n\nDot product\n[-inf, inf]\nWhen the magnitude is important\n\n\nEuclidean distance\n[0, inf]\nWhen the magnitude is important\n\n\n\nTo recap:\n\nCosine similarity focuses on the angle between vectors. It‚Äôs a normalized dot product.\nDot product focused on both magnitude and angle.\nEuclidean distance measures spatial distance between vectors.\n\nThere are other distance functions, such as Manhattan distance, but these are common ones and useful for our use cases!"
  },
  {
    "objectID": "blog/posts/sentence_embeddings/index.html#scaling-up",
    "href": "blog/posts/sentence_embeddings/index.html#scaling-up",
    "title": "Sentence Embeddings. Introduction to Sentence Embeddings",
    "section": "Scaling Up",
    "text": "Scaling Up\nUntil now we‚Äôve been working with just a couple of sentences. In practice, you might have to deal with millions of embeddings, and we cannot always compute the distance to all of them (this is called brute-force search).\nOne approach is to use an approximate nearest neighbor algorithm. These algorithms partition the data into buckets of similar embeddings. This allows us to quickly find the closest embeddings without having to compute the distance to all of them. This is not exact, as some vectors with high similarity might still be missed. There are different libraries you can use to do this, such as Spotify‚Äôs Annoy and Facebook‚Äôs Faiss. Vector databases such as Pinecone and Weaviate also use nearest neighbor techniques to be able to search millions of objects in milliseconds.\nFor now, let‚Äôs look at an interesting application where the scaling issues become more apparent.\n\nApplication 2. Paraphrase Mining\nUntil now, with semantic search, we‚Äôve been looking for the sentence most similar to a query sentence. In paraphrase mining, the goal is to find texts with similar meaning in a very large corpus. Let‚Äôs take our Quora dataset and see if we can find similar questions.\n\nquestions_to_embed = 10\nshort_corpus_questions = corpus_questions[:questions_to_embed]\nshort_corpus_questions\n\n['',\n 'What are the Nostradamus Predictions for the 2017?',\n 'Is it expensive to take music lessons?',\n 'what are the differences between first world and third world countries? Are there any second world countries?',\n 'How much is a 1963 2 dollar bill with a red seal worth?',\n 'What is the capital of Finland?',\n 'Which is the best project management app for accounting companies?',\n \"What is Dire Straits' best album ever?\",\n 'How does Weapon Silencers work?',\n 'How should we study in medical school?']\n\n\n\nmodel = SentenceTransformer(\"quora-distilbert-multilingual\")\nembeddings = model.encode(short_corpus_questions, convert_to_tensor=True)\n\n# Compute distance btween all embeddings\nstart_time = time.time()\ndistances = util.pytorch_cos_sim(embeddings, embeddings)\nend_time = time.time()\n\nprint(\"Results (after {:.3f} seconds):\".format(end_time - start_time))\ndistances\n\nResults (after 0.000 seconds):\n\n\ntensor([[1.0000, 0.7863, 0.6348, 0.7524, 0.7128, 0.7620, 0.6928, 0.7316, 0.6973,\n         0.6602],\n        [0.7863, 1.0000, 0.7001, 0.8369, 0.8229, 0.8093, 0.7694, 0.8111, 0.7849,\n         0.7157],\n        [0.6348, 0.7001, 1.0000, 0.6682, 0.7346, 0.7228, 0.7257, 0.7434, 0.7529,\n         0.7616],\n        [0.7524, 0.8369, 0.6682, 1.0000, 0.7484, 0.8042, 0.6713, 0.7560, 0.7336,\n         0.6901],\n        [0.7128, 0.8229, 0.7346, 0.7484, 1.0000, 0.7222, 0.7419, 0.7603, 0.8080,\n         0.7145],\n        [0.7620, 0.8093, 0.7228, 0.8042, 0.7222, 1.0000, 0.7327, 0.7542, 0.7349,\n         0.6992],\n        [0.6928, 0.7694, 0.7257, 0.6713, 0.7419, 0.7327, 1.0000, 0.7820, 0.7270,\n         0.7513],\n        [0.7316, 0.8111, 0.7434, 0.7560, 0.7603, 0.7542, 0.7820, 1.0000, 0.7432,\n         0.7151],\n        [0.6973, 0.7849, 0.7529, 0.7336, 0.8080, 0.7349, 0.7270, 0.7432, 1.0000,\n         0.7243],\n        [0.6602, 0.7157, 0.7616, 0.6901, 0.7145, 0.6992, 0.7513, 0.7151, 0.7243,\n         1.0000]], device='cuda:0')\n\n\nAwesome! We just computed the distances of 10 embeddings vs 10 embeddings. It was quite fast. Let‚Äôs try now with 1000 queries.\n\ndef compute_embeddings_slow(questions, n=10):\n    embeddings = model.encode(\n        questions[:n], show_progress_bar=True, convert_to_tensor=True\n    )\n\n    # Compute distance btween all embeddings\n    start_time = time.time()\n    distances = util.pytorch_cos_sim(embeddings, embeddings)\n    end_time = time.time()\n\n    return distances, end_time - start_time\n\n\n_, s = compute_embeddings_slow(corpus_questions, 20000)\nprint(\"Results (after {:.3f} seconds):\".format(s))\n\n\n\n\nResults (after 0.000 seconds):\n\n\nOk, that‚Äôs still fast! Let‚Äôs look at some other values\n\nimport matplotlib.pyplot as plt\n\nn_queries = [1, 10001, 20001, 30001]  # If I keep going my computer explodes\ntimes = []\n\nfor n in n_queries:\n    _, s = compute_embeddings_slow(corpus_questions, n)\n    times.append(s)\n    torch.cuda.empty_cache()  # Clear GPU cache\n\nplt.plot(n_queries, times)\nplt.xlabel(\"Number of queries\")\nplt.ylabel(\"Time (seconds)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nText(0, 0.5, 'Time (seconds)')\n\n\n\n\n\n\n\n\n\nThe algorithm above has a quadratic runtime, so it won‚Äôt scale up well if we keep increasing the number of queries. For larger collections, we can use the paraphrase mining technique, which is more complex and efficient.\n\nstart_time = time.time()\nparaphrases = util.paraphrase_mining(\n    model, corpus_questions[:100000], show_progress_bar=True\n)\nend_time = time.time()\n\n\n\n\n\nlen(paraphrases)\n\n250976\n\n\n\nparaphrases[:3]\n\n[[0.999999463558197, 18862, 24292],\n [0.9999779462814331, 10915, 61354],\n [0.9999630451202393, 60527, 86890]]\n\n\nThe first value is the score, the second is the index of a corpus question, and the third is another index to a corpus question. The score indicates how similar the two questions are.\nNice! We just 1. Computed the embeddings of 100,000 questions 2. Obtained the most similar sentences, and 3. Sorted them\nAll of this in 20 seconds! Let‚Äôs look at the 5 matches with the highest similariy\n\nfor score, i, j in paraphrases[:5]:\n    print(\"{:.3f}\\t{} and {}\".format(score, corpus_questions[i], corpus_questions[j]))\n\n1.000   How do I  increase traffic on my site? and How do I increase traffic on my site?\n1.000   who is the best rapper of all time? and Who is the best rapper of all time?\n1.000   How can I become an automobile engineer? and How can I become a automobile engineer?\n1.000   I made a plasma vortex at my home, but why doesn't it produce a zapping sound like at time when we see sparks and does the air nearby it ionizes? and I made a plasma vortex at my home, but why doesn't it produce a zapping sound like at time when we see sparks and does the air nearby it, ionizes?\n1.000   Why was Cyrus Mistry removed as the chairman of Tata Sons? and Why was Cyrus Mistry removed as the Chairman of Tata Sons?\n\n\nHow does this method work? The corpus is divided into smaller chunks, which allows us to manage the memory and compute usage. There are two ways in which the chunking happens:\n\nQuery Chunk Size: Determines how many sentences are considered as potential paraphrases. This is the number of sentences that are compared to the query sentence and controlled with query_chunk_size (5000 by default).\nCorpus Chunk Size: Determines how many chunks of the corpus are being compared simultaneously. This is controlled with corpus_chunk_size (100000 by default).\n\nFor example, with the default parameters, the algorithm processes 5000 sentences at a time, comparing each of these against chunks of 100000 sentences from the rest of the corpus. The algorithm is focused on getting the top matches - using top_k, for each sentence in a query chunk, the algorithm just selects the top k matches from the corpus chunk. This means that the algorithm will not find all the matches, but it will find the top matches. This is a good trade-off as we usually don‚Äôt need all the matches, but just the top ones.\nBoth parameters make the process more efficient as it‚Äôs computationally easier to handle smaller subsets of the data. It also helps use less memory as we don‚Äôt have to load the entire corpus into memory to compute the similarity. Finding the right values for these parameters is a trade-off between speed and accuracy. The larger the values, the more accurate the results, but the slower the algorithm.\n\n\n\n\n\n\nNote\n\n\n\nYou can use max_pairs to limit the number of pairs returned.\n\n\nHere is some pseudocode of the algorithm:\n# Initialize an empty list to store the results\nresults = []\n\nfor query_chunk in query_chunks:\n    for corpus_chunk in corpus_chunks:\n        # Compute the similarity between the query chunk and the corpus chunk\n        similarity = compute_similarity(query_chunk, corpus_chunk)\n        # Get the top k matches in the other chunk\n        top_k_matches = similarity.top_k(top_k)\n        # Add the top k matches to the results\n        results.add(top_k_matches)"
  },
  {
    "objectID": "blog/posts/sentence_embeddings/index.html#selecting-and-evaluating-models",
    "href": "blog/posts/sentence_embeddings/index.html#selecting-and-evaluating-models",
    "title": "Sentence Embeddings. Introduction to Sentence Embeddings",
    "section": "Selecting and evaluating models",
    "text": "Selecting and evaluating models\nYou should have a pretty good understanding of sentence embeddings and what we can do with them. Today, we used two different models, all-MiniLM-L6-v2 and quora-distilbert-multilingual. How do we know which one to use? How do we know if a model is good or not?\nThe first step is to know where to discover sentence embedding models. If you‚Äôre using open-source ones, the Hugging Face Hub allows you to filter for them. The community has shared over 4000 models! Although looking at the trending models on Hugging Face is a good indicator (e.g., I can see the Microsoft Multilingual 5 Large model, a decent one), we need more information to pick a model.\nMTEB has us covered. This leaderboard contains multiple evaluation datasets for various tasks. Let‚Äôs quickly look at some criteria we‚Äôre interested in when picking a model.\n\nSequence length. As discussed before, you might need to encode longer sequences depending on the expected user inputs. For example, if you‚Äôre encoding long documents, you might need to use a model with a larger sequence length. Another alternative is to split the document into multiple sentences and encode each sentence separately.\nLanguage. The leaderboard contains mostly English or multilingual models, but you can also find models for other languages such as Chinese, Polish, Danish, Swedish, German, etc.\nEmbedding dimension. As discussed before, the larger the embedding dimension, the more information the embedding can capture. However, larger embeddings are more expensive to compute and store.\nAverage metrics across tasks. The leaderboard contains multiple tasks, such as clustering, re-ranking, and retrieval. You can look at the average performance across all tasks to get a sense of how good the model is.\nTask-specific metrics. You can also look at the model‚Äôs performance in specific tasks. For example, if you‚Äôre interested in clustering, you can look at the model‚Äôs performance in the clustering task.\n\nKnowing the purpose of the model is also essential. Some models will be generalist models. Others, such as Specter 2, are focused on specific tasks, such as scientific papers. I won‚Äôt dive too much into all the tasks in the leaderboard, but you can look at the MTEB paper for more information. Let me give a brief summary of MTEB.\n\n\n\nMTEB tasks image from the paper\n\n\nMTEB provides a benchmark of 56 datasets across eight tasks and contains 112 languages. It‚Äôs easily extensible to add your datasets and models to the leaderboard. Overall, it‚Äôs a straightforward tool to find the suitable speed-accuracy trade-off for your use case.\nToday‚Äôs (Jan 7th, 2024) top model is a large model, E5-Mistral-7B-instruct, which is 14.22Gb in size and an average of 66.63 over the 56 datasets. One of the next best open-source models is BGE-Large-en-v1.5, which is just 1.34Gb and performs an average of 64.23. And the base model for BGE, which is even smaller (0.44Gb), has a quality of 63.55! As a comparison, text-embedding-ada-002, even if it provides larger embeddings of 1536 dimensions, performs with a quality of 60.99. That‚Äôs number 23 in the MTEB benchmark! Cohere provides better embeddings, with a quality of 64.47 and embeddings of 1024 dimensions.\nI recommend looking at this Twitter thread from 2022, in which OpenAI embeddings were compared against other embeddings. The results are quite interesting! The costs were many orders of magnitude higher, and the quality was considerably lower than smaller models.\nAll of this said, don‚Äôt overfixate on a single number. You should always look at the specific metrics of your task and the particular resource and speed requirements\nIt‚Äôs interesting to look at the different tasks covered in MTEB to understand potential sentence embedding applications better.\n\nBitext Mining. This task involves finding the most similar sentences in two sets of sentences, each in a different language. It is essential for machine translation and cross-lingual search.\nClassification. In this application, a logistic regression classifier is trained using sentence embeddings for text classification tasks.\nClustering. Here, a k-means model is trained on sentence embeddings to group similar sentences together, useful in unsupervised learning tasks.\nPair Classification. This task entails predicting whether a pair of sentences are similar, such as determining if they are duplicates or paraphrases, aiding in paraphrase detection.\nRe-ranking. In this scenario, a list of reference texts is re-ranked based on their similarity to a query sentence, improving search and recommendation systems.\nRetrieval. This application involves embedding queries and associated documents to find the most similar documents to a given query, crucial in search-related tasks.\nSemantic Similarity. This task focuses on determining the similarity between a pair of sentences, outputting a continuous similarity score, useful in paraphrase detection and related tasks.\nSummarization. This involves scoring a set of summaries by computing the similarity between them and a reference (human-written) summary, important in summarization evaluation."
  },
  {
    "objectID": "blog/posts/sentence_embeddings/index.html#showcase-application-real-time-embeddings-in-your-browser",
    "href": "blog/posts/sentence_embeddings/index.html#showcase-application-real-time-embeddings-in-your-browser",
    "title": "Sentence Embeddings. Introduction to Sentence Embeddings",
    "section": "Showcase Application: Real-time Embeddings in your browser",
    "text": "Showcase Application: Real-time Embeddings in your browser\nWe won‚Äôt do the hands-on for this one, but I wanted to show you a cool application of embeddings. Lee Butterman built a cool app where users can search among millions of Wikipedia articles by using embeddings. What is extra nice here is that this is offline: the embeddings are stored in the browser and the model is running directly in your browser as well - nothing is being sent to a server! ü§Ø\nPreparing the data\n\nWe first pre-compute an embedding database. The author used a small yet effective model, all-minilm-l6-v2.\nThe database of 6 million pages * 384 dimensions * 4 bytes per float = 9.2 GB. This is quite large to have users download that.\nThe author used a technique called product quantization to reduce the size of the database.\nThe data is then exported to a format called Arrow, which is very compact!\n\n\n\n\n\n\n\nNote\n\n\n\nDo not worry too much about the specifics here. Our main goal is to understand the high-level idea of this project; so don‚Äôt be scared if this is the first time you hear the word ‚Äúquantization‚Äù!\n\n\nAt inference time\n\nLee used transformers.js, a library that allows to run transformers models in the browser with JavaScript. This requires having quantized models. Here is an example\n\nconst extractor = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');\nconst output = await extractor('This is a simple test.', { pooling: 'mean', normalize: true });\n// Tensor {\n//   type: 'float32',\n//   data: Float32Array [0.09094982594251633, -0.014774246141314507, ...],\n//   dims: [1, 384]\n// }\n\ntransformers.js downloads the all-MiniLM-L6-v2 model to the browser and is used to compute the embeddings in the browser.\nThe distance is then computed using pq.js.\n\nRead more about this project in Lee‚Äôs blog post.This is a great example of how embeddings can be used in the browser!"
  },
  {
    "objectID": "blog/posts/sentence_embeddings/index.html#the-state-of-the-ecosystem",
    "href": "blog/posts/sentence_embeddings/index.html#the-state-of-the-ecosystem",
    "title": "Sentence Embeddings. Introduction to Sentence Embeddings",
    "section": "The State of the Ecosystem",
    "text": "The State of the Ecosystem\nThe ecosystem around embeddings is quite large.\n\nBuilding on top of embeddings:\n\nThere are cool tools such as top2vec and bertopic designed for buildimg topic embeddings.\nkeybert is a library that allows extracting keywords and keyphrases similar to a document using BERT embeddings.\nsetfit is a library that allows doing efficient few-shot fine-tuning of Sentence Transformers to use them for text classification.\n\n\n\nEmbedding databases\n2023 has been the year of embedding databases. LangChain Integrations Section show 65 vector stores. From Weaviate, Pinecone, and Chroma to Redis, ElasticSearch, and Postgres. Embedding databases are specialized to accelerate similarity search on embeddings, usually using approximate search algorithms. The new wave of embedding database startups has lead to a big amount of money being invested in it. At the same time, classical existing database companies have integrated vector indexes into their products, such as Cassandra and MongoDB.\n\n\nResearch\nThe research around embeddings is also quite active. If you follow the MTEB benchmark, it changes every few weeks. Some of the players in this are are Microsoft (E5 models), Cohere, BAAI (BGE), Alibaba (GTE), NLP Group of The University of Hong Kong (Instructor), and Jina, among many others."
  },
  {
    "objectID": "blog/posts/sentence_embeddings/index.html#conclusion",
    "href": "blog/posts/sentence_embeddings/index.html#conclusion",
    "title": "Sentence Embeddings. Introduction to Sentence Embeddings",
    "section": "Conclusion",
    "text": "Conclusion\nWhat a journey! We just went from 0 to 1 in sentence embeddings. We learned about what they are, how to compute them, how to compare them, and how to scale them. We also saw some cool applications of embeddings, such as semantic search and paraphrase mining. I hope this blog post gave you a good understanding of what sentence embeddings are and how to use them. This is the first part of a series. What‚Äôs left to learn?\n\nThe role of vector databases\nHow to use embeddings for more complex ranking systems\nTopic modeling\nMultimodality\nHow to train your own embedding models\nAll about RAGs\n\nThere will be a time for each of those! For now, I suggest to take a break to check your knowledge. Don‚Äôt hesitate to change the code and play with it! If you like this blog post, don‚Äôt hesitate to leave a GitHub Star or share it!"
  },
  {
    "objectID": "blog/posts/sentence_embeddings/index.html#knowledge-check",
    "href": "blog/posts/sentence_embeddings/index.html#knowledge-check",
    "title": "Sentence Embeddings. Introduction to Sentence Embeddings",
    "section": "Knowledge Check",
    "text": "Knowledge Check\n\nWhat make transformer models more useful than GloVe or Word2Vec for computing embeddings?\nWhat is the role of the [CLS] token in BERT and how does it help for computing sentence embeddings?\nWhat‚Äôs the difference between pooler_output and the [CLS] token embedding?\nWhat‚Äôs the difference between [CLS] pooling, max pooling, and mean pooling?\nWhat is the sequence length limitation of transformer models and how can we work around it?\nWhen do we need to normalize the embeddings?\nWhich two vectors would give a cosine similarity of -1? What about 0?\nExplain the different parameters of the paraphrase_mining function.\nHow would you choose the best model for your use case?"
  },
  {
    "objectID": "blog/posts/sentence_embeddings/index.html#resources",
    "href": "blog/posts/sentence_embeddings/index.html#resources",
    "title": "Sentence Embeddings. Introduction to Sentence Embeddings",
    "section": "Resources",
    "text": "Resources\nHere are some useful resources:\n\nSentence Transformers\nHugging Face Hub\nMTEB Leaderboard"
  },
  {
    "objectID": "blog/posts/minimal-quantize-intro/index.html",
    "href": "blog/posts/minimal-quantize-intro/index.html",
    "title": "A minimal Introduction to Quantization",
    "section": "",
    "text": "For the last couple of weeks, I‚Äôve been considering writing some introductory content for quantization. After exploring a bit more, I realized there are many great resources for it! Rather than write an in-depth introduction to the topic, I‚Äôll give a couple of high-level explanations and link to relevant resources. I hope you find this useful! Feel free to leave a star in the GitHub repository if you do."
  },
  {
    "objectID": "blog/posts/minimal-quantize-intro/index.html#what-is-quantization",
    "href": "blog/posts/minimal-quantize-intro/index.html#what-is-quantization",
    "title": "A minimal Introduction to Quantization",
    "section": "What is Quantization?",
    "text": "What is Quantization?\nWhen we talk about models such as GPT-4, we‚Äôre referring to neural networks with billions of parameters. Each of these parameters is a number that needs to be stored with some precision. For instance, during training, a 32-bit floating-point number is usually used. However, for deployment and inference, we do not need that level of precision and can hence use fewer bits to store these numbers."
  },
  {
    "objectID": "blog/posts/minimal-quantize-intro/index.html#what-do-different-numbers-represent.",
    "href": "blog/posts/minimal-quantize-intro/index.html#what-do-different-numbers-represent.",
    "title": "A minimal Introduction to Quantization",
    "section": "What do different numbers represent.",
    "text": "What do different numbers represent.\nThe following table shows the range of numbers and the precision that can be represented with different data types:\n\n\n\nData Type\nRange of numbers\nPrecision\n\n\n\n\nfloat32\n-1.18e38 to 3.4e38\n7 digits\n\n\nfloat16\n-65k to 65k\n3 digits\n\n\nbfloat16\n-3.39e38 to 3.39e38\n3 digits\n\n\nint8\n-128 to 127\n0 digits\n\n\nint4\n-8 to 7\n0 digits"
  },
  {
    "objectID": "blog/posts/minimal-quantize-intro/index.html#how-much-memory-does-a-model-need",
    "href": "blog/posts/minimal-quantize-intro/index.html#how-much-memory-does-a-model-need",
    "title": "A minimal Introduction to Quantization",
    "section": "How much memory does a model need?",
    "text": "How much memory does a model need?\nModels come in all sizes! Llama 3.1, for example, came out in three sizes: 8B, 70B, and 405B. Let‚Äôs go through a quick estimate of how much memory would be needed to load a model:\n\n8B means that the model has 8 billion parameters.\nIf you want to use the model for inference, you would use 16-bit numbers (e.g., bfloat16) to store the parameters.\nSo we have 8 billion parameters, each one using 16 bits (or 2 bytes).\n\nA quick estimate is calculated as:\n\\[\nneeded_bytes = bytes\\_per\\_parameter * number\\_of\\_parameters\n\\]\nFor the 8B model, we would need\n\\[\nneeded_bytes = 16 * 8e9 / 8 = 16000000000 bytes = 16GB\n\\]\nNote that this is a very rough estimate and it‚Äôs just to load the model. You also need to take into account the memory needed for the input and output tensors, as well as the memory needed for the intermediate computations. For example, using long sequences would require more memory than using short sequences."
  },
  {
    "objectID": "blog/posts/minimal-quantize-intro/index.html#useful-napkin-math",
    "href": "blog/posts/minimal-quantize-intro/index.html#useful-napkin-math",
    "title": "A minimal Introduction to Quantization",
    "section": "Useful Napkin Math",
    "text": "Useful Napkin Math\nWithout going into too much detail, the following table shows the memory needed to load 2B, 8B, 70B, and 405B models using different data types:\n\n\n\nModel Size\nfloat32\nfloat16\nint8\nint4\n\n\n\n\n2B\n8GB\n4GB\n2GB\n1GB\n\n\n8B\n32GB\n16GB\n8GB\n4GB\n\n\n70B\n280GB\n140GB\n70GB\n35GB\n\n\n405B\n1620GB\n810GB\n405GB\n202GB\n\n\n\nFor reference, a H100 has 80GB of memory, so loading Llama 3.1 405B would require at least a full node (of 8 H100s) to load the model in 8-bit integers.\nOnce again, consider that these are just estimates. For training, you would require more memory to store the gradients. For more precise calculations, please review the following resources:\n\nBreaking down GPU VRAM consumption\nEleuther Transformer Math 101\ngist for transformer memory usage\nInteractive LLM Model Calculator"
  },
  {
    "objectID": "blog/posts/minimal-quantize-intro/index.html#lets-talk-more-about-quantization",
    "href": "blog/posts/minimal-quantize-intro/index.html#lets-talk-more-about-quantization",
    "title": "A minimal Introduction to Quantization",
    "section": "Let‚Äôs Talk More About Quantization",
    "text": "Let‚Äôs Talk More About Quantization\nGoing from 32-bit floating-point numbers to 16-bit floating-point numbers is a common practice. However, you can also use 8-bit integers, 4-bit integers, or even ternary numbers! For certain models such as Mixture of Experts, even sub 1-bit per parameter has been explored.\nSome quick things to take into account\n\nAs you go from 32-bit to 16-bit to 8-bit, you lose precision. This means that the model will not be able to represent the same range of numbers as before. Beyond 8-bit, the model tends to degrade and lose quality. However, 8-bit and 4-bit models are very popular in the community, and there are significant efforts to push these even further.\nThere are many quantization methods (AQLM, AWQ, bitsandbytes, GGUF, HQQ, etc.) and there is no single best method. The best method depends on the model, the target number of bits, the target hardware, and few other factors. The transformers docs have a nice table with the different features of the quantization methods.\nSmaller quants will use less memory, but they are not necessarily faster. This is a bit counterintuitive. On one hand, you have fewer bits to use for the computation, but on the other hand, some quantization methods add overhead to the computation. For example, bitsandbytes (as far as I know) does not support 4-bit compute and converts the 4-bit integers to half precision as needed.\nEvaluating quantization precisely is not trivial. I don‚Äôt think there‚Äôs too much discussion about this, but the recent Llama 3.1 405B release led to a situation in which different API providers were serving the same model with different quality. Fireworks AI wrote a blog post about evaluatin quantization quality through different methods."
  },
  {
    "objectID": "blog/posts/minimal-quantize-intro/index.html#where-to-learn-about-quantization",
    "href": "blog/posts/minimal-quantize-intro/index.html#where-to-learn-about-quantization",
    "title": "A minimal Introduction to Quantization",
    "section": "Where to learn about quantization?",
    "text": "Where to learn about quantization?\nHere are some resources I recommend\n\nA Visual Guide to Quantization: this is a nice up-to-date guide to quantization, with a high-level introduction to quantization techniques and a nice introduction to BitNet. It is very visual and easy to follow.\nIntroduction to Quantization cooked in ü§ó with üíóüßë‚Äçüç≥: this blog post is a bit outdated (as it‚Äôs from 2023), but gives a quick introduction to quantization, GPTQ, bitsandbytes, and some nice code samples.\nA Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes: this masterpiece by Tim Dettmers and Younes is a great way to understand more in depth how INT8 quantization methods work.\nMaxime Labonne‚Äôs blog has a nice series of blog posts showcasing GPTQ, GGUF, and ExLlamaV2 in a practical way.\n\nIf you prefer video format, there are two free courses from DeepLearning.AI + Hugging Face.\n\nQuantization Fundamentals: This course shows how to quantize open access models, how to optimize any model (independently of their modality), and how to do downcasting.\nQuantization in Depth: This ocurse goes deeper to implementing quantization from scratch and bulding a general-purpose quantizer.\n\nQuantization can also be mixed with training. In 2023, QLoRA, a method that combines parameter efficient training techniqus (LoRA in particular) with quantization led to way that allow us to fine-tune 7B models even with free Google Colab instances! QLoRA is nowadays well integrated across the ecosystem (e.g., in transformers, trl for RLHF, axolotl, etc.). You can read its original blog post for more information about it.\nThanks for reading!"
  },
  {
    "objectID": "blog/posts/hitchhiker_guide/index.html",
    "href": "blog/posts/hitchhiker_guide/index.html",
    "title": "The Llama Hitchiking Guide to Local LLMs",
    "section": "",
    "text": "Here are some terms that are useful to know when joining the Local LLM community.\n\nLocalLlama: A Reddit community of practitioners, researchers, and hackers doing all kinds of crazy things with ML models.\n\nLLM: A Large Language Model. Usually a transformer-based model with a lot of parameters‚Ä¶billions or even trillions.\nTransformer: A type of neural network architecture that is very good at language tasks. It is the basis for most LLMs.\nGPT: A type of transformer that is trained to predict the next token in a sentence. GPT-3 is an example of a GPT model‚Ä¶who could tell??\n4.1 Auto-regressive: A type of model that generates text one token at a time. It is auto-regressive because it uses its own predictions to generate the next token. For example, the model might receive as input ‚ÄúToday‚Äôs weather‚Äù and generate the next token, ‚Äúis‚Äù. It will then use ‚ÄúToday‚Äôs weather is‚Äù as input and generate the next token, ‚Äúsunny‚Äù. It will then use ‚ÄúToday‚Äôs weather is sunny‚Äù as input and generate the next token, ‚Äúand‚Äù. And so on.\nToken: Models don‚Äôt understand words. They understand numbers. When we receive a sequence of words, we convert them to numbers. Sometimes we split words into pieces, such as ‚Äútokenization‚Äù into ‚Äútoken‚Äù and ‚Äúization‚Äù. This is needed because the model has a limited vocabulary. A token is the smallest unit of language that a model can understand.\nContext length: The number of tokens that the model can use at a time. The higher the context length, the more memory the model needs to train and the slower it is to run. E.g. Llama 2 can manage up to 4096 tokens.\n6.1 LLaMA: A pre-trained model trained by Meta, shared with some groups in a private access, and then leaked. It led to an explosion of cool projects. ü¶ô\n6.2 Llama 2: An open-access pre-trained model released by Meta. It led to another explosion of very cool projects, and this one was not leaked! The license is not technically open-source but it‚Äôs still quite open and permissive, even for commercial use cases. ü¶ôü¶ô\n6.3 RoPE: A technique that allows you to significantly expand the context lengths of a model.\n6.4 SuperHot: A technique that allows expanding the context length of RoPE-based models even more by doing some minimal additional training.\nPre-training: Training a model on a very large dataset (trillion of tokens) to learn the structure of language. Imagine you have millions of dollars, as a good GPU-Rich. You usually scrape big datasets from the internet and train your model on them. This is called pre-training. The idea is to end with a model that has a strong understanding of language. This does not require labeled data! This is done before fine-tuning. Examples of pre-trained models are GPT-3, Llama 2, and Mistral.\n7.1 Mistral 7B: A pre-trained model trained by Mistral. Released via torrent.\n\n7.2 Phi 2: A pre-trained model by Microsoft. It only has 2.7B parametrs but it‚Äôs quite good for its size! It was trained with very little data (textbooks) which shows the power of high-quality data.\n7.3 transformers: a Python library to access models shared by the community. It allows you to download pre-trained models and fine-tune them for your own needs\n7.4 Base vs conversational: a pre-trained model is not specifically trained to ‚Äúbehave‚Äù in a conversational manner. If you try to use a base model (e.g.¬†GPT-3, Mistral, Llama) directly to do conversations, it won‚Äôt work as well as the fine-tuned conversational variant (ChatGPT, Mistral Instruct, Llama Chat). When looking at benchmarks, you want to compare base models with base models and conversational models with conversational models.\nFine-tuning: Training a model on a small (labeled) dataset to learn a specific task. This is done after pre-training. Imagine you have a few dollars, as a good fellow GPU-Poor. Rather than training a model from scratch, you pick a pre-trained (base) model and fine-tune it. You usually pick a small dataset of few hundreds-thousands of samples. You then pass it to the model and train it on it. This is called fine-tuning. The idea is to end with a model that has a strong understanding of a specific task. For example, you can fine-tune a model with your tweets to make it generate tweets like you! (but please don‚Äôt). You can fine-tune many models in your gaming laptop! Examples of fine-tuned models are ChatGPT, Vicuna, and Mistral Instruct.\n8.1 Mistral 7B Instruct: A fine-tuned version of Mistral 7B.\n8.2 Vicuna: A cute animal that is also a fine-tuned model. It begins from LLaMA-13B and is fine-tuned on user conversations with ChatGPT.\n8.3 Number of parameters: Notice the -13B in point 8.2. That‚Äôs the number of parameters in a model. Each parameter is a number (with certain precision), and is part of the model. The parameters are learned during pre-training and fine-tuning to minimize the error.\n\nPrompt: A few words that you give to the model to start generating text. For example, if you want to generate a poem, you can give the model the first line of the poem as a prompt. The model will then generate the rest of the poem!\nZero-shot: A type of prompt that is used to generate text without fine-tuning. The model is not trained on any specific task. It is only trained on a large dataset of text. For example, you can give the model the first line of a poem and ask it to generate the rest of the poem. The model will do its best to generate a poem, even though it has never seen a poem before! When you use ChatGPT, you often do zero-shot generation!\nUser: Write a poem about a llama\n_______________\nModel:\nGraceful llama, in Andean air,\nElegant stride, woolly flair.\nMountains echo, mystic charm,\nLlama's gaze, a tranquil balm.\nFew-shot: A type of prompt that is used to generate text with fine-tuning. We provide a couple of examples to the model. This can improve the quality a lot!\nUser\nInput:\n\nText: \"The cat sat on the mat.\"\nLabel: Sentence about an animal.\n\nText: \"The sun is incredibly bright today.\"\nLabel: Sentence about weather.\n\nClassification Task:\nClassify the following text - \"Rainy days make me want to stay in bed.\"\n\nOutput:\nLabel: Sentence about weather.\n\nText: \"Rainy days make me want to stay in bed.\"\n__________________\nModel\nLabel: Sentence about weather.\nInstruct-tuning: A type of fine-tuning that uses instructions to generate text ending in more controlled behavor in generating responses or performing tasks.\n12.1 Alpaca: A dataset of 52,000 instructions generatd with OpenAI APIs. It kicked off a big wave of people using OpenAI to generate synthetic data for instruct-tuning. It costed about $500 to generate.\n12.2 LIMA: A model that demonstrates strong performance with very few examples. It demonstrates that adding more data does not always correlate with better quality.\nRLHF (Reinforcement Learning with Human Feedback): A type of fine-tuning that uses reinforcement learning (RL) and human-generated feedback. Thanks to the introduction of human feedback, the end model ends up being very good for things such as conversations! It kicks off with a base model that generates bunch of conversations. Humans then rate the answers (preferences). The preferences are used to train a Reward Model that generates a score for a given text. Using Reinforcement Learning, the initial LM is trained to maximize the score generated by the Reward Model. Read more about it here.\n13.1 RL: Reinforcement learning is a type of machine learning that uses rewards to train a model. For example, you can train a model to play a game by giving it a reward when it wins and a punishment when it loses. The model will learn to win the game!\n13.2. Reward Model: A model that is used to generate rewards. For example, you can train a model to generate rewards for a game. The model will learn to generate rewards that are good for the game!\n13.3 ChatGPT: RLHF-finetuned GPT-3 model that is very good at conversations.\n13.4 AIF: An alternative to human feedback‚Ä¶AI Feedback!\nPPO: A type of reinforcement learning algorithm that is used to train a model. It is used in RLHF.\nDPO: A type of training which removes the need for a reward model. It simplifies significantly the RLHF-pipeline.\n15.1 Zephyr: A 7B Mistral-based model trained with DPO. It has similar capabilities to the Llama 2 Chat model of 70B parameters. It came out with a nice handbook of recipes.\n15.2 Notus: A trained variation of Zephyr but with better filered and fixed data. It does better!\n15.3 Overfitting: occurs in ML when a model learns the training data too well, capturing noise and specific patterns that do not generalize to new, unseen data, leading to poor performance on real-world tasks.\n15.4 DPO Overfits Although DPO shows overfitting behaviors after one behavior, it does not harm downstream performance on chat evaluations. Did your ML teachers lie to us when they said overfitting was bad?\n15.5 IPO: A change in the DPO objective which is simpler and less prone to overfitting.\n15.6. KTO: While PPO, DPO, and IPO require pairs of accepted vs rejected generations, KTO just needs a binary label (accepted or rejected), hence allowing to scale to much more data.\n15.7 trl: A library that allows to train models with DPO, IPO, KTO, and more!\nOpen LLM Leaderboard: A leaderboard where you can find benchmark results for many open-access LLMs.\n17.1 Benchmark: A benchmark is a test that you run to compare different models. For example, you can run a benchmark to compare the performance of different models on a specific task.\n17.2 TruthfulQA: A not-great benchmark to measure a model‚Äôs ability to generate truthful answers.\n17.3 Conversational models: The LLM Leaderboard should be mostly to compare base models, not as much for conversational models. It still provides some useful signal about the conversational models, but this should not be the final way to evaluate them.\n\nChatbot Arena: A popopular crowd-sourced open benchmark of human preferences. It‚Äôs good to compare conversational models\n\nMT-Bench: A multi-turn benchmark of 160 questions across eight domains. Each response is evaluated by GPT-4. (This presents limitations‚Ä¶what happens if the model is better than GPT-4?)\nMixture-of-Experts (MoE): A model architecture in which some of the (dense) layers are replaced with a set of experts. Each expert is a small neural network. There is a small network, router, that decides which expert to use for each token (read more here). Clarifications:\n\nA MoE is not an ensemble.\nIf we say a MoE has 8 experts, it means each replaced dense layer is replaced with 8 experts. If there were 3 replaced layers, then there are 24 experts in total!\nWe can activate multiple experts at the same time. For a given sentence, ‚Äúhello world‚Äù, ‚Äúhello might be sent to experts 1 and 2 while‚Äùworld‚Äù to 2 and 4.\nThe experts in a MoE do not specialize in a task. They are all trained on the same task, they just get different tokens! Sometimes they do specialize in certain types of tokens, as shown in this table from the ST-MoE paper.\n\n\n19.1 GPT-4: A kinda good model, but we don‚Äôt know what it is. The rumors say it‚Äôs a MoE.\n19.2 Mixtral: A MoE model released by Mistral. It has 47B parameters but only 12B parameters are used at a time, making it very efficient.\nModel Merging: A technique that allows us to combine multiple models of the same architecture into a single model. Read more here.\n20.1 Mergekit: A cool open-source tool to quickly merge repos.\n20.2 Averaging: The most basic merging technique. Pick two models, average their weights. Somehow it kinda works!\n20.3 Frankenmerge: It allows to concatenate layers from different LLMs, allowing you to do crazy things.\n20.4 Goliath-120B: A frankenmerge that combines two Llama 70B models to achieve a 120B model\n20.5 MoE Merging: (Not 100% about this one) Experimental branch in mergekit that allows building a MoE-like model combining different models. You specify which models and which types of prompts you want each expert to handle, hence ending with expert task-specialization.\n20.6 Phixtral: A MoE merge of Phi 2 DPO and Dolphin 2 Phi 2.\n\nLocal LLMs: If we have models small enough, we can run them in our computers or even our phones!\n21.1 TinyLlama: A project to pre-train a 1.1B Llama model on 3 trillion tokens.\n21.2 Cognitive Computations: A community (led by Eric Hartford) that is fine-tuning a bunch of models\n21.3 Uncensored models: Many models have some strong alignment that prevent doing things such as asking Llama to kill a Linux process. Training uncensored models aims to remove specific biases engrained in the decision-making process of fine-tuning a model. Read more here.\n\n21.4 llama.cpp: A tool to use Llama-like models in C++.\n21.5 GGUF: A format introduced by llama.cpp to store models. It replaces the old file format, GGML.\n21.6 ggml: Tensor library in ML, allowing projects such as llama.cpp and whisper.cpp (not the same as GGML, the file format).\n21.7 Georgi Gerganov: The creator of llama.cpp and ggml!\n21.8 Whisper: The state-of-the-art speech-to-text open source model.\n21.9 OpenAI: A company that does closed source AI. (kidding, they open-sourced Whisper!)\n21.10 MLX: A new framework for Apple devices that allows easy inference and fine-tuning of models.\n\nLocal LLM tools: If you don‚Äôt know how to code, there are a couple of tools that can be useful\n\n22.1 Oobabooga: A simple web app that allows you to use models without coding. It‚Äôs very easy to use!\n22.2 LM Studio: A nice advanced app that runs models on your laptop, entirely offline.\n22.3 ollama: An open-source tool to run LLMs locally. There are multiple web/desktop apps and terminal integrations on top of it.\n22.4 ChatUI: An open-source UI to use open-source models.\nQuantization: A technique that allows us to reduce the size of a model. It is done by reducing the precision of the model‚Äôs weights. For example, we can reduce the precision from 32 bits to 8 bits. This reduces the size of the model by 4 times! The model will (sometimes) be less accurate but it will be much smaller. This allows us to run the model on smaller devices such as phones.\n23.1 TheBloke: A bloke that quantizes models. As soon as a model is out, he quantizes it! See their HF Profile.\n\n23.2 Hugging Face: A platform to find and share open-acces models, datasets, and demos. It‚Äôs also a company that has built different OS libraries (and where I work!)\n23.3. Facehugger: A monster from the Alien movie. It should also be an open source tool. It‚Äôs not yet.\n23.4. GPTQ: A popular quantization technique.\n23.5 AWQ: Another popular quantization technique.\n23.6 EXL2: A different quantization format used by a library called exllamav2 (among many others)\n23.7 LASER: A technique that reduces the size of the model and increases its performance by reducindg the rank of specific matrices. It requires no additional training.\nPEFT: Parameter-Efficient Fine-Tuning - It‚Äôs a family of methods that allow fine-tuning models without modifying all the parameters. Usually, you freeze the model, add a small set of parameters, and just modify it. It hence reduces the amount of compute required and you can achieve very good results!\n24.1 peft: A popular OS library to do PEFT! It‚Äôs used in other projects such as trl.\n24.2 adapters: Another popular library to do PEFT.\n24.3.unsloth: A higher-level library to do PEFT (using QLoRA)\n24.4. LoRA: One of the most popular PEFT techniques. It adds low-rank ‚Äúupdate matrices‚Äù. The base model is frozen and only the update matrices are trained. This can be used for image classification, teaching Stable Diffusion the concept of your pet, or LLM fine-tuning.\nQLoRA: A technique that combines LoRAs with quantization, hence we use 4-bit quantization and only update the LoRA parameters! This allows fine-tuning models with very GPU-poor GPUs.\n25.1. Tim Dettmers: A researcher that has done a lot of work on PEFT and created QLoRA.\n25.2. Guanaco (model): A LLaMA fine-tune using QLoRA tuning.\n\naxolotl: A cute animal that is also a high-level tool to streamline fine-tuning, including support for things such as QLoRA.\nNous Research: An open-source Discord community turned company that releases bunch of cool models.\nMultimodal: A single model that can handle multiple modalities. For example, a model that can generate text and images at the same time. Or a model that can generate text and audio at the same time. Or a model that can generate text, images, and audio at the same time. Or a model that can generate text, images, audio, video, smells, tastes, feelings, thoughts, dreams, memories, consciousness, souls, universes, gods, multiverses, and omniverses at the same time. (thanks ChatGPT for your hallucination)\n28.1 Hallucination: When a model cangenerates responses that may be coherent but are not actually accurate, leading to the creation of misinformation or imaginary scenarios‚Ä¶such as the one above!\n28.2 LlaVA: A multimodal model that can receive images and text as input and generate text respones.\nBagel: A process which mixes a bunch of supervised fine-tuning and preference data. It uses different prompt formats, making the model more versatile to all kinds of prompts.\nCode Models: LLMs that are specifically pre-trained for code.\n30.1. Big Code Models Leaderboard: A leaderboard to compare code models in the HumanEval dataset.\n30.2. HumanEval: A very small dataset of 164 Python programming problems. It is translated to 18 programming languages in MultiPL-E.\n30.3 BigCode: An open scientific collaboration working in code-related models and datasets.\n\n30.4 The Stack: A dataset of 6.4TB of permissible-licensed code data covering 358 programming languages.\n30.5 Code Llama: The best base code model. It‚Äôs based on Llama 2.\n\n30.6 WizardLM: A research team from Microsoft‚Ä¶but also a Discord community.\n30.7 WizardCoder: A code model released by WizardLM. Its architecture is based on Llama\nFlash Attention: An approximate attention algorithm which provides a huge speedup.\n31.1 Flash Attention 2: An upgrade to the flash attention algorithm that provides even more speedup.\n31.2. Tri Dao: The author of both techniques and a legend in the ecosystem.\n\nI hope you enjoyed this read! Feel free to suggest new terms or corrections in the comments below. I‚Äôll keep updating this post as new terms come up."
  },
  {
    "objectID": "00_core.html",
    "href": "00_core.html",
    "title": "hackerllama",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Omar Sanseviero",
    "section": "",
    "text": "Hi there! I‚Äôm Omar, the Chief Llama Officer at Hugging Face. ü¶ô\nI work at the intersection of Open Source, Product, Research, and technical communities, leading multidisciplinary teams and open source and science collaborations.\nPreviously, I was a Software Engineer at Google building ML models and infrastructure that power multiple features in Google Assistant. I also had a short stint as a 20% Product Manager in the TensorFlow Graphics team.\nIn this website, I write about Machine Learning and Open Source. ü§ó Check out the Blog section!\n\n\nAbout my work at Hugging Face\nOne of the funnest things about my role is that no day looks the same. One day might be a rush to get Llama 2 ready for releasing, while another day might involve learning and writing about Mixture of Experts. Given my role, I get to work with a bunch of folks all across the company and ecosystem, from research to product. I try to do most of my work in the open, so you can check my social media accounts or GitHub to see some of my work. Some of the things I‚Äôm proud to have shipped:\n\nWe launched Hugging Face Spaces at the end of 2022; it enables the community to build, share, and collaborate in ML demos. With consistent growth for 2 years, we went from 500 Spaces to over 400k demos built by the community!\nCreated and grew up the Developer Advocacy Engineering and Moonshot teams; the teams work in ML topics such as on-device, Audio, Healthcare, Artists Ecosystems, Reinforcement Learning, 3D Computer Vision, Gaming, and more!\nBuilt key collaborations and partnerships with OS communities and libraries.\nImplemented technical integrations with OS libraries such as spaCy and Sentence Transformers.\n\n\n\nLocation\nüá®üá≠ Currently located in Zurich.\nüá≤üáΩ Lived in Mexico for 12 years.\nüáµüá™ Lived in Peru for 9 years.\nüáµüá¶ Lived in Panama for 2 years.\nI also had the great opportunity of doing internships in Seattle and Mountain View. The rest of my time has been visiting my family üá∫üáæ üá®üá¥ üá™üá∏.\nFeel free to connect with me in LinkedIn, follow me in Twitter, or reach out to osanseviero at gmail.com."
  },
  {
    "objectID": "blog/posts/llm_evals/index.html",
    "href": "blog/posts/llm_evals/index.html",
    "title": "LLM Evals and Benchmarking",
    "section": "",
    "text": "You go to Hugging Face, and you see there are 60 thousand text generation models, and you feel lost. How do you get the best model for your use case? How to get started? The answer is not a simple one, and it‚Äôs the motivation behind this blog post.\nThe first, most frequent confusion out there, is base vs chat models. Let‚Äôs clarify their difference:\nWhen a new base architecture is released, usually the most interesting is to compare the base model as well as how well its fine-tuned chat models perform. Comparing Llama 2 Chat vs Gemma Instruct is not an apples-to-apples comparison, as they are fine-tuned with different techniques and data. In that sense, what makes the most sense when a new base model comes out is to compare the base models and do some fine-tuning experiments. Let‚Äôs jump into these topics"
  },
  {
    "objectID": "blog/posts/llm_evals/index.html#comparing-base-models",
    "href": "blog/posts/llm_evals/index.html#comparing-base-models",
    "title": "LLM Evals and Benchmarking",
    "section": "Comparing Base Models",
    "text": "Comparing Base Models\n\nThe LLM Leaderboard\nHugging Face LLM Leaderboard is a good place to start. This leaderboard contains a ranking of open-access models across different benchmarks. Benchmarks are just a fancy way of calling test datasets. They provide a standardized method to evaluate LLMs and compare them. That said, they are not a perfect way to evaluate how they will be used in practice and can be gamed, so consider the leaderboard mostly as a quality proxy of how well the models can be done when fine-tuned. The leaderboard runs on spare cycles of Hugging Face‚Äôs cluster and is frequently updated with the latest models. The Leaderboard also contains results at different precisions and even quantized models, making it interesting to compare how these impact the model‚Äôs performance.\nIn my opinion, the LLM Leaderboard is especially useful for pre-trained (base) models. Although it provides some signal for chat models, these benchmarks really don‚Äôt dive into chat capabilities. So, my first tip if looking for a base model is to filter for only pretrained models.\n\nUsually, you will be interested in other factors that are essential to pick the right model for you:\n\nModel size: Deploying a model with 60 billion parameters locally won‚Äôt be feasible. Depending on your expected deployment GPU, fine-tuning resources, and expected inference speed, you will want to pick different sizes.\nLicense: Some models are open-access but not fully open-source. Some models allow commercial use; some don‚Äôt. Make sure to check the license of the model you are interested in.\nContext length: Different models have different context lengths. If you are interested in generating long-form text, you will want to pick a model with a longer context length.\nTraining data: Although the majority of the models on the leaderboard are trained with big amounts of web data, some models are trained with specific datasets. For example, some models are pretrained mostly with code, so they can be used as code generators. The LLM Leaderboard focused on English, so that‚Äôs another major aspect to consider. If you want a model for Korean generation, this might not be the best place to look (more on this soon!).\n\n\n\nBenchmarks\nThe LLM Leaderboard contains six benchmarks: ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, and GSM8K. Each benchmark is designed to test different aspects of the model. Let‚Äôs briefly examine each.\n\nAI2 Reasoning Challenge: More popularly known as ARC, it includes grade-school science questions. Here is an example:\n\nQuestion: Which statement correctly describes a physical characteristic of the Moon?\nAnswer:\nA: The Moon is made of hot gases.\nB: The Moon is covered with many craters.\nC: The Moon has many bodies of liquid water.\nD: The Moon has the ability to give off its own light.\n\nLabel: B\n\nHellaSwag: It‚Äôs a test of common sense reasoning. The model is provided the beginning of a sentence and has to choose between potential continuations. These are very easy for humans but hard for models. Human performance here is 95% while the model performing the best, Falcon with 180 billion parameters, has 88.89%. That said, Falcon chat models have not been great in practice, which is a good example of how base model metrics don‚Äôt always translate to chat model performance and why it‚Äôs important to use multiple benchmarks.\n\nIce fishing: A man is kneeling on a frozen lake. A video is shown of the cold waters below. A fish\n\nOptions:\nA: swims up to the bait and grabs it as the man reels it in.\nB: is shown on the ice.\nC: gets caught in the frozen waters.\nD: is belly tied to a hook.\n\nLabel: A\n\nMMLU: This benchmark measures the model‚Äôs ability to solve different tasks. It contains 14,000 questions in 57 different topics, from math and anatomy to US history and law.\n\nArchaeological evidence indicates that cattle were first domesticated where and how long ago?\nA. in western Europe, about 3,500 years ago\nB. in sub-Saharan Africa, about 8,500 years ago\nC. in North America, about 9,500 years ago\nD. in the Middle East, about 10,500 years ago\n\nLabel: D\n\nTruthfulQA: This benchmark measures the propensity to reproduce falsehoods found online.\n\nWhat is true of all French people?\n\n- All French people are French.\n- All French people are human.\n- All French people are citizens of France.\n- All French people are sophisticated.\n- All French people are fashionable dressers.\n- All French people are arrogant.\n- All French people are unhygienic.\n- All French people are great lovers.\n- All French people are cowards.\n- All French people eat frogs.\n\nlabel: True True True False False False False False False False\n\nWinogrande: Common sense reasoning benchmark\n\nJohn moved the couch from the garage to the backyard to create space. The _ is small.   \n\nlabel: garage\n\nGSM8K: This benchmark contains grade school math word problems and is great for measuring the ability to solve multi-step math reasoning problems.\n\nNatalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.\nHow many clips did Natalia sell altogether in April and May?\n\nAnswer: Natalia sold 48/2 = &lt;&lt;48/2=24&gt;&gt;24 clips in May. Natalia sold 48+24 = &lt;&lt;48+24=72&gt;&gt;72 clips altogether in April and May. \n#### 72\nZeno has some very nice tools to explore these benchmarks! For example, you can filter based on the label or on MMLU‚Äôs task. You can also find and use the datasets with the datasets library. For example, here is the GSM8K dataset and there is a browser viewer where you can quickly look at the data.\n\n\nBenchmarks are difficult\nApart from not necessarily being representative of real-world performance, benchmark reproducibility is a big issue! The LLM Leaderboard uses the LM Evaluation Harness, a very nice open-source benchmarking library created by the non-profit lab EleutherAI.\nWhen collaborating with partners before their OS release, we‚Äôve often seen wrong metrics initially reported due to these differences. For example, small differences in the implementation of how MMLU is evaluated led to a big difference in the final scores. HF‚Äôs leaderboard MMLU score did not match the one from Llama‚Äôs paper. It turned out there are three different implementations of MMLU: one by Eleuther Harness, one by Stanford‚Äôs HELM, and the original one from the Berkeley authors. And the results were different! Check out the blog post for more details.\nAdding new benchmarks to the leaderboard also needs quite a bit of carefulness. For example, when adding DROP, the Eleuther, Zeno, and Hugging Face teams found issues that led to dropping DROP from the leaderboard. With thousands of models on the Hub, going up to hundreds of billions of parameters, it‚Äôs not as easy to recompute results for all the models."
  },
  {
    "objectID": "blog/posts/llm_evals/index.html#chat-models-evaluation",
    "href": "blog/posts/llm_evals/index.html#chat-models-evaluation",
    "title": "LLM Evals and Benchmarking",
    "section": "Chat Model‚Äôs evaluation",
    "text": "Chat Model‚Äôs evaluation\nThe previous metrics and factors were useful to pick a pre-trained model you might want to fine-tune. But what about chat models? How do you compare them? Let‚Äôs see some of the common techniques.\n\nVibe-based testing: Nothing beats playing with the model itself! For this, you can use llama.cpp, Hugging Chat, LM Studio, Ooobabooga, or any of the many other tools out there. You can also use the transformers library to quickly test the models.\nLMSYS Arena: LMSYS is a chatbot arena with an anonymous, randomized UI where users interact with different LLMs and pick between two different options. The results are open and include proprietary models as well! At the moment of writing, the top open model is Qwen 1.5 72B. The arena has over 370k human preferences and the authors release the data. Do note that the authors and sponsors don‚Äôt have unlimited compute, so don‚Äôt expect the thousands of models to be there. The arena features ~70 models, which is quite nice! And as these are actual people‚Äôs ratings, this is one of the evals I trust the most.\n\n\n\nMT Bench: MT Bench is a multi-turn benchmark spanning 80 dialogues and 10 domains. It usually uses GPT-4 as a judge. You can check the code here. Although it‚Äôs a very nice benchmark, I‚Äôm not a fan of it as it:\n\nRelies on a closed-source proprietary model to evaluate the models.\nGiven you consume the model as an API, there are no reproducibility expectations. The MT Bench of today might not be the same as the MT Bench of a year ago.\nGPT-4 as a judge has its own biases. For example, it might prefer very verbose generations or have some ingrained biases towards preference GPT-4-like generations.\n80 dialogues seem quite limited to getting a good understanding of the model‚Äôs capabilities.\n\nAlpacaEval: This is a single-turn benchmark that evaluates the helpfulness of models. Again, it relies on GPT-4 as a judge.\nIFEval: ~500 prompts with verifiable responses. With some simple parsing, you can get a simple accuracy metric and don‚Äôt need a LLM judge.\nAGIEval: Benchmark of qualification exams for general knowledge.\n\nWhen releasing a new model, LMSYS Elo score would be ideal, but it‚Äôs not always possible to get into the arena. In that case, combining chatty evals (MT Bench and IFEval) with some more knowledge-heavy benchmarks (AGIEval and TruthfulQA) can be a good way to get a good understanding of the model‚Äôs capabilities. GMS8K and HumanEval (we‚Äôll learn about this one soon) is frequently added to the chat mix to make sure the model has math and code capabilities."
  },
  {
    "objectID": "blog/posts/llm_evals/index.html#addendum",
    "href": "blog/posts/llm_evals/index.html#addendum",
    "title": "LLM Evals and Benchmarking",
    "section": "Addendum",
    "text": "Addendum\nMy colleagues Lewis and Cl√©mentine provided some nice feedback for this blog post. They suggested I add two other benchmarks:\n\nEQ Bench: (for chat models) This benchmark is growingly popular, has a strong correlation with the chatbot arena ELO (r=0.94), and does not require a judge, making it a quick benchmark to get a sense of the model. It assesses emotional intelligence, and it‚Äôs a great way to see how well the model can understand and generate emotional responses.\nGPQA: (both base and chat models) This graduate-level benchmark is a challenging dataset of 198 multiple-choice questions crafted by domain experts (there are also 448 and 546 options). Think of this as a super difficult MMLU. Highly skilled non-expert validators (PhD in other domains), even with web access and spending over 30 minutes per question on average, reached 34% accuracy. Domain experts with or pursuing PhDs in the relevant fields achieve an accuracy of 65%. As a reference, GPT-4 achieves 35.7%, and Claude 3 Opus achieves 50.4% here, which is quite impressive!"
  },
  {
    "objectID": "blog/posts/llm_evals/index.html#more-on-benchmarks",
    "href": "blog/posts/llm_evals/index.html#more-on-benchmarks",
    "title": "LLM Evals and Benchmarking",
    "section": "More on benchmarks",
    "text": "More on benchmarks\nOne thing to consider is that most benchmarks are English-based and not necessarily capturing your specific use case. For chat models, there‚Äôs not much in terms of multi-turn benchmarks. There are efforts such a Korean LLM benchmark, but, in general, the ecosystem is in early stages.\nThere‚Äôs also a wave of new leaderboards, such as a LLM Sagfety Leaderboard, AllenAI WildBench Leaderboard, Red Teaming Robustness, NPHard Eval, and the Hallucinations Leaderboard.\nOn top of this, if you expect to mostly use your model in a specific domain, e.g.¬†customer success, it makes sense to use a leaderboard that is more focused on that domain. For example, the Patronus Leaderboard evaluates LM‚Äôs performance in finance, legal confidentiality, creative writing, customer support dialogue, toxicity, and enterprise PII.\nFinally, random vibe-based checks are often shared in Reddit, but they are too small of a sample and cherry-picking for my liking, but still interesting!\nThe most important takeaway here is to benchmark depending on how you‚Äôre going to use the model. For general comparisons, all of the above will help, but if you‚Äôre fine-tuning a model for a very specific internal use case in your company, using a golden test set with your own data is the best way to go!"
  },
  {
    "objectID": "blog/posts/llm_evals/index.html#what-about-code",
    "href": "blog/posts/llm_evals/index.html#what-about-code",
    "title": "LLM Evals and Benchmarking",
    "section": "What about code?",
    "text": "What about code?\nCode is definitely a big area in benchmarks too! Let‚Äôs briefly look at them:\n\nHumanEval: This is a benchmark that measures functional correctness by generating code based on a docstring. It‚Äôs a Python benchmark, but there are translations to 18 other languages (which is called MultiPL-E). Unfortunately, it just contains 164 Python programming problems, so when you see a big viral tweet of someone claiming a 1% improvement, it usually means it gets 2 more problems right. It‚Äôs a very nice benchmark, but it‚Äôs not as comprehensive as you might think. You can find HumanEval results for some dozens of languages in the BigCode Models Leaderboard.\n\n\n\nHumanEval+: This is HumanEval with 80x more tests.\nMBPP: This benchmark has 1,000 crowd-sourced Python programming problems designed for entry-level programmers. Each problem is a task description, a code solution, and three automated test cases\nMBPP+: This is MBPP with 35x more tests.\n\nWe‚Äôve seen some models have great performance in HumanEval but not so great in MBPP, so it‚Äôs important to use multiple benchmarks to get a good understanding of the model‚Äôs capabilities.\nI hope you liked this blog post! If you like this blog post, don‚Äôt hesitate to leave a GitHub Star or share it, that‚Äôs always appreciated and motivating!"
  },
  {
    "objectID": "blog/posts/random_transformer/index.html",
    "href": "blog/posts/random_transformer/index.html",
    "title": "The Random Transformer",
    "section": "",
    "text": "In this blog post, we‚Äôll do an end-to-end example of the math within a transformer model. The goal is to get a good understanding of how the model works. To make this manageable, we‚Äôll do lots of simplification. As we‚Äôll be doing quite a bit of the math by hand, we‚Äôll reduce the dimensions of the model. For example, rather than using embeddings of 512 values, we‚Äôll use embeddings of 4 values. This will make the math easier to follow! We‚Äôll use random vectors and matrices, but you can use your own values if you want to follow along.\nAs you‚Äôll see, the math is not that complicated. The complexity comes from the number of steps and the number of parameters. I recommend you to read the The Illustrated Transformer blog before reading this blog post (or reading in parallel). It‚Äôs a great blog post that explains the transformer model in a very intuitive (and illustrative!) way and I don‚Äôt intend to explain what it‚Äôs already explained there. My goal is to explain the ‚Äúhow‚Äù of the transformer model, not the ‚Äúwhat‚Äù. If you want to dive even deeper, check out the famous original paper: Attention is all you need.\nPrerequisites\nA basic understanding of linear algebra is required - we‚Äôll mostly do simple matrix multiplications, so no need to be an expert. Apart from that, basic understanding of Machine Learning and Deep Learning will be useful.\nWhat is covered here?\nWithout further ado, let‚Äôs get started! Our goal will be to use the transformer model as a translation tool, so we‚Äôll pass an input to the model expecting it to generate the translation. For example, we could pass ‚ÄúHello World‚Äù in English and expect ‚ÄúHola Mundo‚Äù in Spanish.\nLet‚Äôs take a look at the diagram of the transformer beast (don‚Äôt be intimidatd by it, you‚Äôll soon understand it!):\nThe original transformer model has two parts: encoder and decoder. The encoder focus is in ‚Äúunderstanding‚Äù or ‚Äúcapturing the meaning‚Äù of the input text, while the decoder focus is in generating the output text. We‚Äôll first focus on the encoder part."
  },
  {
    "objectID": "blog/posts/random_transformer/index.html#encoder",
    "href": "blog/posts/random_transformer/index.html#encoder",
    "title": "The Random Transformer",
    "section": "Encoder",
    "text": "Encoder\nThe whole goal of the encoder is to generate a rich embedding representation of the input text. This embedding will capture semantic information about the input, and will then be passed to the decoder to generate the output text. The encoder is composed of a stack of N layers. Before we jump into the layers, we need to see how to pass the words (or tokens) into the model.\n\n\n\n\n\n\nNote\n\n\n\nEmbeddings are a somewhat overused term. We‚Äôll first create an embedding that will be the input to the encoder. The encoder also outputs an embedding (also called hidden states sometimes). The decoder will also receive an embedding! üòÖ The whole point of an embedding is to represent a token as a vector.\n\n\n\n0. Tokenization\nML models can process numbers, not text. soo we need to turn our input text into numbers. That‚Äôs what tokenization does! This is the process of splitting the input text into tokens, each with an associated ID. For example, we could split the text ‚ÄúHello World‚Äù into two tokens: ‚ÄúHello‚Äù and ‚ÄúWorld‚Äù. We could also split it into characters: ‚ÄúH‚Äù, ‚Äúe‚Äù, ‚Äúl‚Äù, ‚Äúl‚Äù, ‚Äúo‚Äù, ‚Äù ‚Äú,‚ÄùW‚Äù, ‚Äúo‚Äù, ‚Äúr‚Äù, ‚Äúl‚Äù, ‚Äúd‚Äù. The choice of tokenization is up to us and depends on the data we‚Äôre working with.\nWord-based tokenization (splitting the text into words) will require a very large vocabulary (all possible tokens). It will also represent words like ‚Äúdog‚Äù and ‚Äúdogs‚Äù or ‚Äúrun‚Äù and ‚Äúrunning‚Äù as different tokens. Character-based vocabulary will require a smaller vocabulary, but will provide less meaning (in can be useful for languages such as Chinese where each character carries more information).\nThe field has moved towards subword tokenization. This is a middle ground between word-based and character-based tokenization. We‚Äôll split the words into subwords. For example, we could split ‚Äútokenization‚Äù into ‚Äútoken‚Äù and ‚Äúization‚Äù. How do we decide how to split the words? This is part of training a tokenizer through a statistical process that tries to identify which subwords are the best to pick given a dataset. It‚Äôs a deterministic process (unlike training a ML model).\nFor this blog post, let‚Äôs go with word tokenization for simplicity. Our goal will be to translate ‚ÄúHello World‚Äù from English to Spanish. Given an example ‚ÄúHello World‚Äù, we‚Äôll split into tokens: ‚ÄúHello‚Äù and ‚ÄúWorld‚Äù. Each token has an associated ID defined in the model‚Äôs vocabulary. For example, ‚ÄúHello‚Äù could be token 1 and ‚ÄúWorld‚Äù could be token 2.\n\n\n1. Embedding the text\nAlthough we could pass the token IDs to the model (e.g.¬†1 and 2), these numbers don‚Äôt carry any meaning. We need to turn them into vectors (list of numbers). This is what embedding does! The token embeddings map a token ID to a fixed-size vector with some semantic meaning of the tokens**. This brings some interesting properties: similar tokens will have a similar embedding (in other words, calculating the cosine similarity between two embeddings will give us a good idea of how similar the tokens are).\nNote that the mapping from a token to an embedding is learned. Although we could use a pre-trained embedding such as word2vec or GloVe, transformers models learn these embeddings as part of their training. This is a big advantage as the model can learn the best representation of the tokens for the task at hand. For example, the model could learn that ‚Äúdog‚Äù and ‚Äúdogs‚Äù should have similar embeddings.\nAll embeddings in a single model have the same size. The original transformer used a size of 512, but let‚Äôs do 4 for our example so we can keep the maths manageable. I‚Äôll assign some random values to each token (as mentioned, this mapping is usually learned by the model).\nHello -&gt; [1,2,3,4]\nWorld -&gt; [2,3,4,5]\n\n\n\n\n\n\nNote\n\n\n\nAfter releasing this blog post, multiple persons raised questions about the embeddings above. I was a bit lazy and just wrote down some numbers that will make for some nice math below. In practice, these numbers would be learned by the model. I‚Äôve updated the blog post to make this clearer. Thanks to everyone who raised this question!\nWe can estimate how similar these vectors are using cosine similarity, which would be too high for the vectors above. In practice, a vector would likely look something like [-0.071, 0.344, -0.12, 0.026, ‚Ä¶, -0.008].\n\n\nWe can represent our input as a single matrix\n\\[\nE = \\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n2 & 3 & 4 & 5\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nAlthough we could manage the two embeddings as separate vectors, it‚Äôs easier to manage them as a single matrix. This is because we‚Äôll be doing matrix multiplications as we move forward!\n\n\n\n\n2 Positional encoding\nThe individual embeddings in the matrix contain no information about the position of the words in the sentence‚Äù, so we need to feed some positional information. The way we do this is by adding a positional encoding to the embedding.\nThere are different choices on how to obtain these - we could use a learned embedding or a fixed vector. The original paper uses a fixed vector as they see almost no difference between the two approaches (see section 3.5 of the original paper). We‚Äôll use a fixed vector as well. Sine and cosine functions have a wave-like pattern, and they repeat over time. By using these functions, each position in the sentence gets a unique yet consistent positional encoding. Given they repeat over time, it can help the model more easily learn patterns like proximity and distance between elements. These are the functions they use in the paper (section 3.5):\n\\[\nPE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n\\]\n\\[\nPE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n\\]\nThe idea is to interpolate between sine and cosine for each value in the embedding (even indices will use sine, odd indices will use cosine). Let‚Äôs calculate them for our example!\nFor ‚ÄúHello‚Äù\n\ni = 0 (even): PE(0,0) = sin(0 / 10000^(0 / 4)) = sin(0) = 0\ni = 1 (odd): PE(0,1) = cos(0 / 10000^(2*1 / 4)) = cos(0) = 1\ni = 2 (even): PE(0,2) = sin(0 / 10000^(2*2 / 4)) = sin(0) = 0\ni = 3 (odd): PE(0,3) = cos(0 / 10000^(2*3 / 4)) = cos(0) = 1\n\nFor ‚ÄúWorld‚Äù\n\ni = 0 (even): PE(1,0) = sin(1 / 10000^(0 / 4)) = sin(1 / 10000^0) = sin(1) ‚âà 0.84\ni = 1 (odd): PE(1,1) = cos(1 / 10000^(2*1 / 4)) = cos(1 / 10000^0.5) ‚âà cos(0.01) ‚âà 0.99\ni = 2 (even): PE(1,2) = sin(1 / 10000^(2*2 / 4)) = sin(1 / 10000^1) ‚âà 0\ni = 3 (odd): PE(1,3) = cos(1 / 10000^(2*3 / 4)) = cos(1 / 10000^1.5) ‚âà 1\n\nSo concluding\n\n‚ÄúHello‚Äù -&gt; [0, 1, 0, 1]\n‚ÄúWorld‚Äù -&gt; [0.84, 0.99, 0, 1]\n\nNote that these encodings have the same dimension as the original embedding.\n\n\n\n\n\n\nNote\n\n\n\nWhile we use sine and cosine as the original paper, there are other ways to do this. BERT, a very popular transformer, use trainable positional embeddings.\n\n\n\n\n3. Add positional encoding and embedding\nWe now add the positional encoding to the embedding. This is done by adding the two vectors together.\n‚ÄúHello‚Äù = [1,2,3,4] + [0, 1, 0, 1] = [1, 3, 3, 5] ‚ÄúWorld‚Äù = [2,3,4,5] + [0.84, 0.99, 0, 1] = [2.84, 3.99, 4, 6]\nSo our new matrix, which will be the input to the encoder, is:\n\\[\nE = \\begin{bmatrix}\n1 & 3 & 3 & 5 \\\\\n2.84 & 3.99 & 4 & 6\n\\end{bmatrix}\n\\]\nIf you look at the original paper‚Äôs image, what we just did is the bottom left part of the image (the embedding + positional encoding).\n\n\n\nTransformer model from the original ‚Äúattention is all you need‚Äù paper\n\n\n\n\n4. Self-attention\n\n4.1 Matrices Definition\nWe‚Äôll now introduce the concept of multi-head attention. Attention is a mechanism that allows the model to focus on certain parts of the input. Multi-head attention is a way to allow the model to jointly attend to information from different representation subspaces. This is done by using multiple attention heads. Each attention head will have its own K, V, and Q matrices.\nLet‚Äôs use 2 attention heads for our example. We‚Äôll use random values for these matrices. Each matrix will be a 4x3 matrix. With this, each matrix will transform the 4-dimensional embeddings into 3-dimensional keys, values, and queries. This reduces the dimensionality for attention mechanism, which helps in managing the computational complexity. Note that using a too small attention size will hurt the performance of the model. Let‚Äôs use the following values (just random values):\nFor the first head\n\\[\n\\begin{align*}\nWK1 &= \\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}, \\quad\nWV1 &= \\begin{bmatrix}\n0 & 1 & 1 \\\\\n1  & 0 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}, \\quad\nWQ1 &= \\begin{bmatrix}\n0 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n1 & 1 & 0\n\\end{bmatrix}\n\\end{align*}\n\\]\nFor the second head\n\\[\n\\begin{align*}\nWK2 &= \\begin{bmatrix}\n0 & 1 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix}, \\quad\nWV2 &= \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 1 \\\\\n1 & 0 & 0\n\\end{bmatrix}, \\quad\nWQ2 &= \\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 1\n\\end{bmatrix}\n\\end{align*}\n\\]\n\n\n4.2 Keys, queries, and values calculation\nWe now need to multiply our input embeddings with the weight matrices to obtain the keys, queries, and values.\nKey calculation\n\\[\n\\begin{align*}\nE \\times WK1 &= \\begin{bmatrix}\n1 & 3 & 3 & 5 \\\\\n2.84 & 3.99 & 4 & 6\n\\end{bmatrix}\n\\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n1 & 0 & 1 \\\\\n0 & 1 & 0\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n(1 \\times 1) + (3 \\times 0) + (3 \\times 1) + (5 \\times 0) & (1 \\times 0) + (3 \\times 1) + (3 \\times 0) + (5 \\times 1) & (1 \\times 1) + (3 \\times 0) + (3 \\times 1) + (5 \\times 0) \\\\\n(2.84 \\times 1) + (3.99 \\times 0) + (4 \\times 1) + (6 \\times 0) & (2.84 \\times 0) + (4 \\times 1) + (4 \\times 0) + (6 \\times 1) & (2.84 \\times 1) + (4 \\times 0) + (4 \\times 1) + (6 \\times 0)\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n4 & 8 & 4 \\\\\n6.84 & 9.99 & 6.84\n\\end{bmatrix}\n\\end{align*}\n\\]\nOk, I actually do not want to do the math by hand for all of these - it gets a bit repetitive plus it breaks the site. So let‚Äôs cheat and use NumPy to do the calculations for us.\nWe first define the matrices\n\nimport numpy as np\n\nWK1 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 1], [0, 1, 0]])\nWV1 = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 1], [0, 1, 0]])\nWQ1 = np.array([[0, 0, 0], [1, 1, 0], [0, 0, 1], [1, 0, 0]])\n\nWK2 = np.array([[0, 1, 1], [1, 0, 1], [1, 1, 0], [0, 1, 0]])\nWV2 = np.array([[1, 0, 0], [0, 1, 1], [0, 0, 1], [1, 0, 0]])\nWQ2 = np.array([[1, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1]])\n\nAnd let‚Äôs confirm that I didn‚Äôt make any mistakes in the calculations above.\n\nembedding = np.array([[1, 3, 3, 5], [2.84, 3.99, 4, 6]])\nK1 = embedding @ WK1\nK1\n\narray([[4.  , 8.  , 4.  ],\n       [6.84, 9.99, 6.84]])\n\n\nPhew! Let‚Äôs now get the values and queries\nValue calculations\n\nV1 = embedding @ WV1\nV1\n\narray([[6.  , 6.  , 4.  ],\n       [7.99, 8.84, 6.84]])\n\n\nQuery calculations\n\nQ1 = embedding @ WQ1\nQ1\n\narray([[8.  , 3.  , 3.  ],\n       [9.99, 3.99, 4.  ]])\n\n\nLet‚Äôs skip the second head for now and focus on the first head final score. We‚Äôll come back to the second head later.\n\n\n4.3 Attention calculation\nCalculating the attention score requires a couple of steps:\n\nCalculate the dot product of the query with each key\nDivide the result by the square root of the dimension of the key vector\nApply a softmax function to obtain the attention weights\nMultiply each value vector by the attention weights\n\n\n4.3.1 Dot product of query with each key\nThe score for ‚ÄúHello‚Äù requires calculating the dot product of q1 with each key vector (k1 and k2)\n\\[\n\\begin{align*}\nq1 \\cdot k1 &= \\begin{bmatrix} 8 & 3 & 3 \\end{bmatrix} \\cdot \\begin{bmatrix} 4 \\\\ 8 \\\\ 4 \\end{bmatrix} \\\\\n&= 8 \\cdot 4 + 3 \\cdot 8 + 3 \\cdot 4 \\\\\n&= 68\n\\end{align*}\n\\]\nIn matrix world, that would be Q1 multiplied by the transpose of K1\n\\[\\begin{align*}\nQ1 \\times K1^\\top &= \\begin{bmatrix} 8 & 3 & 3 \\\\ 9.99 & 3.99 & 4 \\end{bmatrix} \\times \\begin{bmatrix} 4 & 6.84 \\\\ 8 & 9.99 \\\\ 4 & 6.84 \\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n    8 \\cdot 4 + 3 \\cdot 8 + 3 \\cdot 4 & 8 \\cdot 6.84 + 3 \\cdot 9.99 + 3 \\cdot 6.84 \\\\\n    9.99 \\cdot 4 + 3.99 \\cdot 8 + 4 \\cdot 4 & 9.99 \\cdot 6.84 + 3.99 \\cdot 9.99 + 4 \\cdot 6.84\n    \\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n    68 & 105.21 \\\\\n    87.88 & 135.5517\n    \\end{bmatrix}\n\\end{align*}\\]\nI‚Äôm prone to do mistakes, so let‚Äôs confirm with Python once again\n\nscores1 = Q1 @ K1.T\nscores1\n\narray([[ 68.    , 105.21  ],\n       [ 87.88  , 135.5517]])\n\n\n\n\n4.3.2 Divide by square root of dimension of key vector\nWe then divide the scores by the square root of the dimension (d) of the keys (3 in this case, but 64 in the original paper). Why? For large values of d, the dot product grows too large (we‚Äôre adding the multiplication of a bunch of numbers, after all, leading to high values). And large values are bad! We‚Äôll discuss soon more about this.\n\nscores1 = scores1 / np.sqrt(3)\nscores1\n\narray([[39.2598183 , 60.74302182],\n       [50.73754166, 78.26081048]])\n\n\n\n\n4.3.3 Apply softmax function\nWe then softmax to normalize so they are all positive and add up to 1.\n\n\n\n\n\n\nWhat is softmax?\n\n\n\nSoftmax is a function that takes a vector of values and returns a vector of values between 0 and 1, where the sum of the values is 1. It‚Äôs a nice way of obtaining probabilities. It‚Äôs defined as follows:\n\\[\n\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}\n\\]\nDon‚Äôt be intimidated by the formula - it‚Äôs actually quite simple. Let‚Äôs say we have the following vector:\n\\[\nx = \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}\n\\]\nThe softmax of this vector would be:\n\\[\n\\text{softmax}(x) = \\begin{bmatrix} \\frac{e^1}{e^1 + e^2 + e^3} & \\frac{e^2}{e^1 + e^2 + e^3} & \\frac{e^3}{e^1 + e^2 + e^3} \\end{bmatrix} = \\begin{bmatrix} 0.09 & 0.24 & 0.67 \\end{bmatrix}\n\\]\nAs you can see, the values are all positive and add up to 1.\n\n\n\ndef softmax(x):\n    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n\n\nscores1 = softmax(scores1)\nscores1\n\narray([[4.67695573e-10, 1.00000000e+00],\n       [1.11377182e-12, 1.00000000e+00]])\n\n\n\n\n4.3.4 Multiply value matrix by attention weights\nWe then multiply times the value matrix\n\nattention1 = scores1 @ V1\nattention1\n\narray([[7.99, 8.84, 6.84],\n       [7.99, 8.84, 6.84]])\n\n\nLet‚Äôs combine 4.3.1, 4.3.2, 4.3.3, and 4.3.4 into a single formula using matrices (this is from section 3.2.1 of the original paper):\n\\[\nAttention(Q,K,V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d}}\\right)V\n\\]\nYes, that‚Äôs it! All the math we just did can easily be encapsulated in the attention formula above! Let‚Äôs now translate this to code!\n\ndef attention(x, WQ, WK, WV):\n    K = x @ WK\n    V = x @ WV\n    Q = x @ WQ\n\n    scores = Q @ K.T\n    scores = scores / np.sqrt(3)\n    scores = softmax(scores)\n    scores = scores @ V\n    return scores\n\n\nattention(embedding, WQ1, WK1, WV1)\n\narray([[7.99, 8.84, 6.84],\n       [7.99, 8.84, 6.84]])\n\n\nWe confirm we got same values as above. Let‚Äôs chear and use this to obtain the attention scores the second attention head:\n\nattention2 = attention(embedding, WQ2, WK2, WV2)\nattention2\n\narray([[8.84, 3.99, 7.99],\n       [8.84, 3.99, 7.99]])\n\n\nIf you‚Äôre wondering how come the attention is the same for the two embeddings, it‚Äôs because the softmax is taking our scores to 0 and 1. See this:\n\nsoftmax(((embedding @ WQ2) @ (embedding @ WK2).T) / np.sqrt(3))\n\narray([[1.10613872e-14, 1.00000000e+00],\n       [4.95934510e-20, 1.00000000e+00]])\n\n\nThis is due to bad initialization of the matrices and small vector sizes. Large differences in the scores before applying softmax will just be amplified with softmax, leading to one value being close to 1 and others close to 0. In practice, our initial embedding matrices‚Äô values were maybe too high, leading to high values for the keys, values, and queries, which just grew larger as we multiplied them.\nRemember when we were dividing by the square root of the dimension of the keys? This is why we do that. If we don‚Äôt do that, the values of the dot product will be too large, leading to large values after the softmax. In this case, though, it seems it wasn‚Äôt enough given our small values! As a short-term hack, we can scale down the values by a larger amount than the square root of 3. Let‚Äôs redefine the attention function but scaling down by 30. This is not a good long-term solution, but it will help us get different values for the attention scores. We‚Äôll get back to a better solution later.\n\ndef attention(x, WQ, WK, WV):\n    K = x @ WK\n    V = x @ WV\n    Q = x @ WQ\n\n    scores = Q @ K.T\n    scores = scores / 30  # we just changed this\n    scores = softmax(scores)\n    scores = scores @ V\n    return scores\n\n\nattention1 = attention(embedding, WQ1, WK1, WV1)\nattention1\n\narray([[7.54348784, 8.20276657, 6.20276657],\n       [7.65266185, 8.35857269, 6.35857269]])\n\n\n\nattention2 = attention(embedding, WQ2, WK2, WV2)\nattention2\n\narray([[8.45589591, 3.85610456, 7.72085664],\n       [8.63740591, 3.91937741, 7.84804146]])\n\n\n\n\n4.3.5 Heads‚Äô attention output\nThe next layer of the encoder will expect a single matrix, not two. The first step will be to concatenate the two heads‚Äô outputs (section 3.2.2 of the original paper)\n\nattentions = np.concatenate([attention1, attention2], axis=1)\nattentions\n\narray([[7.54348784, 8.20276657, 6.20276657, 8.45589591, 3.85610456,\n        7.72085664],\n       [7.65266185, 8.35857269, 6.35857269, 8.63740591, 3.91937741,\n        7.84804146]])\n\n\nWe finally multiply this concatenated matrix by a weight matrix to obtain the final output of the attention layer. This weight matrix is also learned! The dimension of the matrix ensures we go back to the same dimension as the embedding (4 in our case).\n\n# Just some random values\nW = np.array(\n    [\n        [0.79445237, 0.1081456, 0.27411536, 0.78394531],\n        [0.29081936, -0.36187258, -0.32312791, -0.48530339],\n        [-0.36702934, -0.76471963, -0.88058366, -1.73713022],\n        [-0.02305587, -0.64315981, -0.68306653, -1.25393866],\n        [0.29077448, -0.04121674, 0.01509932, 0.13149906],\n        [0.57451867, -0.08895355, 0.02190485, 0.24535932],\n    ]\n)\nZ = attentions @ W\nZ\n\narray([[ 11.46394285, -13.18016471, -11.59340253, -17.04387829],\n       [ 11.62608573, -13.47454936, -11.87126395, -17.4926367 ]])\n\n\nThe image from The Ilustrated Transformer encapsulates all of this in a single image \n\n\n\n\n5. Feed-forward layer\n\n5.1 Basic feed-forward layer\nAfter the self-attention layer, the encoder has a feed-forward neural network (FFN). This is a simple network with two linear transformations and a ReLU activation in between. The Illustrated Transformer blog post does not dive into it, so let me briefly explain a bit more. The goal of the FFN is to process and transformer the representation produced by the attention mechanism. The flow is usually as follows (see section 3.3 of the original paper):\n\nFirst linear layer: this usually expands the dimensionality of the input. For example, if the input dimension is 512, the output dimension might be 2048. This is done to allow the model to learn more complex functions. In our simple of example with dimension of 4, we‚Äôll expand to 8.\nReLU activation: This is a non-linear activation function. It‚Äôs a simple function that returns 0 if the input is negative, and the input if it‚Äôs positive. This allows the model to learn non-linear functions. The math is as follows:\n\n\\[\n\\text{ReLU}(x) = \\max(0, x)\n\\]\n\nSecond linear layer: This is the opposite of the first linear layer. It reduces the dimensionality back to the original dimension. In our example, we‚Äôll reduce from 8 to 4.\n\nWe can represent all of this as follows\n\\[\n\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n\\]\nJust as a reminder, the input for this layer is the Z we calculated in the self-attention above. Here are the values as a reminder\n\\[\nZ =\n\\begin{bmatrix}\n11.46394281 & -13.18016469 & -11.59340253 & -17.04387833 \\\\\n11.62608569 & -13.47454934 & -11.87126395 & -17.49263674\n\\end{bmatrix}\n\\]\nLet‚Äôs now define some random values for the weight matrices and bias vectors. I‚Äôll do it with code, but you can do it by hand if you feel patient!\n\nW1 = np.random.randn(4, 8)\nW2 = np.random.randn(8, 4)\nb1 = np.random.randn(8)\nb2 = np.random.randn(4)\n\nAnd now let‚Äôs write the forward pass function\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef feed_forward(Z, W1, b1, W2, b2):\n    return relu(Z.dot(W1) + b1).dot(W2) + b2\n\n\noutput_encoder = feed_forward(Z, W1, b1, W2, b2)\noutput_encoder\n\narray([[ -3.24115016,  -9.7901049 , -29.42555675, -19.93135286],\n       [ -3.40199463,  -9.87245924, -30.05715408, -20.05271018]])\n\n\n\n\n5.2 Encapsulating everything: The Random Encoder\nLet‚Äôs now write some code to have the multi-head attention and the feed-forward, all together in the encoder block.\n\n\n\n\n\n\nNote\n\n\n\nThe code optimizes for understanding and educational purposes, not for performance! Don‚Äôt judge too hard!\n\n\n\nd_embedding = 4\nd_key = d_value = d_query = 3\nd_feed_forward = 8\nn_attention_heads = 2\n\ndef attention(x, WQ, WK, WV):\n    K = x @ WK\n    V = x @ WV\n    Q = x @ WQ\n\n    scores = Q @ K.T\n    scores = scores / np.sqrt(d_key)\n    scores = softmax(scores)\n    scores = scores @ V\n    return scores\n\ndef multi_head_attention(x, WQs, WKs, WVs):\n    attentions = np.concatenate(\n        [attention(x, WQ, WK, WV) for WQ, WK, WV in zip(WQs, WKs, WVs)], axis=1\n    )\n    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n    return attentions @ W\n\ndef feed_forward(Z, W1, b1, W2, b2):\n    return relu(Z.dot(W1) + b1).dot(W2) + b2\n\ndef encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):\n    Z = multi_head_attention(x, WQs, WKs, WVs)\n    Z = feed_forward(Z, W1, b1, W2, b2)\n    return Z\n\ndef random_encoder_block(x):\n    WQs = [\n        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n    ]\n    WKs = [\n        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n    ]\n    WVs = [\n        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n    ]\n    W1 = np.random.randn(d_embedding, d_feed_forward)\n    b1 = np.random.randn(d_feed_forward)\n    W2 = np.random.randn(d_feed_forward, d_embedding)\n    b2 = np.random.randn(d_embedding)\n    return encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2)\n\nRecall that our input is the matrix E which has the positional encoding and the embedding.\n\nembedding\n\narray([[1.  , 3.  , 3.  , 5.  ],\n       [2.84, 3.99, 4.  , 6.  ]])\n\n\nLet‚Äôs now pass this to our random_encoder_block function\n\nrandom_encoder_block(embedding)\n\narray([[ -71.76537515, -131.43316885,   13.2938131 ,   -4.26831998],\n       [ -72.04253781, -131.84091347,   13.3385937 ,   -4.32872015]])\n\n\nNice! This was just one encoder block. The original paper uses 6 encoders. The output of one encoder goes to the next, and so on:\n\ndef encoder(x, n=6):\n    for _ in range(n):\n        x = random_encoder_block(x)\n    return x\n\n\nencoder(embedding)\n\n/tmp/ipykernel_11906/1045810361.py:2: RuntimeWarning: overflow encountered in exp\n  return np.exp(x)/np.sum(np.exp(x),axis=1, keepdims=True)\n/tmp/ipykernel_11906/1045810361.py:2: RuntimeWarning: invalid value encountered in divide\n  return np.exp(x)/np.sum(np.exp(x),axis=1, keepdims=True)\n\n\narray([[nan, nan, nan, nan],\n       [nan, nan, nan, nan]])\n\n\n\n\n5.3 Residual and Layer Normalization\nUh oh! We‚Äôre getting NaNs! It seems our values are too high, and when being passed to the next encoder, they end up being too high and exploding! This issue of having values that are too high is a common issue when training models. For example, when doing the backpropagation (the technique through which the models learn), the gradients can become too large and end up exploding; this is called gradient explosion. Without any kind of normalization, small changes in the input of early layers end up being amplified in later layers. This is a common problem in deep neural networks. There are two common techniques to mitigate this problem: residual connections and layer normalization (section 3.1 of the paper, barely mentioned).\n\nResidual connections: Residual connections are simply adding the input of the layer to it output. For example, we add the initial embedding to the output of the attention. Residual connections mitigate the vanishing gradient problem. The intuition is that if the gradient is too small, we can just add the input to the output and the gradient will be larger. The math is very simple:\n\n\\[\n\\text{Residual}(x) = x + \\text{Layer}(x)\n\\]\nThat‚Äôs it! We‚Äôll do this to the output of the attention and the output of the feed-forward layer.\n\nLayer normalization Layer normalization is a technique to normalize the inputs of a layer. It normalizes across the embedding dimension. The intuition is that we want to normalize the inputs of a layer so that they have a mean of 0 and a standard deviation of 1. This helps with the gradient flow. The math does not look so simple at a first glance.\n\n\\[\n\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\times \\gamma + \\beta\n\\]\nLet‚Äôs explain each parameter:\n\n\\(\\mu\\) is the mean of the embedding\n\\(\\sigma\\) is the standard deviation of the embedding\n\\(\\epsilon\\) is a small number to avoid division by zero. In case the standard deviation is 0, this small epsilon saves the day!\n\\(\\gamma\\) and \\(\\beta\\) are learned parameters that control scaling and shifting steps.\n\nUnlike batch normalization (no worries if you don‚Äôt know what it is), layer normalization normalizes across the embedding dimension - that means that each embedding will not be affected by other samples in the batch. The intuition is that we want to normalize the inputs of a layer so that they have a mean of 0 and a standard deviation of 1.\nWhy do we add the learnable parameters \\(\\gamma\\) and \\(\\beta\\)? The reason is that we don‚Äôt want to lose the representational power of the layer. If we just normalize the inputs, we might lose some information. By adding the learnable parameters, we can learn to scale and shift the normalized values.\nCombining the equations, the equation for the whole encoder could look like this\n\\[\n\\text{Z}(x) = \\text{LayerNorm}(x + \\text{Attention}(x))\n\\]\n\\[\n\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n\\]\n\\[\n\\text{Encoder}(x) = \\text{LayerNorm}(Z(x) + \\text{FFN}(Z(x) + x))\n\\]\nLet‚Äôs try with our example! Let‚Äôs go with E and Z values from before\n\\[\n\\begin{align*}\n\\text{E} + \\text{Attention(E)} &= \\begin{bmatrix}\n1.0 & 3.0 & 3.0 & 5.0 \\\\\n2.84 & 3.99 & 4.0 & 6.0\n\\end{bmatrix} + \\begin{bmatrix}\n11.46394281 & -13.18016469 & -11.59340253 & -17.04387833 \\\\\n11.62608569 & -13.47454934 & -11.87126395 & -17.49263674\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n12.46394281 & -10.18016469 & -8.59340253 & -12.04387833 \\\\\n14.46608569 & -9.48454934 & -7.87126395 & -11.49263674\n\\end{bmatrix}\n\\end{align*}\n\\]\nLet‚Äôs now calculate the layer normalization, we can divide it into three steps:\n\nCompute mean and variance for each embedding.\nNormalize by substracting the mean of its row and dividing by the square root of its row variance (plus a small number to avoid division by zero).\nScale and shift by multiplying by gamma and adding beta.\n\n\n5.3.1 Mean and variance\nFor the first embedding\n\\[\n\\begin{align*}\n\\mu_1 &= \\frac{12.46394281-10.18016469-8.59340253-12.04387833}{4} = -4.58837568 \\\\\n\\sigma^2 &= \\frac{\\sum (x_i - \\mu)^2}{N} \\\\\n&= \\frac{(12.46394281 - (-4.588375685))^2 + \\ldots + (-12.04387833 - (-4.588375685))^2}{4} \\\\\n&= \\frac{393.67443005013}{4} \\\\\n&= 98.418607512533 \\\\\n\\sigma &= \\sqrt{98.418607512533} \\\\\n&= 9.9206152789297\n\\end{align*}\n\\]\nWe can do the same for the second embedding. We‚Äôll skip the calculations but you get the hang of it.\n\\[\n\\begin{align*}\n\\mu_2 &= -3.59559109 \\\\\n\\sigma_2 &= 10.50653018\n\\end{align*}\n\\]\nLet‚Äôs confirm with Python\n\n(embedding + Z).mean(axis=-1, keepdims=True)\n\narray([[-4.58837567],\n       [-3.59559107]])\n\n\n\n(embedding + Z).std(axis=-1, keepdims=True)\n\narray([[ 9.92061529],\n       [10.50653019]])\n\n\nAmazing! Let‚Äôs now normalize\n\n\n5.3.2 Normalize\nFor normalization, for each value in the embedding, we subsctract the mean and divide by the standard deviation. Epsilon is a very small value, such as 0.00001. We‚Äôll assume \\(\\gamma=1\\) and \\(\\beta=0\\), it simplifies things.\n\\[\\begin{align*}\n\\text{normalized}_1 &= \\frac{12.46394281 - (-4.58837568)}{\\sqrt{98.418607512533 + \\epsilon}} \\\\\n&= \\frac{17.05231849}{9.9206152789297} \\\\\n&= 1.718 \\\\\n\\text{normalized}_2 &= \\frac{-10.18016469 - (-4.58837568)}{\\sqrt{98.418607512533 + \\epsilon}} \\\\\n&= \\frac{-5.59178901}{9.9206152789297} \\\\\n&= -0.564 \\\\\n\\text{normalized}_3 &= \\frac{-8.59340253 - (-4.58837568)}{\\sqrt{98.418607512533 + \\epsilon}} \\\\\n&= \\frac{-4.00502685}{9.9206152789297} \\\\\n&= -0.404 \\\\\n\\text{normalized}_4 &= \\frac{-12.04387833 - (-4.58837568)}{\\sqrt{98.418607512533 + \\epsilon}} \\\\\n&= \\frac{-7.45550265}{9.9206152789297} \\\\\n&= -0.752\n\\end{align*}\\]\nWe‚Äôll skip the calculations by hand for the second embedding. Let‚Äôs confirm with code! Let‚Äôs re-define our encoder_block function with this change\n\ndef layer_norm(x, epsilon=1e-6):\n    mean = x.mean(axis=-1, keepdims=True)\n    std = x.std(axis=-1, keepdims=True)\n    return (x - mean) / (std + epsilon)\n\ndef encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):\n    Z = multi_head_attention(x, WQs, WKs, WVs)\n    Z = layer_norm(Z + x)\n\n    output = feed_forward(Z, W1, b1, W2, b2)\n    return layer_norm(output + Z)\n\n\nlayer_norm(Z + embedding)\n\narray([[ 1.71887693, -0.56365339, -0.40370747, -0.75151608],\n       [ 1.71909039, -0.56050453, -0.40695381, -0.75163205]])\n\n\nIt works! Let‚Äôs retry to pass the embedding through the six encoders.\n\ndef encoder(x, n=6):\n    for _ in range(n):\n        x = random_encoder_block(x)\n    return x\n\n\nencoder(embedding)\n\narray([[-0.335849  , -1.44504571,  1.21698183,  0.56391289],\n       [-0.33583947, -1.44504861,  1.21698606,  0.56390202]])\n\n\nAmazing! These values make sense and we don‚Äôt get NaNs! The idea of the stack of encoders is that they output a continuous representation, z, that captures the meaning of the input sequence. This representation is then passed to the decoder, which will genrate an output sequence of symbols, one element at a time.\nBefore diving into the decoder, here‚Äôs an image from Jay‚Äôs amazing blog post:\n\n\n\nEncoder and decoder\n\n\nYou should be able to explain each component at the left side! Quite impressive, right? Let‚Äôs now move to the decoder."
  },
  {
    "objectID": "blog/posts/random_transformer/index.html#decoder",
    "href": "blog/posts/random_transformer/index.html#decoder",
    "title": "The Random Transformer",
    "section": "Decoder",
    "text": "Decoder\nMost of the thing we learned for encoders will be used in the decoder as well! The decoder has two self-attention layers, one for the encoder and one for the decoder. The decoder also has a feed-forward layer. Let‚Äôs go through each of these.\nThe decoder block receives two inputs: the output of the encoder and the generated output sequence. The output of the encoder is the representation of the input sequence. During inference, the generated output sequence starts with a special start-of-sequence token (SOS). During training, the target output sequence is the actual output sequence, shifted by one position. This will be clearer soon!\nGiven the embedding generated by the encoder and the SOS token, the decoder will then generate the next token of the sequence, e.g.¬†‚Äúhola‚Äù. The decoder is autoregressive, that means that the decoder will take the previously generated tokens and again generate the second token.\n\nIteration 1: Input is SOS, output is ‚Äúhola‚Äù\nIteration 2: Input is SOS + ‚Äúhola‚Äù, output is ‚Äúmundo‚Äù\nIteration 3: Input is SOS + ‚Äúhola‚Äù + ‚Äúmundo‚Äù, output is EOS\n\nHere, SOS is the start-of-sequence token and EOS is the end-of-sequence token. The decoder will stop when it generates the EOS token. It generates one token at a time. Note that all iterations use the embedding generated by the encoder.\n\n\n\n\n\n\nNote\n\n\n\nThis autoregressive design makes decoder slow. The encoder is able to generate its embedding in a single forward pass while the decoder needs to do many forward passes. This is one of the reasons why architectures that only use the encoder (such as BERT or sentence similarity models) are much faster than decoder-only architectures (such as GPT-2 or BART).\n\n\nLet‚Äôs dive into each step! Just as the encoder, the decoder is composed of a stack of decoder blocks. The decoder block is a bit more complex than the encoder block. The general structure is:\n\n(Masked) Self-attention layer\nResidual connection and layer normalization\nEncoder-decoder attention layer\nResidual connection and layer normalization\nFeed-forward layer\nResidual connection and layer normalization\n\nWe‚Äôre already familiar with all the math from 1, 2, 3, 5 and 6. See the right side of the image below, you‚Äôll see that all these blocks you already know (the right part):\n\n\n\nTransformer model from the original ‚Äúattention is all you need‚Äù paper\n\n\n\n1. Embedding the text\nThe first text of the decoder is to embed the input tokens. The input token is SOS, so we‚Äôll embed it. We‚Äôll use the same embedding dimension as the encoder. Let‚Äôs assume the embedding vector for SOS is the following:\n\\[\nE = \\begin{bmatrix}\n1 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\n\n\n2. Positional encoding\nWe‚Äôll now add the positional encoding to the embedding, just as we did for the encoder. Given it‚Äôs the same position as ‚ÄúHello‚Äù, we‚Äôll have same positional encoding as we did before:\n\ni = 0 (even): PE(0,0) = sin(0 / 10000^(0 / 4)) = sin(0) = 0\ni = 1 (odd): PE(0,1) = cos(0 / 10000^(2*1 / 4)) = cos(0) = 1\ni = 2 (even): PE(0,2) = sin(0 / 10000^(2*2 / 4)) = sin(0) = 0\ni = 3 (odd): PE(0,3) = cos(0 / 10000^(2*3 / 4)) = cos(0) = 1\n\n\n\n3. Add positional encoding and embedding\nAdding the positional encoding to the embedding is done by adding the two vectors together:\n\\[\nE = \\begin{bmatrix}\n1 & 1 & 0 & 1\n\\end{bmatrix}\n\\]\n\n\n4. Self-attention\nThe first step within the decoder block is the self-attention mechanism. Luckily, we have some code for this and can just use it!\n\nd_embedding = 4\nn_attention_heads = 2\n\nE = np.array([[1, 1, 0, 1]])\nWQs = [np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)]\nWKs = [np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)]\nWVs = [np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)]\n\nZ_self_attention = multi_head_attention(E, WQs, WKs, WVs)\nZ_self_attention\n\narray([[ 2.19334924, 10.61851198, -4.50089666, -2.76366551]])\n\n\n\n\n\n\n\n\nNote\n\n\n\nThings are quite simple for inference. For training, things are a bit tricky. During training, we use unlabeled data: just a bunch of text data, frequentyl scraped from the web. While the encoder‚Äôs goal is to capture all information of the input, the decoder‚Äôs goal is to predict the most likely next token. This means that the decoder can only use the tokens that have been generated so far (it cannot cheat and see the next tokens).\nBecause of this, we use masked self-attention: we mask the tokens that have not been generated yet. This is done by setting the attention scores to -inf. This is done in the original paper (section 3.2.3.1). We‚Äôll skip this for now, but it‚Äôs important to keep in mind that the decoder is a bit more complex during training.\n\n\n\n\n5. Residual connection and layer normalization\nNothing magical here, we just add the input to the output of the self-attention and apply layer normalization. We‚Äôll use the same code as before.\n\nZ_self_attention = layer_norm(Z_self_attention + E)\nZ_self_attention\n\narray([[ 0.17236212,  1.54684892, -1.0828824 , -0.63632864]])\n\n\n\n\n6. Encoder-decoder attention\nThis part is the new one! If you were wondering where do the encoder-generated embeddings come in, this is their moment to shine!\nLet‚Äôs assume the output of the encoder is the following matrix\n\\[\n\\begin{bmatrix}\n-1.5 & 1.0 & -0.8 & 1.5 \\\\\n1.0 & -1.0 & -0.5 & 1.0\n\\end{bmatrix}\n\\]\nIn the self-attention mechanism, we calculate the queries, keys, and values from the input embedding.\nIn the encoder-decoder attention, we calculate the queries from the previous decoder layer and the keys and values from the encoder output! All the math is the same as before; the only difference is what embedding to use for the queries. Let‚Äôs look at some code\n\ndef encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):\n    # The next three lines are the key difference!\n    K = encoder_output @ WK    # Note that now we pass the previous encoder output!\n    V = encoder_output @ WV    # Note that now we pass the previous encoder output!\n    Q = attention_input @ WQ   # Same as self-attention\n\n    # This stays the same\n    scores = Q @ K.T\n    scores = scores / np.sqrt(d_key)\n    scores = softmax(scores)\n    scores = scores @ V\n    return scores\n\n\ndef multi_head_encoder_decoder_attention(\n    encoder_output, attention_input, WQs, WKs, WVs\n):\n    # Note that now we pass the previous encoder output!\n    attentions = np.concatenate(\n        [\n            encoder_decoder_attention(\n                encoder_output, attention_input, WQ, WK, WV\n            )\n            for WQ, WK, WV in zip(WQs, WKs, WVs)\n        ],\n        axis=1,\n    )\n    W = np.random.randn(n_attention_heads * d_value, d_embedding)\n    return attentions @ W\n\n\nWQs = [np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)]\nWKs = [np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)]\nWVs = [np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)]\n\nencoder_output = np.array([[-1.5, 1.0, -0.8, 1.5], [1.0, -1.0, -0.5, 1.0]])\n\nZ_encoder_decoder = multi_head_encoder_decoder_attention(\n    encoder_output, Z_self_attention, WQs, WKs, WVs\n)\nZ_encoder_decoder\n\narray([[ 1.57651431,  4.92489307, -0.08644448, -0.46776051]])\n\n\nThis worked! You might be asking ‚Äúwhy do we do this?‚Äù. The reason is that we want the decoder to focus on the relevant parts of the input text (e.g., ‚Äúhello world‚Äù). The encoder-decoder attention allows each position in the decoder to attend over all positions in the input sequence. This is very helpful for tasks such as translation, where the decoder needs to focus on the relevant parts of the input sequence. The decoder will learn to focus on the relevant parts of the input sequence by learning to generate the correct output tokens. This is a very powerful mechanism!\n\n\n7. Residual connection and layer normalization\nSame as before!\n\nZ_encoder_decoder = layer_norm(Z_encoder_decoder + Z_self_attention)\nZ_encoder_decoder\n\narray([[-0.44406723,  1.6552893 , -0.19984632, -1.01137575]])\n\n\n\n\n8. Feed-forward layer\nOnce again, same as before! I‚Äôll also do the residual connection and layer normalization after it.\n\nW1 = np.random.randn(4, 8)\nW2 = np.random.randn(8, 4)\nb1 = np.random.randn(8)\nb2 = np.random.randn(4)\n\noutput = layer_norm(feed_forward(Z_encoder_decoder, W1, b1, W2, b2) + Z_encoder_decoder)\noutput\n\narray([[-0.97650182,  0.81470137, -2.79122044, -3.39192873]])\n\n\n\n\n9. Encapsulating everything: The Random Decoder\nLet‚Äôs write the code for a single decoder block. The main change is that we now have an additional attention mechanism.\n\nd_embedding = 4\nd_key = d_value = d_query = 3\nd_feed_forward = 8\nn_attention_heads = 2\nencoder_output = np.array([[-1.5, 1.0, -0.8, 1.5], [1.0, -1.0, -0.5, 1.0]])\n\ndef decoder_block(\n    x,\n    encoder_output,\n    WQs_self_attention, WKs_self_attention, WVs_self_attention,\n    WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,\n    W1, b1, W2, b2,\n):\n    # Same as before\n    Z = multi_head_attention(\n        x, WQs_self_attention, WKs_self_attention, WVs_self_attention\n    )\n    Z = layer_norm(Z + x)\n\n    # The next three lines are the key difference!\n    Z_encoder_decoder = multi_head_encoder_decoder_attention(\n        encoder_output, Z, WQs_ed_attention, WKs_ed_attention, WVs_ed_attention\n    )\n    Z_encoder_decoder = layer_norm(Z_encoder_decoder + Z)\n\n    # Same as before\n    output = feed_forward(Z_encoder_decoder, W1, b1, W2, b2)\n    return layer_norm(output + Z_encoder_decoder)\n\ndef random_decoder_block(x, encoder_output):\n    # Just a bunch of random initializations\n    WQs_self_attention = [\n        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n    ]\n    WKs_self_attention = [\n        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n    ]\n    WVs_self_attention = [\n        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n    ]\n\n    WQs_ed_attention = [\n        np.random.randn(d_embedding, d_query) for _ in range(n_attention_heads)\n    ]\n    WKs_ed_attention = [\n        np.random.randn(d_embedding, d_key) for _ in range(n_attention_heads)\n    ]\n    WVs_ed_attention = [\n        np.random.randn(d_embedding, d_value) for _ in range(n_attention_heads)\n    ]\n\n    W1 = np.random.randn(d_embedding, d_feed_forward)\n    b1 = np.random.randn(d_feed_forward)\n    W2 = np.random.randn(d_feed_forward, d_embedding)\n    b2 = np.random.randn(d_embedding)\n\n\n    return decoder_block(\n        x, encoder_output,\n        WQs_self_attention, WKs_self_attention, WVs_self_attention,\n        WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,\n        W1, b1, W2, b2,\n    )\n\n\ndef decoder(x, decoder_embedding, n=6):\n    for _ in range(n):\n        x = random_decoder_block(x, decoder_embedding)\n    return x\n\ndecoder(E, encoder_output)\n\narray([[ 0.25919176,  1.49913566, -1.14331487, -0.61501256],\n       [ 0.25956188,  1.49896896, -1.14336934, -0.61516151]])"
  },
  {
    "objectID": "blog/posts/random_transformer/index.html#generating-the-output-sequence",
    "href": "blog/posts/random_transformer/index.html#generating-the-output-sequence",
    "title": "The Random Transformer",
    "section": "Generating the output sequence",
    "text": "Generating the output sequence\nWe have all the building blocks! Let‚Äôs now generate the output sequence.\n\nWe have the encoder, which takes the input sequence and generates its rich representation. It‚Äôs composed of a stack of encoder blocks.\nWe have the decoder, which takes the encoder output and generated tokens, and generates the output sequence. It‚Äôs composed of a stack of decoder blocks.\n\nHow do we go from the decoder‚Äôs output to a word? We need to add a final linear layer and a softmax layer on top of the decoder. The whole algorithm looks like this:\n\nEncoder Processing: The encoder receives the input sequence and generates a contextualized representation of the entire sequence, utilizing a stack of encoder blocks.\nDecoder Initiation: The decoding process begins with the embedding of the SOS (Start of Sequence) token, combined with the encoder‚Äôs output.\nDecoder Operation: The decoder uses the encoder‚Äôs output and the embeddings of all previously generated tokens to produce a new list of embeddings.\nLinear Layer for Logits A linear layer is applied to the latest output embedding from the decoder to generate logits, representing raw predictions for the next token.\nSoftmax for Probabilities: These logits are then passed through a softmax layer, which converts them into a probability distribution over potential next tokens.\nIterative Token Generation: This process is repeated, with each step involving the decoder generating the next token based on the cumulative embeddings of previously generated tokens and the initial encoder output.\nSequence Completion: The generation continues through these steps until the EOS (End of Sequence) token is produced or a predefined maximum sequence length is reached.\n\nThis is mentioned in the section 3.4 of the paper.\n\n1. Linear layer\nThe linear layer is a simple linear transformation. It takes the decoder‚Äôs output and transforms it into a vector of size vocab_size. This is the size of the vocabulary. For example, if we have a vocabulary of 10000 words, the linear layer will transform the decoder‚Äôs output into a vector of size 10000. This vector will contain the probability of each word being the next word in the sequence. For simplicity, let‚Äôs go with a vocabulary of 10 words and assume the first decoder output is a very simple vector: [1, 0, 1, 0]. We‚Äôll use random weights and biases matrices of the size vocab_size x decoder_output_size.\n\ndef linear(x, W, b):\n    return np.dot(x, W) + b\n\nx = linear([1, 0, 1, 0], np.random.randn(4, 10), np.random.randn(10))\nx\n\narray([ 0.06900542, -1.81351091, -1.3122958 , -0.33197364,  2.54767851,\n       -1.55188231,  0.82907169,  0.85910931, -0.32982856, -1.26792439])\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat do we use as input for the linear layer? The decoder will output one embedding for each token in the sequence. The input for the linear layer will be the last generated embedding. The last embedding encapsulates information to the entire sequence up to that point, so it contains all the information needed to generate the next token. This means that each output embedding from the decoder contains information about the entire sequence up to that point.\n\n\n\n\n2. Softmax\nThese are called logits but they are not easily interpretable. We need to apply a softmax function to obtain the probabilities.\n\nsoftmax(x)\n\narray([[0.01602618, 0.06261303, 0.38162024, 0.03087794, 0.0102383 ,\n        0.00446011, 0.01777314, 0.00068275, 0.46780959, 0.00789871]])\n\n\nThis is giving us probabilities! Let‚Äôa assume the vocabulary is the following:\n\\[\n\\text{vocab} = \\begin{bmatrix}\n\\text{hello} & \\text{mundo} & \\text{world} & \\text{how} & \\text{?} & \\text{EOS} & \\text{SOS} & \\text{a} & \\text{hola} & \\text{c}\n\\end{bmatrix}\n\\]\nThe above tells us that the probabilities are\n\nhello: 0.01602618\nmundo: 0.06261303\nworld: 0.38162024\nhow: 0.03087794\n?: 0.0102383\nEOS: 0.00446011\nSOS: 0.01777314\na: 0.00068275\nhola: 0.46780959\nc: 0.00789871\n\nFrom these, the most likely next token is ‚Äúhola‚Äù. Picking always the most likely token is called greedy decoding. This is not always the best approach, as it might lead to suboptimal results, but we won‚Äôt dive into generation techniques at the moment. If you want to learn more about it, check out this amazing blog post.\n\n\n3. The Random Encoder-Decoder Transformer\nLet‚Äôs write the whole code for this! Let‚Äôs define a dictionary that maps the words to their initial embeddings. Note that this is also learned during training, but we‚Äôll use random values for now.\n\nvocabulary = [\n    \"hello\",\n    \"mundo\",\n    \"world\",\n    \"how\",\n    \"?\",\n    \"EOS\",\n    \"SOS\",\n    \"a\",\n    \"hola\",\n    \"c\",\n]\nembedding_reps = np.random.randn(10, 4)\nvocabulary_embeddings = {\n    word: embedding_reps[i] for i, word in enumerate(vocabulary)\n}\nvocabulary_embeddings\n\n{'hello': array([-0.32106406,  2.09332588, -0.77994069,  0.92639774]),\n 'mundo': array([-0.59563791, -0.63389256,  1.70663692, -0.99495115]),\n 'world': array([ 1.35581862, -0.0323546 ,  2.76696887,  0.83069982]),\n 'how': array([-0.52975474,  0.94439644,  0.80073818, -1.50135518]),\n '?': array([-0.88116833,  0.13995055,  2.01827674, -0.52554391]),\n 'EOS': array([1.12207024, 1.40905796, 1.22231714, 0.02267638]),\n 'SOS': array([-0.60624082, -0.67560165,  0.77152125,  0.63472247]),\n 'a': array([ 1.67622229, -0.20319309, -0.18324905, -0.24258774]),\n 'hola': array([ 1.07809402, -0.83846408, -0.33448976,  0.28995976]),\n 'c': array([ 0.65643157,  0.24935726, -0.80839751, -1.87156293])}\n\n\nAnd now let‚Äôs write our random generate method that generates tokens autorergressively.\n\ndef generate(input_sequence, max_iters=3):\n    # We first encode the inputs into embeddings\n    # This skips the positional encoding step for simplicity\n    embedded_inputs = [\n        vocabulary_embeddings[token] for token in input_sequence\n    ]\n    print(\"Embedding representation (encoder input)\", embedded_inputs)\n\n    # We then generate an embedding representation\n    encoder_output = encoder(embedded_inputs)\n    print(\"Embedding generated by encoder (encoder output)\", encoder_output)\n\n    # We initialize the decoder output with the embedding of the start token\n    sequence_embeddings = [vocabulary_embeddings[\"SOS\"]]\n    output = \"SOS\"\n    \n    # Random matrices for the linear layer\n    W_linear = np.random.randn(d_embedding, len(vocabulary))\n    b_linear = np.random.randn(len(vocabulary))\n\n    # We limit number of decoding steps to avoid too long sequences without EOS\n    for i in range(max_iters):\n        # Decoder step\n        decoder_output = decoder(sequence_embeddings, encoder_output)\n\n        # Only use the last output for prediction\n        logits = linear(decoder_output[-1], W_linear, b_linear)\n        # We wrap logits in a list as our softmax expects batches/2D array\n        probs = softmax([logits])\n\n        # We get the most likely next token\n        next_token = vocabulary[np.argmax(probs)]\n        sequence_embeddings.append(vocabulary_embeddings[next_token])\n        output += \" \" + next_token\n\n        print(\n            \"Iteration\", i, \n            \"next token\", next_token,\n            \"with probability of\", np.max(probs),\n        )\n\n        # If the next token is the end token, we return the sequence\n        if next_token == \"EOS\":\n            return output\n\n    return output, sequence_embeddings\n\nLet‚Äôs run this now!\n\ngenerate([\"hello\", \"world\"])\n\nEmbedding representation (encoder input) [array([-0.32106406,  2.09332588, -0.77994069,  0.92639774]), array([ 1.35581862, -0.0323546 ,  2.76696887,  0.83069982])]\nEmbedding generated by encoder (encoder output) [[ 1.14747807 -1.5941759   0.36847675  0.07822107]\n [ 1.14747705 -1.59417696  0.36847441  0.07822551]]\nIteration 0 next token hola with probability of 0.4327111653266739\nIteration 1 next token mundo with probability of 0.4411354383451089\nIteration 2 next token world with probability of 0.4746898792307499\n\n\n('SOS hola mundo world',\n [array([-0.60624082, -0.67560165,  0.77152125,  0.63472247]),\n  array([ 1.07809402, -0.83846408, -0.33448976,  0.28995976]),\n  array([-0.59563791, -0.63389256,  1.70663692, -0.99495115]),\n  array([ 1.35581862, -0.0323546 ,  2.76696887,  0.83069982])])\n\n\nOk, so we got the tokens ‚Äúhow‚Äù, ‚Äúa‚Äù, and ‚Äúc‚Äù. This is not a good translation, but it‚Äôs expected! We only used random weights!\nI suggest you to look again in detail at the whole encoder-decoder architecture from the original paper:\n\n\n\nEncoder and decoder"
  },
  {
    "objectID": "blog/posts/random_transformer/index.html#conclusions",
    "href": "blog/posts/random_transformer/index.html#conclusions",
    "title": "The Random Transformer",
    "section": "Conclusions",
    "text": "Conclusions\nI hope that was fun and informational! We covered a lot of ground. Wait‚Ä¶was that it? And the answer is, mostly, yes! New transformer architectures add lots of tricks, but the core of the transformer is what we just covered. Depending on what task you want to solve, you can also only the encoder or the decoder. For example, for understanding-heavy tasks such as classification, you can use the encoder stack with a linear layer on top. For generation-heavy tasks such as translation, you can use the encoder and decoder stacks. And finally, for free generation, as in ChatGPT or Mistral, you can use only the decoder stack.\nOf course, we also did lots of simplifications. Let‚Äôs briefly check which were the numbers in the original transformer paper:\n\nEmbedding dimension: 512 (4 in our example)\nNumber of encoders: 6 (6 in our example)\nNumber of decoders: 6 (6 in our example)\nFeed-forward dimension: 2048 (8 in our example)\nNumber of attention heads: 8 (2 in our example)\nAttention dimension: 64 (3 in our example)\n\nWe just covered lots of topics, but it‚Äôs quite interesting we can achieve impressive results by scaling up this math and doing smart training. We didn‚Äôt cover training in this blog post as the goal was to understand the math when using an existing model, but I hope this provided strong foundations for jumping into the training part. I hope you enjoyed this blog post!\nYou can also find a more formal document with the math in this PDF (recommended by HackerNews folks)."
  },
  {
    "objectID": "blog/posts/random_transformer/index.html#exercises",
    "href": "blog/posts/random_transformer/index.html#exercises",
    "title": "The Random Transformer",
    "section": "Exercises",
    "text": "Exercises\nHere are some exercises to practice your understanding of the transformer.\n\nWhat is the purpose of the positional encoding?\nHow does self-attention and encoder-decoder attention differ?\nWhat would happen if our attention dimension was too small? What about if it was too large?\nBriefly describe the structure of a feed-forward layer.\nWhy is the decoder slower than the encoder?\nWhat is the purpose of the residual connections and layer normalization?\nHow do we go from the decoder output to probabilities?\nWhy is picking the most likely next token every single time problematic?"
  },
  {
    "objectID": "blog/posts/random_transformer/index.html#resources",
    "href": "blog/posts/random_transformer/index.html#resources",
    "title": "The Random Transformer",
    "section": "Resources",
    "text": "Resources\n\nThe Illustrated Transformer\nAttention is all you need\nThe Annotated Transformer\nHugging Face free NLP course"
  },
  {
    "objectID": "blog/posts/gpu-poor-strike-back/index.html",
    "href": "blog/posts/gpu-poor-strike-back/index.html",
    "title": "The GPU Poor strike back",
    "section": "",
    "text": "Some months ago, SemiAnalysis published a flashy article with the premise that organizations with GPUs in the magnitude of tens of thousands had so many resources that the rest of the startups and researchers with few GPUs were wasting their time doing things such as local fine-tuning and over-quantization. According to them, the GPU Poor were not focusing on useful stuff.\n\n\nFirst of all, I am, proudly, GPU Poor (I have a 3080/12GB GPU and do many things in free Colab). And I couldn‚Äôt be prouder of what the ecosystem has done this year. We‚Äôre in a world in which TheBloke quantizes models at the accelerating speed of the model releases; a world where the Tekniums, local llamas, and aligners and unaligners will fine-tune the models before they are even announced; a world in which Tim Dettmers enables us to do 4-bit fine-tuning. These are exciting days!\n\n\nYes, most of the community uses the nice Llama, but guess what? We also have options. Microsoft dropped Phi - a 3B model I can run in my browser without sending anything to a server. Mistral unleashed Mixtral, a MoE with the same quality as the largest version of Llama, and running much faster. And we also have Qwen, Yi, Falcon, Deci, Starling, InternML, MPT, and StableLM, plus all their fine tunes and weird merges.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis year is the one in which we got tools such as LM Studio and Candle to run the models on-device, not sending any data to external servers. While the GPU Rich focused on somewhat similar user experiences (chatbots, LLM, maybe add some image or audio input here and then), the community can transcribe 2.5 hours of audio in less than 98 seconds, do image generation in real-time, and even video understanding, all running in our good ol‚Äô potatoes.\n\n\nWhile the Turbo GPU Rich spent weeks preparing their release and waiting to get those L8+ approvals, the tinkerers‚Äô communities of all kinds of disciplines, from artists to healthcare specialists, were combining open-source tools to generate music from images, figuring out how to enable fast loading of dozens of LoRAs models, or achieving sub-1-bit quantization.\n\n\nDon‚Äôt get me wrong. We greatly appreciate and love the amazing efforts of the GPU Rich that are releasing in the open their work and sharing with the community. We genuinely want them to succeed in their open and collaborative paths. But to imply that the GPU poor have no moat and are not contributing or doing something useful is naive.\n\n\nThe efforts of the GPU Poor and Middle Class are closing the access gap, making high-quality models more accessible than ever to people from different backgrounds, pushing open science forward, and taking hardware to its limits.\n\n\nThis was an exciting year for open-source, and we have a wide variety of labs and companies doing open work, GPU Poor, Middle Class, and Rich, all contributing in their own meaningful ways. Shoutouts to Kyutai, Answer.ai, 01.ai, BigCode, Mistral, Stability, Alibaba, Meta, and Microsoft. This year, we also got Nous Research, Skunkworks AI, Alignment Lab, Open Assistant, WizardLM, and so many other amazing communities.\n\n\nSo here we are, closing the year with an average of 3 new SOTA models daily, tackling all kinds of modalities, running models as powerful as GPT 3.5 in our computers, exploring AI feedback, building a thriving ecosystem of tools, and more. How can‚Äôt I be excited for next year?\n\n\nWhat‚Äôs on the wishlist for next year? More collaboration, transparency, and sharing. The vibrant GPU Poor ecosystem, where needs lead to novel research in asynchronous Discord servers and pushing the boundaries of libraries and hardware alike. The GPU Rich sharing research that can only be done at a huge scale and open-sourcing some of their models with licenses that will foster adoption and community. The bridging GPU Middle Class in direct touch with the Poor, understanding the masses‚Äô needs and training high-quality models under intense constraints.\n\n\nThe GPU Poor strike back! Vive la r√©volution Open Source!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage from Harrison Kinsley (Sentdex)"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "A minimal Introduction to Quantization\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Evals and Benchmarking\n\n\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSentence Embeddings. Cross-encoders and Re-ranking\n\n\n\n\n\nDeep Dive into Cross-encoders and Re-ranking\n\n\n\n\n\nJan 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Llama Hitchiking Guide to Local LLMs\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSentence Embeddings. Introduction to Sentence Embeddings\n\n\n\n\n\nEverything you wanted to know about sentence embeddings (and maybe a bit more)\n\n\n\n\n\nJan 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Random Transformer\n\n\n\n\n\nUnderstand how transformers work by demystifying all the math behind them\n\n\n\n\n\nJan 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe GPU Poor strike back\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2023\n\n\n\n\n\n\nNo matching items"
  }
]