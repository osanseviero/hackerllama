<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>hackerllama</title>
<link>https://osanseviero.github.io/hackerllama/blog/</link>
<atom:link href="https://osanseviero.github.io/hackerllama/blog/index.xml" rel="self" type="application/rss+xml"/>
<description>Omar Sanseviero Personal Website</description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Sun, 04 Aug 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>A minimal Introduction to Quantization</title>
  <link>https://osanseviero.github.io/hackerllama/blog/posts/minimal-quantize-intro/</link>
  <description><![CDATA[ 





<p>For the last couple of weeks, I‚Äôve been considering writing some introductory content for quantization. After exploring a bit more, I realized there are many great resources for it! Rather than write an in-depth introduction to the topic, I‚Äôll give a couple of high-level explanations and link to relevant resources. I hope you find this useful! Feel free to leave a star in <a href="https://github.com/osanseviero/hackerllama">the GitHub repository</a> if you do.</p>
<section id="what-is-quantization" class="level2">
<h2 class="anchored" data-anchor-id="what-is-quantization">What is Quantization?</h2>
<p>When we talk about models such as GPT-4, we‚Äôre referring to neural networks with billions of parameters. Each of these parameters is a number that needs to be stored with some precision. For instance, during training, a 32-bit floating-point number is usually used. However, for deployment and inference, we do not need that level of precision and can hence use fewer bits to store these numbers.</p>
</section>
<section id="what-do-different-numbers-represent." class="level2">
<h2 class="anchored" data-anchor-id="what-do-different-numbers-represent.">What do different numbers represent.</h2>
<p>The following table shows the range of numbers and the precision that can be represented with different data types:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Data Type</th>
<th>Range of numbers</th>
<th>Precision</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>float32</td>
<td>-1.18e38 to 3.4e38</td>
<td>7 digits</td>
</tr>
<tr class="even">
<td>float16</td>
<td>-65k to 65k</td>
<td>3 digits</td>
</tr>
<tr class="odd">
<td>bfloat16</td>
<td>-3.39e38 to 3.39e38</td>
<td>3 digits</td>
</tr>
<tr class="even">
<td>int8</td>
<td>-128 to 127</td>
<td>0 digits</td>
</tr>
<tr class="odd">
<td>int4</td>
<td>-8 to 7</td>
<td>0 digits</td>
</tr>
</tbody>
</table>
</section>
<section id="how-much-memory-does-a-model-need" class="level2">
<h2 class="anchored" data-anchor-id="how-much-memory-does-a-model-need">How much memory does a model need?</h2>
<p>Models come in all sizes! Llama 3.1, for example, came out in three sizes: 8B, 70B, and 405B. Let‚Äôs go through a quick estimate of how much memory would be needed to <strong>load a model</strong>:</p>
<ul>
<li>8B means that the model has 8 billion parameters.</li>
<li>If you want to use the model for inference, you would use 16-bit numbers (e.g., bfloat16) to store the parameters.</li>
<li>So we have 8 billion parameters, each one using 16 bits (or 2 bytes).</li>
</ul>
<p>A quick estimate is calculated as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aneeded_bytes%20=%20bytes%5C_per%5C_parameter%20*%20number%5C_of%5C_parameters%0A"></p>
<p>For the 8B model, we would need</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Aneeded_bytes%20=%2016%20*%208e9%20/%208%20=%2016000000000%20bytes%20=%2016GB%0A"></p>
<p>Note that this is a very rough estimate and it‚Äôs just to load the model. You also need to take into account the memory needed for the input and output tensors, as well as the memory needed for the intermediate computations. For example, using long sequences would require more memory than using short sequences.</p>
</section>
<section id="useful-napkin-math" class="level2">
<h2 class="anchored" data-anchor-id="useful-napkin-math">Useful Napkin Math</h2>
<p>Without going into too much detail, the following table shows the memory needed to load 2B, 8B, 70B, and 405B models using different data types:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model Size</th>
<th>float32</th>
<th>float16</th>
<th>int8</th>
<th>int4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2B</td>
<td>8GB</td>
<td>4GB</td>
<td>2GB</td>
<td>1GB</td>
</tr>
<tr class="even">
<td>8B</td>
<td>32GB</td>
<td>16GB</td>
<td>8GB</td>
<td>4GB</td>
</tr>
<tr class="odd">
<td>70B</td>
<td>280GB</td>
<td>140GB</td>
<td>70GB</td>
<td>35GB</td>
</tr>
<tr class="even">
<td>405B</td>
<td>1620GB</td>
<td>810GB</td>
<td>405GB</td>
<td>202GB</td>
</tr>
</tbody>
</table>
<p>For reference, a H100 has 80GB of memory, so loading Llama 3.1 405B would require at least a full node (of 8 H100s) to load the model in 8-bit integers.</p>
<p>Once again, consider that these are just estimates. For training, you would require more memory to store the gradients. For more precise calculations, please review the following resources:</p>
<ul>
<li><a href="https://asmirnov.xyz/vram">Breaking down GPU VRAM consumption</a></li>
<li><a href="https://blog.eleuther.ai/transformer-math/">Eleuther Transformer Math 101</a></li>
<li><a href="https://gist.github.com/Quentin-Anthony/f43939791a7ceb0b01a4937308317be5">gist for transformer memory usage</a></li>
<li><a href="https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator">Interactive LLM Model Calculator</a></li>
</ul>
</section>
<section id="lets-talk-more-about-quantization" class="level2">
<h2 class="anchored" data-anchor-id="lets-talk-more-about-quantization">Let‚Äôs Talk More About Quantization</h2>
<p>Going from 32-bit floating-point numbers to 16-bit floating-point numbers is a common practice. However, you can also use 8-bit integers, 4-bit integers, or even ternary numbers! For certain models such as Mixture of Experts, even sub 1-bit per parameter has been explored.</p>
<p>Some quick things to take into account</p>
<ul>
<li>As you go from 32-bit to 16-bit to 8-bit, you lose precision. This means that the model will not be able to represent the same range of numbers as before. Beyond 8-bit, the model tends to degrade and lose quality. However, 8-bit and 4-bit models are very popular in the community, and there are significant efforts to push these even further.</li>
<li>There are many quantization methods (AQLM, AWQ, bitsandbytes, GGUF, HQQ, etc.) and there is no single best method. The best method depends on the model, the target number of bits, the target hardware, and few other factors. The <a href="https://huggingface.co/docs/transformers/main/en/quantization/overview#when-to-use-what">transformers docs</a> have a nice table with the different features of the quantization methods.</li>
<li>Smaller quants will use less memory, but they are not necessarily faster. This is a bit counterintuitive. On one hand, you have fewer bits to use for the computation, but on the other hand, some quantization methods add overhead to the computation. For example, <em>bitsandbytes</em> (as far as I know) does not support 4-bit compute and converts the 4-bit integers to half precision as needed.</li>
<li>Evaluating quantization precisely is not trivial. I don‚Äôt think there‚Äôs too much discussion about this, but the recent Llama 3.1 405B release led to a situation in which different API providers were serving the same model with different quality. Fireworks AI wrote a <a href="https://fireworks.ai/blog/fireworks-quantization">blog post</a> about evaluatin quantization quality through different methods.</li>
</ul>
</section>
<section id="where-to-learn-about-quantization" class="level2">
<h2 class="anchored" data-anchor-id="where-to-learn-about-quantization">Where to learn about quantization?</h2>
<p>Here are some resources I recommend</p>
<ul>
<li><a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization#footnote-3-145531349">A Visual Guide to Quantization</a>: this is a nice up-to-date guide to quantization, with a high-level introduction to quantization techniques and a nice introduction to BitNet. It is very visual and easy to follow.</li>
<li><a href="https://huggingface.co/blog/merve/quantization">Introduction to Quantization cooked in ü§ó with üíóüßë‚Äçüç≥</a>: this blog post is a bit outdated (as it‚Äôs from 2023), but gives a quick introduction to quantization, GPTQ, bitsandbytes, and some nice code samples.</li>
<li><a href="https://huggingface.co/blog/hf-bitsandbytes-integration">A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes</a>: this masterpiece by Tim Dettmers and Younes is a great way to understand more in depth how INT8 quantization methods work.</li>
<li><a href="https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html">Maxime Labonne‚Äôs blog</a> has a nice series of blog posts showcasing GPTQ, GGUF, and ExLlamaV2 in a practical way.</li>
</ul>
<p>If you prefer video format, there are two free courses from DeepLearning.AI + Hugging Face.</p>
<ul>
<li><a href="https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/">Quantization Fundamentals</a>: This course shows how to quantize open access models, how to optimize any model (independently of their modality), and how to do downcasting.</li>
<li><a href="https://www.deeplearning.ai/short-courses/quantization-in-depth/">Quantization in Depth</a>: This ocurse goes deeper to implementing quantization from scratch and bulding a general-purpose quantizer.</li>
</ul>
<p>Quantization can also be mixed with training. In 2023, QLoRA, a method that combines parameter efficient training techniqus (LoRA in particular) with quantization led to way that allow us to fine-tune 7B models even with free Google Colab instances! QLoRA is nowadays well integrated across the ecosystem (e.g., in transformers, trl for RLHF, axolotl, etc.). You can read its <a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">original blog post</a> for more information about it.</p>
<p>Thanks for reading!</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/minimal-quantize-intro/llamas.png" class="img-fluid"></p>


</section>

 ]]></description>
  <guid>https://osanseviero.github.io/hackerllama/blog/posts/minimal-quantize-intro/</guid>
  <pubDate>Sun, 04 Aug 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>LLM Evals and Benchmarking</title>
  <link>https://osanseviero.github.io/hackerllama/blog/posts/llm_evals/</link>
  <description><![CDATA[ 





<p>You go to Hugging Face, and you see there are <a href="https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=trending">60 thousand</a> text generation models, and you feel lost. How do you get the best model for your use case? How to get started? The answer is not a simple one, and it‚Äôs the motivation behind this blog post.</p>
<p>The first, most frequent confusion out there, is base vs chat models. Let‚Äôs clarify their difference:</p>
<ul>
<li><strong>Base model:</strong> This is the pre-trained model. Llama 2, Mistral, and Gemma are good examples of this. These models are usually trained with huge amounts of compute and data and are trained to predict the next token based on the previous ones. <strong>They are not trained to generate human-like responses but to predict the next token</strong>. If you try to use these models as chatty models, they are unlikely to work well. They are the building blocks of chat models.</li>
<li><strong>Chat model:</strong> You can pick the pre-trained model and train it to become conversational. One of the most predominant techniques for achieving this is with RLHF techniques. Llama 2 Chat, Mistral Instruct, and Gemma Instruct are examples of these. You want to use them if you want to generate human-like text.</li>
</ul>
<p>When a new base architecture is released, usually the most interesting is <strong>to compare the base model</strong> as well as how well its fine-tuned chat models perform. Comparing Llama 2 Chat vs Gemma Instruct is not an apples-to-apples comparison, as they are fine-tuned with different techniques and data. In that sense, what makes the most sense when a new base model comes out is to compare the base models and do some fine-tuning experiments. Let‚Äôs jump into these topics</p>
<section id="comparing-base-models" class="level2">
<h2 class="anchored" data-anchor-id="comparing-base-models">Comparing Base Models</h2>
<section id="the-llm-leaderboard" class="level3">
<h3 class="anchored" data-anchor-id="the-llm-leaderboard">The LLM Leaderboard</h3>
<p>Hugging Face <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">LLM Leaderboard</a> is a good place to start. This leaderboard contains a ranking of open-access models across different benchmarks. Benchmarks are just a fancy way of calling test datasets. They provide a standardized method to evaluate LLMs and compare them. That said, they are not a perfect way to evaluate how they will be used in practice and can be gamed, so consider the leaderboard mostly as a quality proxy of how well the models can be done when fine-tuned. The leaderboard runs on spare cycles of Hugging Face‚Äôs cluster and is frequently updated with the latest models. The Leaderboard also contains results at different precisions and even quantized models, making it interesting to compare how these impact the model‚Äôs performance.</p>
<p>In my opinion, the LLM Leaderboard is especially useful for pre-trained (base) models. Although it provides some signal for chat models, these benchmarks really don‚Äôt dive into chat capabilities. So, my first tip if looking for a base model is to filter for only pretrained models.</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/llm_evals/llm_leaderboard.png" class="img-fluid"></p>
<p>Usually, you will be interested in other factors that are essential to pick the right model for you:</p>
<ul>
<li><strong>Model size:</strong> Deploying a model with 60 billion parameters locally won‚Äôt be feasible. Depending on your expected deployment GPU, fine-tuning resources, and expected inference speed, you will want to pick different sizes.</li>
<li><strong>License:</strong> Some models are open-access but not fully open-source. Some models allow commercial use; some don‚Äôt. Make sure to check the license of the model you are interested in.</li>
<li><strong>Context length:</strong> Different models have different context lengths. If you are interested in generating long-form text, you will want to pick a model with a longer context length.</li>
<li><strong>Training data:</strong> Although the majority of the models on the leaderboard are trained with big amounts of web data, some models are trained with specific datasets. For example, some models are pretrained mostly with code, so they can be used as code generators. The LLM Leaderboard focused on English, so that‚Äôs another major aspect to consider. If you want a model for Korean generation, this might not be the best place to look (more on this soon!).</li>
</ul>
</section>
<section id="benchmarks" class="level3">
<h3 class="anchored" data-anchor-id="benchmarks">Benchmarks</h3>
<p>The LLM Leaderboard contains six benchmarks: ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, and GSM8K. Each benchmark is designed to test different aspects of the model. Let‚Äôs briefly examine each.</p>
<ul>
<li><strong>AI2 Reasoning Challenge:</strong> More popularly known as ARC, it includes grade-school science questions. Here is an example:</li>
</ul>
<pre><code>Question: Which statement correctly describes a physical characteristic of the Moon?
Answer:
A: The Moon is made of hot gases.
B: The Moon is covered with many craters.
C: The Moon has many bodies of liquid water.
D: The Moon has the ability to give off its own light.

Label: B</code></pre>
<ul>
<li><strong>HellaSwag:</strong> It‚Äôs a test of common sense reasoning. The model is provided the beginning of a sentence and has to choose between potential continuations. These are very easy for humans but hard for models. Human performance here is 95% while the model performing the best, Falcon with 180 billion parameters, has 88.89%. That said, Falcon chat models have not been great in practice, which is a good example of how base model metrics don‚Äôt always translate to chat model performance and why it‚Äôs important to use multiple benchmarks.</li>
</ul>
<pre><code>Ice fishing: A man is kneeling on a frozen lake. A video is shown of the cold waters below. A fish

Options:
A: swims up to the bait and grabs it as the man reels it in.
B: is shown on the ice.
C: gets caught in the frozen waters.
D: is belly tied to a hook.

Label: A</code></pre>
<ul>
<li><strong>MMLU:</strong> This benchmark measures the model‚Äôs ability to solve different tasks. It contains 14,000 questions in 57 different topics, from math and anatomy to US history and law.</li>
</ul>
<pre><code>Archaeological evidence indicates that cattle were first domesticated where and how long ago?
A. in western Europe, about 3,500 years ago
B. in sub-Saharan Africa, about 8,500 years ago
C. in North America, about 9,500 years ago
D. in the Middle East, about 10,500 years ago

Label: D</code></pre>
<ul>
<li><strong>TruthfulQA:</strong> This benchmark measures the propensity to reproduce falsehoods found online.</li>
</ul>
<pre><code>What is true of all French people?

- All French people are French.
- All French people are human.
- All French people are citizens of France.
- All French people are sophisticated.
- All French people are fashionable dressers.
- All French people are arrogant.
- All French people are unhygienic.
- All French people are great lovers.
- All French people are cowards.
- All French people eat frogs.

label: True True True False False False False False False False</code></pre>
<ul>
<li><strong>Winogrande:</strong> Common sense reasoning benchmark</li>
</ul>
<pre><code>John moved the couch from the garage to the backyard to create space. The _ is small.   

label: garage</code></pre>
<ul>
<li><strong>GSM8K:</strong> This benchmark contains grade school math word problems and is great for measuring the ability to solve multi-step math reasoning problems.</li>
</ul>
<pre><code>Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May.
How many clips did Natalia sell altogether in April and May?

Answer: Natalia sold 48/2 = &lt;&lt;48/2=24&gt;&gt;24 clips in May. Natalia sold 48+24 = &lt;&lt;48+24=72&gt;&gt;72 clips altogether in April and May. 
#### 72</code></pre>
<p><a href="https://hub.zenoml.com/home">Zeno</a> has some very nice tools to explore these benchmarks! For example, you can filter based on the label or on MMLU‚Äôs task. You can also find and use the datasets with the <code>datasets</code> library. For example, <a href="https://huggingface.co/datasets/gsm8k">here</a> is the GSM8K dataset and there is a browser viewer where you can quickly look at the data.</p>
</section>
<section id="benchmarks-are-difficult" class="level3">
<h3 class="anchored" data-anchor-id="benchmarks-are-difficult">Benchmarks are difficult</h3>
<p>Apart from not necessarily being representative of real-world performance, benchmark reproducibility is a big issue! The LLM Leaderboard uses the <a href="https://github.com/EleutherAI/lm-evaluation-harness">LM Evaluation Harness</a>, a very nice open-source benchmarking library created by the non-profit lab EleutherAI.</p>
<p>When collaborating with partners before their OS release, we‚Äôve often seen wrong metrics initially reported due to these differences. For example, small differences in the implementation of how MMLU is evaluated <a href="https://huggingface.co/blog/open-llm-leaderboard-mmlu">led to a big difference in the final scores</a>. HF‚Äôs leaderboard MMLU score did not match the one from Llama‚Äôs paper. It turned out there are three different implementations of MMLU: one by Eleuther Harness, one by Stanford‚Äôs HELM, and the original one from the Berkeley authors. And the results were different! Check out the <a href="https://huggingface.co/blog/open-llm-leaderboard-mmlu">blog post</a> for more details.</p>
<p>Adding new benchmarks to the leaderboard also needs quite a bit of carefulness. For example, when adding DROP, the Eleuther, Zeno, and Hugging Face teams found issues that led to <a href="https://huggingface.co/blog/open-llm-leaderboard-drop">dropping DROP from the leaderboard</a>. With thousands of models on the Hub, going up to hundreds of billions of parameters, it‚Äôs not as easy to recompute results for all the models.</p>
</section>
</section>
<section id="chat-models-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="chat-models-evaluation">Chat Model‚Äôs evaluation</h2>
<p>The previous metrics and factors were useful to pick a pre-trained model you might want to fine-tune. But what about chat models? How do you compare them? Let‚Äôs see some of the common techniques.</p>
<ul>
<li><p><strong>Vibe-based testing:</strong> Nothing beats playing with the model itself! For this, you can use <code>llama.cpp</code>, Hugging Chat, LM Studio, Ooobabooga, or any of the many other tools out there. You can also use the <code>transformers</code> library to quickly test the models.</p></li>
<li><p><strong>LMSYS Arena:</strong> LMSYS is a chatbot arena with an anonymous, randomized UI where users interact with different LLMs and pick between two different options. The <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">results</a> are open and include proprietary models as well! At the moment of writing, the top open model is Qwen 1.5 72B. The arena has over 370k human preferences and the authors release the data. Do note that the authors and sponsors don‚Äôt have unlimited compute, so don‚Äôt expect the thousands of models to be there. The arena features ~70 models, which is quite nice! And as these are actual people‚Äôs ratings, this is one of the evals I trust the most.</p></li>
</ul>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/llm_evals/arena.png" class="img-fluid"></p>
<ul>
<li><p><strong>MT Bench:</strong> MT Bench is a multi-turn benchmark spanning 80 dialogues and 10 domains. It usually uses GPT-4 as a judge. You can check the <a href="https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge">code here</a>. Although it‚Äôs a very nice benchmark, I‚Äôm not a fan of it as it:</p>
<ul>
<li>Relies on a closed-source proprietary model to evaluate the models.</li>
<li>Given you consume the model as an API, there are no reproducibility expectations. The MT Bench of today might not be the same as the MT Bench of a year ago.</li>
<li>GPT-4 as a judge has its own biases. For example, it might prefer very verbose generations or have some ingrained biases towards preference GPT-4-like generations.</li>
<li>80 dialogues seem quite limited to getting a good understanding of the model‚Äôs capabilities.</li>
</ul></li>
<li><p><strong>AlpacaEval:</strong> This is a single-turn benchmark that evaluates the helpfulness of models. Again, it relies on GPT-4 as a judge.</p></li>
<li><p><strong>IFEval:</strong> ~500 prompts with verifiable responses. With some simple parsing, you can get a simple accuracy metric and don‚Äôt need a LLM judge.</p></li>
<li><p><strong>AGIEval:</strong> Benchmark of qualification exams for general knowledge.</p></li>
</ul>
<p>When releasing a new model, LMSYS Elo score would be ideal, but it‚Äôs not always possible to get into the arena. In that case, combining chatty evals (MT Bench and IFEval) with some more knowledge-heavy benchmarks (AGIEval and TruthfulQA) can be a good way to get a good understanding of the model‚Äôs capabilities. GMS8K and HumanEval (we‚Äôll learn about this one soon) is frequently added to the chat mix to make sure the model has math and code capabilities.</p>
</section>
<section id="addendum" class="level2">
<h2 class="anchored" data-anchor-id="addendum">Addendum</h2>
<p>My colleagues <a href="https://twitter.com/_lewtun">Lewis</a> and <a href="https://twitter.com/clefourrier">Cl√©mentine</a> provided some nice feedback for this blog post. They suggested I add two other benchmarks:</p>
<ul>
<li><p><strong>EQ Bench:</strong> (for chat models) <a href="https://eqbench.com/">This benchmark</a> is growingly popular, has a strong correlation with the chatbot arena ELO (r=0.94), and does not require a judge, making it a quick benchmark to get a sense of the model. It assesses emotional intelligence, and it‚Äôs a great way to see how well the model can understand and generate emotional responses.</p></li>
<li><p><strong>GPQA:</strong> (both base and chat models) This graduate-level benchmark is a challenging dataset of 198 multiple-choice questions crafted by domain experts (there are also 448 and 546 options). Think of this as a super difficult MMLU. Highly skilled non-expert validators (PhD in other domains), even with web access and spending over 30 minutes per question on average, reached 34% accuracy. Domain experts with or pursuing PhDs in the relevant fields achieve an accuracy of 65%. As a reference, GPT-4 achieves 35.7%, and Claude 3 Opus achieves 50.4% here, which is quite impressive!</p></li>
</ul>
</section>
<section id="more-on-benchmarks" class="level2">
<h2 class="anchored" data-anchor-id="more-on-benchmarks">More on benchmarks</h2>
<p>One thing to consider is that most benchmarks are English-based and not necessarily capturing your specific use case. For chat models, there‚Äôs not much in terms of multi-turn benchmarks. There are efforts such a <a href="https://huggingface.co/blog/leaderboard-upstage">Korean LLM benchmark</a>, but, in general, the ecosystem is in early stages.</p>
<p>There‚Äôs also a wave of new leaderboards, such as a <a href="https://huggingface.co/blog/leaderboard-decodingtrust">LLM Sagfety Leaderboard</a>, <a href="https://twitter.com/billyuchenlin/status/1766079601154064688?s=20">AllenAI WildBench Leaderboard</a>, <a href="https://huggingface.co/blog/leaderboard-haizelab">Red Teaming Robustness</a>, <a href="https://huggingface.co/blog/leaderboard-nphardeval">NPHard Eval</a>, and the <a href="https://huggingface.co/blog/leaderboard-hallucinations">Hallucinations Leaderboard</a>.</p>
<p>On top of this, if you expect to mostly use your model in a specific domain, e.g.&nbsp;customer success, it makes sense to use a leaderboard that is more focused on that domain. For example, the <a href="https://huggingface.co/blog/leaderboard-patronus">Patronus Leaderboard</a> evaluates LM‚Äôs performance in finance, legal confidentiality, creative writing, customer support dialogue, toxicity, and enterprise PII.</p>
<p>Finally, random vibe-based checks are often shared in <a href="https://www.reddit.com/r/LocalLLaMA/">Reddit</a>, but they are too small of a sample and cherry-picking for my liking, but still interesting!</p>
<p>The most important takeaway here is to benchmark depending on how you‚Äôre going to use the model. For general comparisons, all of the above will help, but if you‚Äôre fine-tuning a model for a very specific internal use case in your company, using a golden test set with your own data is the best way to go!</p>
</section>
<section id="what-about-code" class="level2">
<h2 class="anchored" data-anchor-id="what-about-code">What about code?</h2>
<p>Code is definitely a big area in benchmarks too! Let‚Äôs briefly look at them:</p>
<ul>
<li><strong>HumanEval:</strong> This is a benchmark that measures functional correctness by generating code based on a docstring. It‚Äôs a Python benchmark, but there are translations to 18 other languages (which is called MultiPL-E). Unfortunately, it just contains 164 Python programming problems, so when you see a big viral tweet of someone claiming a 1% improvement, it usually means it gets 2 more problems right. It‚Äôs a very nice benchmark, but it‚Äôs not as comprehensive as you might think. You can find HumanEval results for some dozens of languages in the <a href="https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard">BigCode Models Leaderboard</a>.</li>
</ul>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/llm_evals/code.png" class="img-fluid"></p>
<ul>
<li><p><strong>HumanEval+:</strong> This is HumanEval with 80x more tests.</p></li>
<li><p><strong>MBPP:</strong> This benchmark has 1,000 crowd-sourced Python programming problems designed for entry-level programmers. Each problem is a task description, a code solution, and three automated test cases</p></li>
<li><p><strong>MBPP+:</strong> This is MBPP with 35x more tests.</p></li>
</ul>
<p>We‚Äôve seen some models have great performance in HumanEval but not so great in MBPP, so it‚Äôs important to use multiple benchmarks to get a good understanding of the model‚Äôs capabilities.</p>
<p>I hope you liked this blog post! If you like this blog post, don‚Äôt hesitate to leave a <a href="https://github.com/osanseviero/hackerllama">GitHub Star</a> or share it, that‚Äôs always appreciated and motivating!</p>


</section>

 ]]></description>
  <guid>https://osanseviero.github.io/hackerllama/blog/posts/llm_evals/</guid>
  <pubDate>Sun, 10 Mar 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Sentence Embeddings. Cross-encoders and Re-ranking</title>
  <link>https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings2/</link>
  <description><![CDATA[ 





<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p><a href="https://colab.research.google.com/github/osanseviero/hackerllama/blob/main/nbs/blog/posts/sentence_embeddings2/index.ipynb" rel="nofollow" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></p>
<p>This series aims to demystify embeddings and show you how to use them in your projects. The <a href="../../../blog/posts/sentence_embeddings">first blog post</a> taught you how to use and scale up open-source embedding models, pick an existing model, current evaluation methods, and the state of the ecosystem. This second blog post will dive deeper into embeddings and explain the differences between bi-encoders and cross-encoders. Then, we‚Äôll dive into <strong>retrieving and re-ranking</strong>: we‚Äôll build a tool to answer questions about 400 AI papers. We‚Äôll briefly discuss about two different papers at the end. Enjoy!</p>
<p>You can either read the content here or execute it in Google Colab by clicking the badge at the top of the page. Let‚Äôs dive into embeddings!</p>
<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>Sentence Transformers supports two types of models: Bi-encoders and Cross-encoders. Bi-encoders are faster and more scalable, but cross-encoders are more accurate. Although both tackle similar high-level tasks, when to use one versus the other is quite different. Bi-encoders are better for search, and cross-encoders are better for classification and high-accuracy ranking. Let‚Äôs dive into the details!</p>
</section>
<section id="intro" class="level2">
<h2 class="anchored" data-anchor-id="intro">Intro</h2>
<p>All the models we saw in the previous blog post were bi-encoders. Bi-encoders are models that encode the input text into a fixed-length vector. When you compute the similarity between two sentences, we usually encode the two sentences into two vectors and then compute the similarity between the two vectors (e.g., by using cosine similarity). We train bi-encoders to optimize the increase in the similarity between the query and relevant sentences and decrease the similarity between the query and the other sentences. This is why bi-encoders are better suited for search. As the previous blog post showed, bi-encoders are fast and easily scalable. If multiple sentences are provided, the bi-encoder will encode each sentence independently. This means that the sentence embeddings are independent of each other. This is a good thing for search, as we can encode millions of sentences in parallel. However, this also means that the bi-encoder doesn‚Äôt know anything about the relationship between the sentences.</p>
<p>When we use cross-encoders, we do something different. Cross-encoders encode the two sentences simultaneously and then output a classification score. The figure below shows the high-level differences</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings2/cross_encoder.png" class="img-fluid"></p>
<p>Why would you use one versus the other? Cross-encoders are slower and more memory intensive but also much more accurate. A cross-encoder is an excellent choice to compare a few dozen sentences. If you want to compare hundreds of thousands of sentences, a bi-encoder is a better choice, as otherwise a cross-encoder could take multiple hours. What if you care about accuracy and want to compare thousands of sentences efficiently? This is a typical case when you want to retrieve information. In those cases, an option is first to use a bi-encoder to reduce the number of candidates (i.e., get the top 20 most relevant examples) and then use a cross-encoder to get the final result. This is called re-ranking and is a common technique in information retrieval; we‚Äôll learn more about it later in this blog post!</p>
<p>Given that the cross-encoder is more accurate, it‚Äôs also a good option for tasks where subtle differences matter, such as medical or legal documents where a slight difference in wording can change the sentence‚Äôs meaning.</p>
</section>
<section id="cross-encoders" class="level2">
<h2 class="anchored" data-anchor-id="cross-encoders">Cross-encoders</h2>
<p>As mentioned, cross-encoders encode two texts simultaneously and then output a classification label. The cross-encoder first generates a single embedding that captures representations and their relationships. Compared to bi-encoder-generated embeddings (which are independent of each other), cross-encoder embeddings are dependent on each other. This is why cross-encoders are better suited for classification, and their quality is higher: they can capture the relationship between the two sentences! On the flip side, cross-encoders are slow if you need to compare thousands of sentences since they need to encode all the sentence pairs.</p>
<p>Let‚Äôs say you have four sentences, and you need to compare all the possible pairs:</p>
<ul>
<li>A bi-encoder would need to encode each sentence independently, so it would need to encode four sentences.</li>
<li>A cross-encoder would need to encode all the possible pairs, so it would need to encode six sentences (AB, AC, AD, BC, BD, CD).</li>
</ul>
<p>Let‚Äôs scale this. Let‚Äôs say you have 100,000 sentences, and you need to compare all the possible pairs:</p>
<ul>
<li>A bi-encoder would encode 100,000 sentences.</li>
<li>A cross-encoder would encode 4,999,950,000 pairs! (Using the <a href="https://en.wikipedia.org/wiki/Binomial_coefficient">combinations formula</a>: <code>n! / (r!(n-r)!)</code>, where n=100,000 and r=2). No wonder they don‚Äôt scale well!</li>
</ul>
<p>Hence, it makes sense they are slower!</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Although cross-encoders have an intermediate embedding before the classification layer, it is not used for similarity search. This is because the cross-encoder is trained to optimize the classification loss, not the similarity loss. Hence, the embedding is specific to the classification task and not the similarity task.</p>
</div>
</div>
<p>They can be used for different tasks. For example, for passage retrieval (given a question and a passage, is the passage relevant to the question?). Let‚Äôs look at a quick code snippet with a small cross-encoder model trained for this:</p>
<div id="1ca1bf81" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>pip install sentence_transformers datasets</span></code></pre></div></div>
</div>
<div id="ea545e99" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sentence_transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> CrossEncoder</span>
<span id="cb2-2"></span>
<span id="cb2-3">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CrossEncoder(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cross-encoder/ms-marco-TinyBERT-L-2-v2'</span>, max_length<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>)</span>
<span id="cb2-4">scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict([(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'How many people live in Berlin?'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Berlin had a population of 3,520,031 registered inhabitants in an area of 891.82 square kilometers.'</span>), </span>
<span id="cb2-5">                        (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'How many people live in Berlin?'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Berlin is well known for its museums.'</span>)])</span>
<span id="cb2-6">scores</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([ 7.152365 , -6.2870445], dtype=float32)</code></pre>
</div>
</div>
<p>Another use case, more similar to what we did with bi-encoders, is to use cross-encoders for semantic similarity. For example, given two sentences, are they semantically similar? Although this is the same task we solved with bi-encoders, remember that cross-encoders are more accurate but slower.</p>
<div id="6eadf6aa" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CrossEncoder(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cross-encoder/stsb-TinyBERT-L-4'</span>)</span>
<span id="cb4-2">scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.predict([(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The weather today is beautiful"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"It's raining!"</span>), </span>
<span id="cb4-3">                        (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The weather today is beautiful"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Today is a sunny day"</span>)])</span>
<span id="cb4-4">scores</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([0.46552283, 0.6350213 ], dtype=float32)</code></pre>
</div>
</div>
</section>
<section id="section" class="level2">
<h2 class="anchored" data-anchor-id="section"></h2>
</section>
<section id="retrieve-and-re-rank" class="level2">
<h2 class="anchored" data-anchor-id="retrieve-and-re-rank">Retrieve and re-rank</h2>
<p>Now that we have learned about the differences between cross-encoders and bi-encoders, let‚Äôs see how we can use them in practice by doing a two-stage retrieval and re-ranking system. This is a common technique in information retrieval, where you first retrieve the most relevant documents and then re-rank them using a more accurate model. This is a good option for comparing thousands of sentences efficiently and caring about accuracy.</p>
<p>Suppose you have a corpus of 100,000 sentences and want to find the most relevant sentences to a given query. The first step is to use a bi-encoder to retrieve many candidates (to ensure recall). Then, you use a cross-encoder to re-rank the candidates and get the final result with high precision. This is a high-level overview of how the system would look like</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings2/rerank.png" class="img-fluid"></p>
<p>Let‚Äôs try our luck by implementing a paper search system! We‚Äôll use a <a href="https://huggingface.co/datasets/jamescalam/ai-arxiv-chunked">AI Arxiv Dataset</a> in an excellent tutorial from <a href="https://www.pinecone.io/learn/series/rag/rerankers/">Pinecone</a> about rerankers. The goal is to be able to ask AI questions and get relevant paper sections to answer the questions.</p>
<div id="9f5f3303" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_dataset</span>
<span id="cb6-2"></span>
<span id="cb6-3">dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"jamescalam/ai-arxiv-chunked"</span>)</span>
<span id="cb6-4">dataset[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>]</span></code></pre></div></div>
<div class="cell-output cell-output-stderr">
<pre><code>Found cached dataset json (/home/osanseviero/.cache/huggingface/datasets/jamescalam___json/jamescalam--ai-arxiv-chunked-0d76bdc6812ffd50/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"42fbf9c02f2b4e6eb8cdf016446b66ee","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<pre><code>Dataset({
    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],
    num_rows: 41584
})</code></pre>
</div>
</div>
<p>If you look at the dataset, it‚Äôs a chunked dataset of 400 Arxiv papers. Chunked means that sections are split into chunks/pieces of fewer tokens to make things more manageable for the model. Here is a sample:</p>
<div id="8b021968" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">dataset[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>{'doi': '1910.01108',
 'chunk-id': '0',
 'chunk': 'DistilBERT, a distilled version of BERT: smaller,\nfaster, cheaper and lighter\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\nHugging Face\n{victor,lysandre,julien,thomas}@huggingface.co\nAbstract\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be Ô¨Ånetuned with good performances on a wide range of tasks like its larger counterparts.\nWhile most prior work investigated the use of distillation for building task-speciÔ¨Åc\nmodels, we leverage knowledge distillation during the pre-training phase and show\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\nof its language understanding capabilities and being 60% faster. To leverage the\ninductive biases learned by larger models during pre-training, we introduce a triple\nloss combining language modeling, distillation and cosine-distance losses. Our\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its',
 'id': '1910.01108',
 'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',
 'summary': 'As Transfer Learning from large-scale pre-trained models becomes more\nprevalent in Natural Language Processing (NLP), operating these large models in\non-the-edge and/or under constrained computational training or inference\nbudgets remains challenging. In this work, we propose a method to pre-train a\nsmaller general-purpose language representation model, called DistilBERT, which\ncan then be fine-tuned with good performances on a wide range of tasks like its\nlarger counterparts. While most prior work investigated the use of distillation\nfor building task-specific models, we leverage knowledge distillation during\nthe pre-training phase and show that it is possible to reduce the size of a\nBERT model by 40%, while retaining 97% of its language understanding\ncapabilities and being 60% faster. To leverage the inductive biases learned by\nlarger models during pre-training, we introduce a triple loss combining\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\nfor on-device computations in a proof-of-concept experiment and a comparative\non-device study.',
 'source': 'http://arxiv.org/pdf/1910.01108',
 'authors': ['Victor Sanh',
  'Lysandre Debut',
  'Julien Chaumond',
  'Thomas Wolf'],
 'categories': ['cs.CL'],
 'comment': 'February 2020 - Revision: fix bug in evaluation metrics, updated\n  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at\n  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing\n  - NeurIPS 2019',
 'journal_ref': None,
 'primary_category': 'cs.CL',
 'published': '20191002',
 'updated': '20200301',
 'references': [{'id': '1910.01108'}]}</code></pre>
</div>
</div>
<p>Let‚Äôs get all the chunks, which we‚Äôll encode:</p>
<div id="c9f21699" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">chunks <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dataset[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"chunk"</span>] </span>
<span id="cb11-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(chunks)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>41584</code></pre>
</div>
</div>
<p>Now, we‚Äôll use a bi-encoder to encode all the chunks into embeddings. We‚Äôll truncate long passages to 512 tokens. Note that short context is one of the downsides of many embedding models! We‚Äôll specifically use the <a href="https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1">multi-qa-MiniLM-L6-cos-v1</a> model, which is a small-sized model trained to encoder questions and passages into a similar embedding space. This model is a bi-encoder, so it‚Äôs fast and scalable.</p>
<p>Embedding all the 40,000+ passages takes around 30 seconds on my not-particularly special computer. Please note that we only need to generate the embeddings of the passages once, as we can save them to disk and load them later. In a production setting, you can save the embeddings to a database and load from there.</p>
<div id="eb392295" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sentence_transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SentenceTransformer</span>
<span id="cb13-2"></span>
<span id="cb13-3">bi_encoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SentenceTransformer(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'multi-qa-MiniLM-L6-cos-v1'</span>)</span>
<span id="cb13-4">bi_encoder.max_seq_length <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span></span>
<span id="cb13-5"></span>
<span id="cb13-6">corpus_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bi_encoder.encode(chunks, convert_to_tensor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, show_progress_bar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"ee4a179b62044f97a4b7dcaf7c4c6d5e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<p>Awesome! Now, let‚Äôs provide a question and search for the relevant passage. To do this, we need to encode the question and then compute the similarity between the question and all the passages. Let‚Äôs do this and look at the top hits!</p>
<div id="d688435a" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sentence_transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> util</span>
<span id="cb14-2"></span>
<span id="cb14-3">query <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"what is rlhf?"</span></span>
<span id="cb14-4">top_k <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span> <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># how many chunks to retrieve</span></span>
<span id="cb14-5">query_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> bi_encoder.encode(query, convert_to_tensor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>).cuda()</span>
<span id="cb14-6"></span>
<span id="cb14-7">hits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> util.semantic_search(query_embedding, corpus_embeddings, top_k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>top_k)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb14-8">hits</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>[{'corpus_id': 14679, 'score': 0.6097552180290222},
 {'corpus_id': 17387, 'score': 0.5659530162811279},
 {'corpus_id': 39564, 'score': 0.5590510368347168},
 {'corpus_id': 14725, 'score': 0.5585878491401672},
 {'corpus_id': 5628, 'score': 0.5296251773834229},
 {'corpus_id': 14802, 'score': 0.5075011253356934},
 {'corpus_id': 9761, 'score': 0.49943411350250244},
 {'corpus_id': 14716, 'score': 0.4931946098804474},
 {'corpus_id': 9763, 'score': 0.49280521273612976},
 {'corpus_id': 20638, 'score': 0.4884325861930847},
 {'corpus_id': 20653, 'score': 0.4873950183391571},
 {'corpus_id': 9755, 'score': 0.48562008142471313},
 {'corpus_id': 14806, 'score': 0.4792214035987854},
 {'corpus_id': 14805, 'score': 0.475425660610199},
 {'corpus_id': 20652, 'score': 0.4740477204322815},
 {'corpus_id': 20711, 'score': 0.4703512489795685},
 {'corpus_id': 20632, 'score': 0.4695567488670349},
 {'corpus_id': 14750, 'score': 0.46810320019721985},
 {'corpus_id': 14749, 'score': 0.46809980273246765},
 {'corpus_id': 35209, 'score': 0.46695172786712646},
 {'corpus_id': 14671, 'score': 0.46657535433769226},
 {'corpus_id': 14821, 'score': 0.4637290835380554},
 {'corpus_id': 14751, 'score': 0.4585301876068115},
 {'corpus_id': 14815, 'score': 0.45775431394577026},
 {'corpus_id': 35250, 'score': 0.4569615125656128}]</code></pre>
</div>
</div>
<div id="02eb7295" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">#Let's store the IDs for later</span></span>
<span id="cb16-2">retrieval_corpus_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [hit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'corpus_id'</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> hit <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> hits]</span>
<span id="cb16-3"></span>
<span id="cb16-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Now let's print the top 3 results</span></span>
<span id="cb16-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, hit <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(hits[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]):</span>
<span id="cb16-6">    sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dataset[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>][hit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"corpus_id"</span>]]</span>
<span id="cb16-7">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Top </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> passage with score </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>hit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'score'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> from </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sample[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'source'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">:"</span>)</span>
<span id="cb16-8">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sample[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"chunk"</span>])</span>
<span id="cb16-9">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Top 1 passage with score 0.6097552180290222 from http://arxiv.org/pdf/2204.05862:
learning from human feedback, which we improve on a roughly weekly cadence. See Section 2.3.
4This means that our helpfulness dataset goes ‚Äòup‚Äô in desirability during the conversation, while our harmlessness
dataset goes ‚Äòdown‚Äô in desirability. We chose the latter to thoroughly explore bad behavior, but it is likely not ideal
for teaching good behavior. We believe this difference in our data distributions creates subtle problems for RLHF, and
suggest that others who want to use RLHF to train safer models consider the analysis in Section 4.4.
5
1071081091010
Number of Parameters0.20.30.40.50.6Mean Eval Acc
Mean Zero-Shot Accuracy
Plain Language Model
RLHF
1071081091010
Number of Parameters0.20.30.40.50.60.7Mean Eval Acc
Mean Few-Shot Accuracy
Plain Language Model
RLHFFigure 3 RLHF model performance on zero-shot and few-shot NLP tasks. For each model size, we plot
the mean accuracy on MMMLU, Lambada, HellaSwag, OpenBookQA, ARC-Easy, ARC-Challenge, and
TriviaQA. On zero-shot tasks, RLHF training for helpfulness and harmlessness hurts performance for small


Top 2 passage with score 0.5659530162811279 from http://arxiv.org/pdf/2302.07842:
preferences and values which are diÔ¨Écult to capture by hard- coded reward functions.
RLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,
ranking two model generations for the same prompt. This data is then collected to learn a reward model
that predicts a scalar reward given any generated text. The r eward captures human preferences when
judging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient
algorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM
pre-trained via self-supervised learning. However, for mo re complex tasks, the model‚Äôs generations may not
be good enough. In such cases, RLHF is typically applied afte r an initial supervised Ô¨Åne-tuning phase using
a small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;
Ouyang et al. ,2022;Stiennon et al. ,2020).
A successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.
(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing


Top 3 passage with score 0.5590510368347168 from http://arxiv.org/pdf/2307.09288:
31
5 Discussion
Here, we discuss the interesting properties we have observed with RLHF (Section 5.1). We then discuss the
limitations of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc (Section 5.2). Lastly, we present our strategy for responsibly releasing these
models (Section 5.3).
5.1 Learnings and Observations
Our tuning process revealed several interesting results, such as L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc ‚Äôs abilities to temporally
organize its knowledge, or to call APIs for external tools.
SFT (Mix)
SFT (Annotation)
RLHF (V1)
0.0 0.2 0.4 0.6 0.8 1.0
Reward Model ScoreRLHF (V2)
Figure 20: Distribution shift for progressive versions of L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , from SFT models towards RLHF.
Beyond Human Supervision. At the outset of the project, many among us expressed a preference for

</code></pre>
</div>
</div>
<p>Great! We got the most similar chunks according to the high-recall but low-precision bi-encoder.</p>
<p>Now, let‚Äôs re-rank by using a higher-accuracy cross-encoder model. We‚Äôll use the <a href="https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2">cross-encoder/ms-marco-MiniLM-L-6-v2</a> model. This model was trained with the MS MARCO Passage Retrieval dataset, a large dataset with real search questions and their relevant text passages. That makes the model quite suitable for making predictions using questions and passages.</p>
<p>We‚Äôll use the same question and the top 10 chunks we got from the bi-encoder. Let‚Äôs see the results! Recall that cross-encoders expect pairs, so we‚Äôll create pairs of the question and each chunk.</p>
<div id="639d583c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sentence_transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span>  CrossEncoder</span>
<span id="cb18-2">cross_encoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CrossEncoder(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cross-encoder/ms-marco-MiniLM-L-6-v2'</span>)</span>
<span id="cb18-3"></span>
<span id="cb18-4">cross_inp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [[query, chunks[hit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'corpus_id'</span>]]] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> hit <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> hits]</span>
<span id="cb18-5">cross_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_encoder.predict(cross_inp)</span>
<span id="cb18-6">cross_scores</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([ 1.2227577 ,  5.048051  ,  1.2897239 ,  2.205767  ,  4.4136825 ,
        1.2272772 ,  2.5638275 ,  0.81847703,  2.35553   ,  5.590804  ,
        1.3877895 ,  2.9497519 ,  1.6762824 ,  0.7211323 ,  0.16303705,
        1.3640019 ,  2.3106787 ,  1.5849439 ,  2.9696884 , -1.1079378 ,
        0.7681126 ,  1.5945492 ,  2.2869687 ,  3.5448399 ,  2.056368  ],
      dtype=float32)</code></pre>
</div>
</div>
<p>Let‚Äôs add a new value with the <code>cross-score</code> and sort by it!</p>
<div id="0224052b" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> idx <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(cross_scores)):</span>
<span id="cb20-2">    hits[idx][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cross-score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_scores[idx]</span>
<span id="cb20-3">hits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sorted</span>(hits, key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: x[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cross-score'</span>], reverse<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb20-4">msmarco_l6_corpus_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [hit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'corpus_id'</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> hit <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> hits] <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># save for later</span></span>
<span id="cb20-5"></span>
<span id="cb20-6">hits</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>[{'corpus_id': 20638, 'score': 0.4884325861930847, 'cross-score': 5.590804},
 {'corpus_id': 17387, 'score': 0.5659530162811279, 'cross-score': 5.048051},
 {'corpus_id': 5628, 'score': 0.5296251773834229, 'cross-score': 4.4136825},
 {'corpus_id': 14815, 'score': 0.45775431394577026, 'cross-score': 3.5448399},
 {'corpus_id': 14749, 'score': 0.46809980273246765, 'cross-score': 2.9696884},
 {'corpus_id': 9755, 'score': 0.48562008142471313, 'cross-score': 2.9497519},
 {'corpus_id': 9761, 'score': 0.49943411350250244, 'cross-score': 2.5638275},
 {'corpus_id': 9763, 'score': 0.49280521273612976, 'cross-score': 2.35553},
 {'corpus_id': 20632, 'score': 0.4695567488670349, 'cross-score': 2.3106787},
 {'corpus_id': 14751, 'score': 0.4585301876068115, 'cross-score': 2.2869687},
 {'corpus_id': 14725, 'score': 0.5585878491401672, 'cross-score': 2.205767},
 {'corpus_id': 35250, 'score': 0.4569615125656128, 'cross-score': 2.056368},
 {'corpus_id': 14806, 'score': 0.4792214035987854, 'cross-score': 1.6762824},
 {'corpus_id': 14821, 'score': 0.4637290835380554, 'cross-score': 1.5945492},
 {'corpus_id': 14750, 'score': 0.46810320019721985, 'cross-score': 1.5849439},
 {'corpus_id': 20653, 'score': 0.4873950183391571, 'cross-score': 1.3877895},
 {'corpus_id': 20711, 'score': 0.4703512489795685, 'cross-score': 1.3640019},
 {'corpus_id': 39564, 'score': 0.5590510368347168, 'cross-score': 1.2897239},
 {'corpus_id': 14802, 'score': 0.5075011253356934, 'cross-score': 1.2272772},
 {'corpus_id': 14679, 'score': 0.6097552180290222, 'cross-score': 1.2227577},
 {'corpus_id': 14716, 'score': 0.4931946098804474, 'cross-score': 0.81847703},
 {'corpus_id': 14671, 'score': 0.46657535433769226, 'cross-score': 0.7681126},
 {'corpus_id': 14805, 'score': 0.475425660610199, 'cross-score': 0.7211323},
 {'corpus_id': 20652, 'score': 0.4740477204322815, 'cross-score': 0.16303705},
 {'corpus_id': 35209, 'score': 0.46695172786712646, 'cross-score': -1.1079378}]</code></pre>
</div>
</div>
<p>As you can see above, the cross-encoder does not agree as much with the bi-encoder. Surprisingly, some of the top cross-encoder results (14815 and 14749) have the lowest bi-encoder scores. This makes sense - bi-encoders compare the similitude of the question and the documents in the embedding space, while cross-encoders consider the relationship between the question and the document.</p>
<div id="548cd2de" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, hit <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(hits[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]):</span>
<span id="cb22-2">    sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dataset[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>][hit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"corpus_id"</span>]]</span>
<span id="cb22-3">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Top </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> passage with score </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>hit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cross-score'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> from </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sample[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'source'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">:"</span>)</span>
<span id="cb22-4">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sample[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"chunk"</span>])</span>
<span id="cb22-5">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Top 1 passage with score 0.9668010473251343 from http://arxiv.org/pdf/2204.05862:
Stackoverflow Good Answer vs. Bad Answer Loss Difference
Python FT
Python FT + RLHF(b)Difference in mean log-prob between good and bad
answers to Stack OverÔ¨Çow questions.
Figure 37 Analysis of RLHF on language modeling for good and bad Stack OverÔ¨Çow answers, over many
model sizes, ranging from 13M to 52B parameters. Compared to the baseline model (a pre-trained LM
Ô¨Ånetuned on Python code), the RLHF model is more capable of distinguishing quality (right) , but is worse
at language modeling (left) .
the RLHF models obtain worse loss. This is most likely due to optimizing a different objective rather than
pure language modeling.
B.8 Further Analysis of RLHF on Code-Model Snapshots
As discussed in Section 5.3, RLHF improves performance of base code models on code evals. In this appendix, we compare that with simply prompting the base code model with a sample of prompts designed to
elicit helpfulness, harmlessness, and honesty, which we refer to as ‚ÄòHHH‚Äô prompts. In particular, they contain
a couple of coding examples. Below is a description of what this prompt looks like:
Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful,


Top 2 passage with score 0.9574587345123291 from http://arxiv.org/pdf/2302.07459:
We examine the inÔ¨Çuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an
increasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of
these models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,
previous work shows that the amount of RLHF training can signiÔ¨Åcantly change metrics on a wide range of
personality, political preference, and harm evaluations for a given model size [41]. As a result, it is important
to control for the amount of RLHF training in the analysis of our experiments.
3.2 Experiments
3.2.1 Overview
We test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping
and discrimination. Stereotyping involves the use of generalizations about groups in ways that are often
harmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ
[40] (¬ß3.2.2) and Windogender [49] (¬ß3.2.3). For discrimination, we focus on whether models make disparate
decisions about individuals based on protected characteristics that should have no relevance to the outcome.5
To measure discrimination, we construct a new benchmark to test for the impact of race in a law school course


Top 3 passage with score 0.9408788084983826 from http://arxiv.org/pdf/2302.07842:
preferences and values which are diÔ¨Écult to capture by hard- coded reward functions.
RLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,
ranking two model generations for the same prompt. This data is then collected to learn a reward model
that predicts a scalar reward given any generated text. The r eward captures human preferences when
judging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient
algorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM
pre-trained via self-supervised learning. However, for mo re complex tasks, the model‚Äôs generations may not
be good enough. In such cases, RLHF is typically applied afte r an initial supervised Ô¨Åne-tuning phase using
a small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;
Ouyang et al. ,2022;Stiennon et al. ,2020).
A successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.
(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing

</code></pre>
</div>
</div>
<p>Nice! The results seem relevant to the query. What can we do to improve the results?</p>
<p>Here we used <a href="https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2">cross-encoder/ms-marco-MiniLM-L-6-v2</a>, which is‚Ä¶well..it‚Äôs three years old and it‚Äôs tiny! It <a href="https://www.sbert.net/docs/pretrained-models/ce-msmarco.html">was</a> one of the best re-ranking models some years ago.</p>
<p>To pick a model, I suggest going to the <a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB leaderboard</a>, clicking reranking, and selecting a good model that meets your requirements. The average column is a good proxy for general quality, but you might be particularly interested in a dataset (e.g., MSMarco in the retrieval tab).</p>
<p>Note that some older models, such as MiniLM, are not there. Additionally, not all of these models are cross-encoders, so it‚Äôs always important to experiment if adding the second-stage, slower re-ranker is worth it. Here are some that are interesting:</p>
<ol type="1">
<li><a href="https://huggingface.co/intfloat/e5-mistral-7b-instruct">E5 Mistral 7B Instruct</a> (Dec 2023): This is a decoder-based embedder (not an encoder-based one as we learned before!). This means the model is massive for most applications (it has 7B params, which is two orders of magnitude higher than MiniLM!). This one is interesting because of the new trend of using decoder models rather than encoders, which could enable working with longer contexts. <a href="https://huggingface.co/papers/2401.00368">Here</a> is the paper.</li>
<li><a href="https://huggingface.co/BAAI/bge-reranker-base">BAAI Reranker</a> (Sep 2023): A high-quality re-ranking model with a decent size (278M parameters). Let‚Äôs get the results with this and compare!</li>
</ol>
<div id="3cff5f0e" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Same code as before, just different model</span></span>
<span id="cb24-2">cross_encoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> CrossEncoder(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'BAAI/bge-reranker-base'</span>)</span>
<span id="cb24-3"></span>
<span id="cb24-4">cross_inp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [[query, chunks[hit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'corpus_id'</span>]]] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> hit <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> hits]</span>
<span id="cb24-5">cross_scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_encoder.predict(cross_inp)</span>
<span id="cb24-6"></span>
<span id="cb24-7"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> idx <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(cross_scores)):</span>
<span id="cb24-8">    hits[idx][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cross-score'</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cross_scores[idx]</span>
<span id="cb24-9"></span>
<span id="cb24-10">hits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sorted</span>(hits, key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> x: x[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cross-score'</span>], reverse<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb24-11">bge_corpus_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [hit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'corpus_id'</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> hit <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> hits]</span>
<span id="cb24-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, hit <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(hits[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]):</span>
<span id="cb24-13">    sample <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dataset[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>][hit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"corpus_id"</span>]]</span>
<span id="cb24-14">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Top </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> passage with score </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>hit[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cross-score'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> from </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>sample[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'source'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">:"</span>)</span>
<span id="cb24-15">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(sample[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"chunk"</span>])</span>
<span id="cb24-16">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Top 1 passage with score 0.9668010473251343 from http://arxiv.org/pdf/2204.05862:
Stackoverflow Good Answer vs. Bad Answer Loss Difference
Python FT
Python FT + RLHF(b)Difference in mean log-prob between good and bad
answers to Stack OverÔ¨Çow questions.
Figure 37 Analysis of RLHF on language modeling for good and bad Stack OverÔ¨Çow answers, over many
model sizes, ranging from 13M to 52B parameters. Compared to the baseline model (a pre-trained LM
Ô¨Ånetuned on Python code), the RLHF model is more capable of distinguishing quality (right) , but is worse
at language modeling (left) .
the RLHF models obtain worse loss. This is most likely due to optimizing a different objective rather than
pure language modeling.
B.8 Further Analysis of RLHF on Code-Model Snapshots
As discussed in Section 5.3, RLHF improves performance of base code models on code evals. In this appendix, we compare that with simply prompting the base code model with a sample of prompts designed to
elicit helpfulness, harmlessness, and honesty, which we refer to as ‚ÄòHHH‚Äô prompts. In particular, they contain
a couple of coding examples. Below is a description of what this prompt looks like:
Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful,


Top 2 passage with score 0.9574587345123291 from http://arxiv.org/pdf/2302.07459:
We examine the inÔ¨Çuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an
increasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of
these models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,
previous work shows that the amount of RLHF training can signiÔ¨Åcantly change metrics on a wide range of
personality, political preference, and harm evaluations for a given model size [41]. As a result, it is important
to control for the amount of RLHF training in the analysis of our experiments.
3.2 Experiments
3.2.1 Overview
We test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping
and discrimination. Stereotyping involves the use of generalizations about groups in ways that are often
harmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ
[40] (¬ß3.2.2) and Windogender [49] (¬ß3.2.3). For discrimination, we focus on whether models make disparate
decisions about individuals based on protected characteristics that should have no relevance to the outcome.5
To measure discrimination, we construct a new benchmark to test for the impact of race in a law school course


Top 3 passage with score 0.9408788084983826 from http://arxiv.org/pdf/2302.07842:
preferences and values which are diÔ¨Écult to capture by hard- coded reward functions.
RLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,
ranking two model generations for the same prompt. This data is then collected to learn a reward model
that predicts a scalar reward given any generated text. The r eward captures human preferences when
judging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient
algorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM
pre-trained via self-supervised learning. However, for mo re complex tasks, the model‚Äôs generations may not
be good enough. In such cases, RLHF is typically applied afte r an initial supervised Ô¨Åne-tuning phase using
a small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;
Ouyang et al. ,2022;Stiennon et al. ,2020).
A successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.
(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing

</code></pre>
</div>
</div>
<p>Let‚Äôs compare the ranking of the three models:</p>
<div id="387a2379" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25</span>):</span>
<span id="cb26-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Top </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> passage. Bi-encoder </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>retrieval_corpus_ids[i]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, Cross-encoder (MS Marco) </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>msmarco_l6_corpus_ids[i]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, BGE </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>bge_corpus_ids[i]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Top 1 passage. Bi-encoder 14679, Cross-encoder (MS Marco) 20638, BGE 14815
Top 2 passage. Bi-encoder 17387, Cross-encoder (MS Marco) 17387, BGE 20638
Top 3 passage. Bi-encoder 39564, Cross-encoder (MS Marco) 5628, BGE 17387
Top 4 passage. Bi-encoder 14725, Cross-encoder (MS Marco) 14815, BGE 14679
Top 5 passage. Bi-encoder 5628, Cross-encoder (MS Marco) 14749, BGE 9761
Top 6 passage. Bi-encoder 14802, Cross-encoder (MS Marco) 9755, BGE 39564
Top 7 passage. Bi-encoder 9761, Cross-encoder (MS Marco) 9761, BGE 20632
Top 8 passage. Bi-encoder 14716, Cross-encoder (MS Marco) 9763, BGE 14725
Top 9 passage. Bi-encoder 9763, Cross-encoder (MS Marco) 20632, BGE 9763
Top 10 passage. Bi-encoder 20638, Cross-encoder (MS Marco) 14751, BGE 14750
Top 11 passage. Bi-encoder 20653, Cross-encoder (MS Marco) 14725, BGE 14805
Top 12 passage. Bi-encoder 9755, Cross-encoder (MS Marco) 35250, BGE 9755
Top 13 passage. Bi-encoder 14806, Cross-encoder (MS Marco) 14806, BGE 14821
Top 14 passage. Bi-encoder 14805, Cross-encoder (MS Marco) 14821, BGE 14802
Top 15 passage. Bi-encoder 20652, Cross-encoder (MS Marco) 14750, BGE 14749
Top 16 passage. Bi-encoder 20711, Cross-encoder (MS Marco) 20653, BGE 5628
Top 17 passage. Bi-encoder 20632, Cross-encoder (MS Marco) 20711, BGE 14751
Top 18 passage. Bi-encoder 14750, Cross-encoder (MS Marco) 39564, BGE 14716
Top 19 passage. Bi-encoder 14749, Cross-encoder (MS Marco) 14802, BGE 14806
Top 20 passage. Bi-encoder 35209, Cross-encoder (MS Marco) 14679, BGE 20711
Top 21 passage. Bi-encoder 14671, Cross-encoder (MS Marco) 14716, BGE 20652
Top 22 passage. Bi-encoder 14821, Cross-encoder (MS Marco) 14671, BGE 14671
Top 23 passage. Bi-encoder 14751, Cross-encoder (MS Marco) 14805, BGE 20653
Top 24 passage. Bi-encoder 14815, Cross-encoder (MS Marco) 20652, BGE 35209
Top 25 passage. Bi-encoder 35250, Cross-encoder (MS Marco) 35209, BGE 35250</code></pre>
</div>
</div>
<p>Interesting, we get very different results! Let‚Äôs briefly look into some of them.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>I suggest doing something like <code>dataset["train"][20638]["chunk"]</code> to print a particular result. Here is a quick summary of the results.</p>
</div>
</div>
<p>The bi-encoder is good at getting some results related to RLHF, but it‚Äôs struggling to get good, precise passages responding to what RLHF is. I looked at the top 5 results for each model. From looking at the passages, 17387 and 20638 are the only passages that really answer the question. Although the three models agree that 17387 is highly relevant, it‚Äôs interesting that the bi-encoder ranks 20638 lowly, while the two cross-encoders rank it highly. You can find them here.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 58%">
<col style="width: 19%">
<col style="width: 8%">
<col style="width: 5%">
</colgroup>
<thead>
<tr class="header">
<th>Corpus ID</th>
<th>Relevant text or summary</th>
<th>Bi-encoder pos (from top 10)</th>
<th>MSMarco pos</th>
<th>BGE pos</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>14679</td>
<td>Discusses implications and applications of RLHF but no definition.</td>
<td>1</td>
<td>20</td>
<td>4</td>
</tr>
<tr class="even">
<td>17387</td>
<td>Describes the process of RLHF in detail and applications</td>
<td>2</td>
<td>2</td>
<td>3</td>
</tr>
<tr class="odd">
<td>39564</td>
<td>This chunk is messy and is more of a discussion section intro than an answer</td>
<td>3</td>
<td>18</td>
<td>6</td>
</tr>
<tr class="even">
<td>14725</td>
<td>Characteristics about RLHF but no definition of what it is</td>
<td>4</td>
<td>11</td>
<td>8</td>
</tr>
<tr class="odd">
<td>20638</td>
<td>‚Äúincreasingly popular technique for reducing harmful behaviors in large language models‚Äù</td>
<td>10</td>
<td>1</td>
<td>2</td>
</tr>
<tr class="even">
<td>5628</td>
<td>Discusses the reward modeling (a component) but does not define RLHF</td>
<td>5</td>
<td>3</td>
<td>16</td>
</tr>
<tr class="odd">
<td>14815</td>
<td>Discusses RLHF but does not define it</td>
<td>24</td>
<td>4</td>
<td>1</td>
</tr>
<tr class="even">
<td>14749</td>
<td>Discusses impact of RLHF but it has no definition</td>
<td>19</td>
<td>5</td>
<td>15</td>
</tr>
<tr class="odd">
<td>9761</td>
<td>Discusses the reward modeling (a component) but does not define RLHF</td>
<td>7</td>
<td>7</td>
<td>5</td>
</tr>
</tbody>
</table>
<p>Reranking is a frequent feature in libraries; <code>llamaindex</code> allows you to use a <code>VectorIndexRetriever</code> to retrieve and a <code>LLMRerank</code> to rerank (see <a href="https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/LLMReranker-Lyft-10k.html">tutorial</a>), Cohere offers a <a href="https://txt.cohere.com/rerank/">Rerank Endpoint</a> and <a href="https://qdrant.tech/articles/hybrid-search/">qdrant</a> supports similar functionality. However, as you saw above, it‚Äôs relatively simple to implement yourself. If you have a high-quality bi-encoder model, you can use it to rerank and benefit from its speed.</p>
<div class="callout callout-style-default callout-note callout-titled" title="LLMs as rerankers">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>LLMs as rerankers
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some people use a generative LLM as a reranker. For example, <a href="https://cookbook.openai.com/examples/search_reranking_with_cross-encoders">OpenAI‚Äôs Coobook</a> has an example in which they use GPT-3 as a reranker by building a prompt asking the model to determine if a document is relevant for the document. Although this shows the impressive capabilities of an LLM, it‚Äôs usually not the best option for the task, as it will likely have worse quality, be more expensive, and be slower than a cross-encoder.</p>
<p>Experiment and see what works best for your data. Using LLMs as rerankers can sometimes be helpful if your documents have very long contexts (for which bert-based models struggle).</p>
</div>
</div>
</section>
<section id="aside-specter2" class="level2">
<h2 class="anchored" data-anchor-id="aside-specter2">Aside: SPECTER2</h2>
<p>If you‚Äôre particularly excited about embeddings for scientific tasks, I suggest looking at <a href="https://huggingface.co/allenai/specter2_base">SPECTER2</a> from AllenAI, a family of models that generate embeddings for scientific papers. These models can be used to do things such as predicting links, looking for nearest papers, find candidate papers for a given query, classify papers using the embeddings as features, and more!</p>
<p>The base model was trained on <a href="https://huggingface.co/datasets/allenai/scirepeval">scirepeval</a>, a dataset of millions of triples of scientific paper citations. After being trained, the authors fine-tuned the model using <a href="https://github.com/adapter-hub/adapters">adapters</a>, a library for parameter-efficient fine-tuning (don‚Äôt worry if you don‚Äôt know what this is). The authors attached a small neural network, called an adapter, to the base model. This adapter is trained to perform a specific task, but training for a specific task requires much fewer data than training the whole model. Because of these differences, one needs to use <code>transformers</code> and <code>adapters</code> to run inference, e.g.&nbsp;by doing something like</p>
<pre><code>model = AutoAdapterModel.from_pretrained('allenai/specter2_base')
model.load_adapter("allenai/specter2", source="hf", load_as="proximity", set_active=True)</code></pre>
<p>I recommend reading the model card to learn more about the model and its usage. You can also read the <a href="https://www.semanticscholar.org/paper/SPECTER%3A-Document-level-Representation-Learning-Cohan-Feldman/a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031">paper</a> for more details.</p>
</section>
<section id="aside-augmented-sbert" class="level2">
<h2 class="anchored" data-anchor-id="aside-augmented-sbert">Aside: Augmented SBERT</h2>
<p><a href="https://arxiv.org/abs/2010.08240">Augmented SBERT</a> is a technique for collecting data to improve bi-encoders. Pre-training and fine-tuning bi-encoders require lots of data, so the authors suggested using cross-encoders to label a large set of input pairs and add that to the training data. For example, if you have very little labeled data, you can train a cross-encoder and then label unlabeled pairs, which can be used to train a bi-encoder.</p>
<p>How do you generate the pairs? We can use random combinations of sentences and then label them using the cross-encoder. This would lead to mostly negative pairs and skew the label distribution. To avoid this, the authors explored different techniques:</p>
<ul>
<li>With <strong>Kernel Density Estimation (KDE)</strong>, the goal is to have similar label distributions between a small, golden dataset and the augmentation dataset. This is achieved by dropping some negative pairs. Of course, this will be inefficient as you‚Äôll need to generate many pairs to get a few positive ones.</li>
<li><strong>BM25</strong> is an algorithm used in search engines based on overlap (e.g., word frequency, length of document, etc.). Based on this, the authors get the top-k similar sentences to retrieve the k most similar sentences, and then, a cross-encoder is used to label them. This is efficient but will only be able to capture semantic similarity if there is little overlap between the sentences.</li>
<li><strong>Semantic Search Sampling</strong> trains a bi-encoder on the golden data and then used to sample other similar pairs.</li>
<li><strong>BM25 + Semantic Search Sampling</strong> combines the two previous methods. This helps find lexical and semantically similar sentences.</li>
</ul>
<p>There are nice figures and example scripts to do this in the <a href="https://www.sbert.net/examples/training/data_augmentation/README.html">Sentence Transformers docs</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings2/augmented.png" class="img-fluid figure-img"></p>
<figcaption>Augmented SBERT - the image is from the original paper</figcaption>
</figure>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>That was fun! We just learned to do one of the most common sentence embedding tasks: retrieve and rerank! We learned about the differences between bi-encoders and cross-encoders and when to use one versus the other. We also learned about some techniques to improve bi-encoders, such as augmented SBERT.</p>
<p>Don‚Äôt hesitate to change the code and play with it! If you like this blog post, don‚Äôt hesitate to <a href="https://github.com/osanseviero/hackerllama">leave a GitHub Star</a> or share it, that‚Äôs always appreciated and motivating!</p>
</section>
<section id="knowledge-check" class="level2">
<h2 class="anchored" data-anchor-id="knowledge-check">Knowledge Check</h2>
<ol type="1">
<li>What is the difference between bi-encoders and cross-encoders?</li>
<li>Explain the different steps of reranking.</li>
<li>How many embeddings would we need to generate to compare 30,000 sentences using a bi-encoder? How many times would we run inference with a cross-encoder?</li>
<li>What are some techniques to improve bi-encoders?</li>
</ol>
<p>Now, you have solid foundations to implement your search system. As a follow-up, I suggest implementing a similar retrieve and rerank system with a different dataset. Explore how changing both retrieval and reranking models impact your results.</p>


</section>

 ]]></description>
  <guid>https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings2/</guid>
  <pubDate>Sat, 20 Jan 2024 00:00:00 GMT</pubDate>
  <media:content url="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings2/cross_encoder.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>The Llama Hitchiking Guide to Local LLMs</title>
  <link>https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/</link>
  <description><![CDATA[ 





<p>Here are some terms that are useful to know when joining the Local LLM community.</p>
<ol type="1">
<li><p><strong>LocalLlama:</strong> A <a href="https://www.reddit.com/r/LocalLLaMA/">Reddit community</a> of practitioners, researchers, and hackers doing all kinds of crazy things with ML models.</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/localllama.jpeg" class="img-fluid"></p></li>
<li><p><strong>LLM:</strong> A Large Language Model. Usually a transformer-based model with a lot of parameters‚Ä¶billions or even trillions.</p></li>
<li><p><strong>Transformer:</strong> A type of neural network architecture that is very good at language tasks. It is the basis for most LLMs.</p></li>
<li><p><strong>GPT:</strong> A type of transformer that is trained to predict the next token in a sentence. GPT-3 is an example of a GPT model‚Ä¶who could tell??</p>
<p>4.1 <strong>Auto-regressive:</strong> A type of model that generates text one token at a time. It is auto-regressive because it uses its own predictions to generate the next token. For example, the model might receive as input ‚ÄúToday‚Äôs weather‚Äù and generate the next token, ‚Äúis‚Äù. It will then use ‚ÄúToday‚Äôs weather is‚Äù as input and generate the next token, ‚Äúsunny‚Äù. It will then use ‚ÄúToday‚Äôs weather is sunny‚Äù as input and generate the next token, ‚Äúand‚Äù. And so on.</p></li>
<li><p><strong>Token:</strong> Models don‚Äôt understand words. They understand numbers. When we receive a sequence of words, we convert them to numbers. Sometimes we split words into pieces, such as ‚Äútokenization‚Äù into ‚Äútoken‚Äù and ‚Äúization‚Äù. This is needed because the model has a limited vocabulary. A token is the smallest unit of language that a model can understand.</p></li>
<li><p><strong>Context length:</strong> The number of tokens that the model can use at a time. The higher the context length, the more memory the model needs to train and the slower it is to run. E.g. Llama 2 can manage up to 4096 tokens.</p>
<p>6.1 <strong>LLaMA:</strong> A pre-trained model trained by Meta, shared with some groups in a private access, and then leaked. It led to an explosion of cool projects. ü¶ô</p>
<p>6.2 <strong>Llama 2:</strong> An open-access pre-trained model released by Meta. It led to another explosion of very cool projects, and this one was not leaked! The license is not technically open-source but it‚Äôs still quite open and permissive, even for commercial use cases. ü¶ôü¶ô</p>
<p>6.3 <strong>RoPE:</strong> A technique that allows you to significantly expand the context lengths of a model.</p>
<p>6.4 <strong>SuperHot:</strong> A technique that allows expanding the context length of RoPE-based models even more by doing some minimal additional training.</p></li>
<li><p><strong>Pre-training:</strong> Training a model on a very large dataset (trillion of tokens) to learn the structure of language. Imagine you have millions of dollars, as a good GPU-Rich. You usually scrape big datasets from the internet and train your model on them. This is called pre-training. The idea is to end with a model that has a strong understanding of language. This does not require labeled data! This is done before fine-tuning. Examples of pre-trained models are GPT-3, Llama 2, and Mistral.</p>
<p>7.1 <strong>Mistral 7B:</strong> A pre-trained model trained by Mistral. Released via torrent.</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/mistral.png" class="img-fluid"></p>
<p>7.2 <strong>Phi 2:</strong> A pre-trained model by Microsoft. It only has 2.7B parametrs but it‚Äôs quite good for its size! It was trained with very little data (textbooks) which shows the power of high-quality data.</p>
<p>7.3 <strong>transformers:</strong> a Python library to access models shared by the community. It allows you to download pre-trained models and fine-tune them for your own needs</p>
<p>7.4 <strong>Base vs conversational:</strong> a pre-trained model is not specifically trained to ‚Äúbehave‚Äù in a conversational manner. If you try to use a base model (e.g.&nbsp;GPT-3, Mistral, Llama) directly to do conversations, it won‚Äôt work as well as the fine-tuned conversational variant (ChatGPT, Mistral Instruct, Llama Chat). When looking at benchmarks, you want to compare base models with base models and conversational models with conversational models.</p></li>
<li><p><strong>Fine-tuning:</strong> Training a model on a small (labeled) dataset to learn a specific task. This is done after pre-training. Imagine you have a few dollars, as a good fellow GPU-Poor. Rather than training a model from scratch, you pick a pre-trained (base) model and fine-tune it. You usually pick a small dataset of few hundreds-thousands of samples. You then pass it to the model and train it on it. This is called fine-tuning. The idea is to end with a model that has a strong understanding of a specific task. For example, you can fine-tune a model with your tweets to make it generate tweets like you! (but please don‚Äôt). You can fine-tune many models in your gaming laptop! Examples of fine-tuned models are ChatGPT, Vicuna, and Mistral Instruct.</p>
<p>8.1 <strong>Mistral 7B Instruct:</strong> A fine-tuned version of Mistral 7B.</p>
<p>8.2 <strong>Vicuna:</strong> A cute animal that is also a fine-tuned model. It begins from LLaMA-13B and is fine-tuned on user conversations with ChatGPT.</p>
<p>8.3 <strong>Number of parameters:</strong> Notice the <code>-13B</code> in point 8.2. That‚Äôs the number of parameters in a model. Each parameter is a number (with certain precision), and is part of the model. The parameters are learned during pre-training and fine-tuning to minimize the error.</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/gpu_poor.png" class="img-fluid"></p></li>
<li><p><strong>Prompt:</strong> A few words that you give to the model to start generating text. For example, if you want to generate a poem, you can give the model the first line of the poem as a prompt. The model will then generate the rest of the poem!</p></li>
<li><p><strong>Zero-shot:</strong> A type of prompt that is used to generate text without fine-tuning. The model is not trained on any specific task. It is only trained on a large dataset of text. For example, you can give the model the first line of a poem and ask it to generate the rest of the poem. The model will do its best to generate a poem, even though it has never seen a poem before! When you use ChatGPT, you often do zero-shot generation!</p>
<pre><code>User: Write a poem about a llama
_______________
Model:
Graceful llama, in Andean air,
Elegant stride, woolly flair.
Mountains echo, mystic charm,
Llama's gaze, a tranquil balm.</code></pre></li>
<li><p><strong>Few-shot:</strong> A type of prompt that is used to generate text with fine-tuning. We provide a couple of examples to the model. This can improve the quality a lot!</p>
<pre><code>User
Input:

Text: "The cat sat on the mat."
Label: Sentence about an animal.

Text: "The sun is incredibly bright today."
Label: Sentence about weather.

Classification Task:
Classify the following text - "Rainy days make me want to stay in bed."

Output:
Label: Sentence about weather.

Text: "Rainy days make me want to stay in bed."
__________________
Model
Label: Sentence about weather.</code></pre></li>
<li><p><strong>Instruct-tuning:</strong> A type of fine-tuning that uses instructions to generate text ending in more controlled behavor in generating responses or performing tasks.</p>
<p>12.1 <strong>Alpaca:</strong> A dataset of 52,000 instructions generatd with OpenAI APIs. It kicked off a big wave of people using OpenAI to generate synthetic data for instruct-tuning. It costed about $500 to generate.</p>
<p>12.2 <strong>LIMA:</strong> A model that demonstrates strong performance with very few examples. It demonstrates that adding more data does not always correlate with better quality.</p></li>
<li><p><strong>RLHF (Reinforcement Learning with Human Feedback):</strong> A type of fine-tuning that uses reinforcement learning (RL) and human-generated feedback. Thanks to the introduction of human feedback, the end model ends up being very good for things such as conversations! It kicks off with a base model that generates bunch of conversations. Humans then rate the answers (preferences). The preferences are used to train a Reward Model that generates a score for a given text. Using Reinforcement Learning, the initial LM is trained to maximize the score generated by the Reward Model. Read more about it <a href="https://huggingface.co/blog/rlhf">here</a>.</p>
<p>13.1 <strong>RL:</strong> Reinforcement learning is a type of machine learning that uses rewards to train a model. For example, you can train a model to play a game by giving it a reward when it wins and a punishment when it loses. The model will learn to win the game!</p>
<p>13.2. <strong>Reward Model:</strong> A model that is used to generate rewards. For example, you can train a model to generate rewards for a game. The model will learn to generate rewards that are good for the game!</p>
<p>13.3 <strong>ChatGPT:</strong> RLHF-finetuned GPT-3 model that is very good at conversations.</p>
<p>13.4 <strong>AIF</strong>: An alternative to human feedback‚Ä¶AI Feedback!</p></li>
<li><p><strong>PPO:</strong> A type of reinforcement learning algorithm that is used to train a model. It is used in RLHF.</p></li>
<li><p><strong>DPO:</strong> A type of training which removes the need for a reward model. It simplifies significantly the RLHF-pipeline.</p>
<p>15.1 <strong>Zephyr:</strong> A 7B Mistral-based model trained with DPO. It has similar capabilities to the Llama 2 Chat model of 70B parameters. It came out with a nice <a href="https://github.com/huggingface/alignment-handbook/tree/main">handbook of recipes</a>.</p>
<p>15.2 <strong>Notus:</strong> A trained variation of Zephyr but with better filered and fixed data. It does better!</p>
<p>15.3 <strong>Overfitting:</strong> occurs in ML when a model learns the training data too well, capturing noise and specific patterns that do not generalize to new, unseen data, leading to poor performance on real-world tasks.</p>
<p>15.4 <strong>DPO Overfits</strong> Although DPO shows overfitting behaviors after one behavior, it does not harm downstream performance on chat evaluations. Did your ML teachers lie to us when they said overfitting was bad?</p>
<p>15.5 <strong>IPO:</strong> A change in the DPO objective which is simpler and less prone to overfitting.</p>
<p>15.6. <strong>KTO:</strong> While PPO, DPO, and IPO require pairs of accepted vs rejected generations, KTO just needs a binary label (accepted or rejected), hence allowing to scale to much more data.</p>
<p>15.7 <strong>trl:</strong> A library that allows to train models with DPO, IPO, KTO, and more!</p></li>
<li><p><strong>Open LLM Leaderboard:</strong> A <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">leaderboard</a> where you can find benchmark results for many open-access LLMs.</p>
<p>17.1 <strong>Benchmark:</strong> A benchmark is a test that you run to compare different models. For example, you can run a benchmark to compare the performance of different models on a specific task.</p>
<p>17.2 <strong>TruthfulQA:</strong> A not-great benchmark to measure a model‚Äôs ability to generate truthful answers.</p>
<p>17.3 <strong>Conversational models:</strong> The LLM Leaderboard should be mostly to compare base models, not as much for conversational models. It still provides some useful signal about the conversational models, but this should not be the final way to evaluate them.</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/benchmark.png" class="img-fluid"></p></li>
<li><p><strong>Chatbot Arena:</strong> A popopular <a href="https://lmsys.org/blog/2023-05-03-arena/">crowd-sourced open benchmark</a> of human preferences. <strong>It‚Äôs good to compare conversational models</strong></p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/chatbot_arena.png" class="img-fluid"></p></li>
<li><p><strong>MT-Bench:</strong> A multi-turn benchmark of 160 questions across eight domains. Each response is evaluated by GPT-4. (This presents limitations‚Ä¶what happens if the model is better than GPT-4?)</p></li>
<li><p><strong>Mixture-of-Experts (MoE):</strong> A model architecture in which some of the (dense) layers are replaced with a set of experts. Each expert is a small neural network. There is a small network, router, that decides which expert to use for each token (read more <a href="https://huggingface.co/blog/moe">here</a>). Clarifications:</p>
<ul>
<li>A MoE is not an ensemble.</li>
<li>If we say a MoE has 8 experts, it means each replaced dense layer is replaced with 8 experts. If there were 3 replaced layers, then there are 24 experts in total!</li>
<li>We can activate multiple experts at the same time. For a given sentence, ‚Äúhello world‚Äù, ‚Äúhello might be sent to experts 1 and 2 while‚Äùworld‚Äù to 2 and 4.</li>
<li>The experts in a MoE do not specialize in a task. They are all trained on the same task, they just get different tokens! Sometimes they do specialize in certain types of tokens, as shown in this table from the ST-MoE paper.</li>
</ul>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/moe.png" class="img-fluid"></p>
<p>19.1 <strong>GPT-4:</strong> A kinda good model, but we don‚Äôt know what it is. The rumors say it‚Äôs a MoE.</p>
<p>19.2 <strong>Mixtral:</strong> A MoE model released by Mistral. It has 47B parameters but only 12B parameters are used at a time, making it very efficient.</p></li>
<li><p><strong>Model Merging:</strong> A technique that allows us to combine multiple models of the same architecture into a single model. Read more <a href="https://huggingface.co/blog/mlabonne/merge-models">here</a>.</p>
<p>20.1 <strong>Mergekit:</strong> A cool open-source tool to quickly merge repos.</p>
<p>20.2 <strong>Averaging:</strong> The most basic merging technique. Pick two models, average their weights. Somehow it kinda works!</p>
<p>20.3 <strong>Frankenmerge:</strong> It allows to concatenate layers from different LLMs, allowing you to do crazy things.</p>
<p>20.4 <strong>Goliath-120B:</strong> A frankenmerge that combines two Llama 70B models to achieve a 120B model</p>
<p>20.5 <strong>MoE Merging:</strong> (Not 100% about this one) Experimental branch in <code>mergekit</code> that allows building a MoE-like model combining different models. You specify which models and which types of prompts you want each expert to handle, hence ending with expert task-specialization.</p>
<p>20.6 <strong>Phixtral:</strong> A MoE merge of Phi 2 DPO and Dolphin 2 Phi 2.</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/merge.jpeg" class="img-fluid"></p></li>
<li><p><strong>Local LLMs:</strong> If we have models small enough, we can run them in our computers or even our phones!</p>
<p>21.1 <strong>TinyLlama:</strong> A project to pre-train a 1.1B Llama model on 3 trillion tokens.</p>
<p>21.2 <strong>Cognitive Computations:</strong> A community (led by Eric Hartford) that is fine-tuning a bunch of models</p>
<p>21.3 <strong>Uncensored models:</strong> Many models have some strong alignment that prevent doing things such as asking Llama to kill a Linux process. Training uncensored models aims to remove specific biases engrained in the decision-making process of fine-tuning a model. Read more <a href="https://erichartford.com/uncensored-models">here</a>.</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/process.png" class="img-fluid"></p>
<p>21.4 <strong>llama.cpp:</strong> A tool to use Llama-like models in C++.</p>
<p>21.5 <strong>GGUF:</strong> A format introduced by llama.cpp to store models. It replaces the old file format, GGML.</p>
<p>21.6 <strong>ggml:</strong> Tensor library in ML, allowing projects such as llama.cpp and whisper.cpp (not the same as GGML, the file format).</p>
<p>21.7 <strong>Georgi Gerganov:</strong> The creator of llama.cpp and ggml!</p>
<p>21.8 <strong>Whisper:</strong> The state-of-the-art speech-to-text open source model.</p>
<p>21.9 <strong>OpenAI:</strong> A company that does closed source AI. (kidding, they open-sourced Whisper!)</p>
<p>21.10 <strong>MLX:</strong> A new framework for Apple devices that allows easy inference and fine-tuning of models.</p></li>
<li><ol type="A">
<li><strong>Local LLM tools:</strong> If you don‚Äôt know how to code, there are a couple of tools that can be useful</li>
</ol>
<p>22.1 <strong>Oobabooga:</strong> A simple web app that allows you to use models without coding. It‚Äôs very easy to use!</p>
<p>22.2 <strong>LM Studio:</strong> A nice advanced app that runs models on your laptop, entirely offline.</p>
<p>22.3 <strong>ollama:</strong> An open-source tool to run LLMs locally. There are multiple web/desktop apps and terminal integrations on top of it.</p>
<p>22.4 <strong>ChatUI:</strong> An open-source UI to use open-source models.</p></li>
<li><p><strong>Quantization:</strong> A technique that allows us to reduce the size of a model. It is done by reducing the precision of the model‚Äôs weights. For example, we can reduce the precision from 32 bits to 8 bits. This reduces the size of the model by 4 times! The model will (sometimes) be less accurate but it will be much smaller. This allows us to run the model on smaller devices such as phones.</p>
<p>23.1 <strong>TheBloke:</strong> A bloke that quantizes models. As soon as a model is out, he quantizes it! See their <a href="https://huggingface.co/TheBloke">HF Profile</a>.</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/thebloke.png" class="img-fluid"></p>
<p>23.2 <strong>Hugging Face:</strong> A platform to find and share open-acces models, datasets, and demos. It‚Äôs also a company that has built different OS libraries (and where I work!)</p>
<p>23.3. <strong>Facehugger:</strong> A monster from the Alien movie. It should also be an open source tool. It‚Äôs not yet.</p>
<p>23.4. <strong>GPTQ:</strong> A popular quantization technique.</p>
<p>23.5 <strong>AWQ:</strong> Another popular quantization technique.</p>
<p>23.6 <strong>EXL2:</strong> A different quantization format used by a library called exllamav2 (among many others)</p>
<p>23.7 <strong>LASER:</strong> A technique that reduces the size of the model and increases its performance by reducindg the rank of specific matrices. It requires no additional training.</p></li>
<li><p><strong>PEFT:</strong> Parameter-Efficient Fine-Tuning - It‚Äôs a family of methods that allow fine-tuning models without modifying all the parameters. Usually, you freeze the model, add a small set of parameters, and just modify it. It hence reduces the amount of compute required and you can achieve very good results!</p>
<p>24.1 <strong>peft:</strong> A popular OS library to do PEFT! It‚Äôs used in other projects such as <code>trl</code>.</p>
<p>24.2 <strong>adapters:</strong> Another popular library to do PEFT.</p>
<p>24.3.<strong>unsloth</strong>: A higher-level library to do PEFT (using QLoRA)</p>
<p>24.4. <strong>LoRA:</strong> One of the most popular PEFT techniques. It adds low-rank ‚Äúupdate matrices‚Äù. The base model is frozen and only the update matrices are trained. This can be used for image classification, teaching Stable Diffusion the concept of your pet, or LLM fine-tuning.</p></li>
<li><p><strong>QLoRA:</strong> A technique that combines LoRAs with quantization, hence we use 4-bit quantization and only update the LoRA parameters! This allows fine-tuning models with very GPU-poor GPUs.</p>
<p>25.1. <strong>Tim Dettmers:</strong> A researcher that has done a lot of work on PEFT and created QLoRA.</p>
<p>25.2. <strong>Guanaco (model):</strong> A LLaMA fine-tune using QLoRA tuning.</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/qlora.png" class="img-fluid"></p></li>
<li><p><strong>axolotl:</strong> A cute animal that is also a high-level tool to streamline fine-tuning, including support for things such as QLoRA.</p></li>
<li><p><strong>Nous Research</strong>: An open-source Discord community turned company that releases bunch of cool models.</p></li>
<li><p><strong>Multimodal:</strong> A single model that can handle multiple modalities. For example, a model that can generate text and images at the same time. Or a model that can generate text and audio at the same time. Or a model that can generate text, images, and audio at the same time. Or a model that can generate text, images, audio, video, smells, tastes, feelings, thoughts, dreams, memories, consciousness, souls, universes, gods, multiverses, and omniverses at the same time. (thanks ChatGPT for your hallucination)</p>
<p>28.1 <strong>Hallucination:</strong> When a model cangenerates responses that may be coherent but are not actually accurate, leading to the creation of misinformation or imaginary scenarios‚Ä¶such as the one above!</p>
<p>28.2 <strong>LlaVA:</strong> A multimodal model that can receive images and text as input and generate text respones.</p></li>
<li><p><strong>Bagel:</strong> A process which mixes a bunch of supervised fine-tuning and preference data. It uses different prompt formats, making the model more versatile to all kinds of prompts.</p></li>
<li><p><strong>Code Models:</strong> LLMs that are specifically pre-trained for code.</p>
<p>30.1. <strong>Big Code Models Leaderboard:</strong> A <a href="https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard">leaderboard</a> to compare code models in the HumanEval dataset.</p>
<p>30.2. <strong>HumanEval:</strong> A very small dataset of 164 Python programming problems. It is translated to 18 programming languages in MultiPL-E.</p>
<p>30.3 <strong>BigCode:</strong> An open scientific collaboration working in code-related models and datasets.</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/bigcode.jpeg" class="img-fluid"></p>
<p>30.4 <strong>The Stack:</strong> A dataset of 6.4TB of permissible-licensed code data covering 358 programming languages.</p>
<p>30.5 <strong>Code Llama:</strong> The best base code model. It‚Äôs based on Llama 2.</p>
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/codellama.jpeg" class="img-fluid"></p>
<p>30.6 <strong>WizardLM:</strong> A research team from Microsoft‚Ä¶but also a Discord community.</p>
<p>30.7 <strong>WizardCoder:</strong> A code model released by WizardLM. Its architecture is based on Llama</p></li>
<li><p><strong>Flash Attention:</strong> An approximate attention algorithm which provides a huge speedup.</p>
<p>31.1 <strong>Flash Attention 2:</strong> An upgrade to the flash attention algorithm that provides even more speedup.</p>
<p>31.2. <strong>Tri Dao:</strong> The author of both techniques and a legend in the ecosystem.</p></li>
</ol>
<p>I hope you enjoyed this read! Feel free to suggest new terms or corrections in the comments below. I‚Äôll keep updating this post as new terms come up.</p>



 ]]></description>
  <guid>https://osanseviero.github.io/hackerllama/blog/posts/hitchhiker_guide/</guid>
  <pubDate>Fri, 12 Jan 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Sentence Embeddings. Introduction to Sentence Embeddings</title>
  <link>https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/</link>
  <description><![CDATA[ 





<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p><a href="https://colab.research.google.com/github/osanseviero/hackerllama/blob/main/nbs/blog/posts/sentence_embeddings/index.ipynb" rel="nofollow" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></p>
<p>This series aims to demystify embeddings and show you how to use them in your projects. This first blog post will teach you how to use and scale up open-source embedding models. We‚Äôll look into the criteria for picking an existing model, current evaluation methods, and the state of the ecosystem. We‚Äôll look into three exciting applications:</p>
<ul>
<li>Finding the most similar Quora or StackOverflow questions</li>
<li>Given a huge dataset, find the most similar items</li>
<li>Running search embedding models directly in the users‚Äô browser (no server required)</li>
</ul>
<p>You can either read the content here or execute it in Google Colab by clicking the badge at the top of the page. Let‚Äôs dive into embeddings!</p>
<section id="the-tldr" class="level2">
<h2 class="anchored" data-anchor-id="the-tldr">The TL;DR</h2>
<p>You keep reading about ‚Äúembeddings this‚Äù and ‚Äúembeddings that‚Äù, but you might still not know exactly what they are. You are not alone! Even if you have a vague idea of what embeddings are, you might use them through a black-box API without really understanding what‚Äôs going on under the hood. This is a problem because the current state of open-source embedding models is very strong - they are pretty easy to deploy, small (and hence cheap to host), and outperform many closed-source models.</p>
<p>An embedding represents information as a vector of numbers (think of it as a list!). For example, we can obtain the embedding of a word, a sentence, a document, an image, an audio file, etc. Given the sentence ‚ÄúToday is a sunny day‚Äù, we can obtain its embedding, which would be a vector of a specific size, such as 384 numbers (such vector could look like [0.32, 0.42, 0.15, ‚Ä¶, 0.72]). What is interesting is that the <strong>embeddings capture the semantic meaning of the information</strong>. For example, embedding the sentence ‚ÄúToday is a sunny day‚Äù will be very similar to that of the sentence ‚ÄúThe weather is nice today‚Äù. Even if the words are different, the meaning is similar, and the embeddings will reflect that.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>If you‚Äôre not sure what words such as ‚Äúvector‚Äù, ‚Äúsemantic similarity‚Äù, the vector size, or ‚Äúpretrained‚Äù mean, don‚Äôt worry! We‚Äôll explain them in the following sections. Focus on the high-level understanding first.</p>
</div>
</div>
</div>
<p>So, this vector captures the semantic meaning of the information, making it easier to compare to each other. For example, we can use embeddings to find similar questions in Quora or StackOverflow, search code, find similar images, etc. Let‚Äôs look into some code!</p>
<p>We‚Äôll use Sentence Transformers, an open-source library that makes it easy to use pre-trained embedding models. In particular, ST allows us to turn sentences into embeddings quickly. Let‚Äôs run an example and then discuss how it works under the hood.</p>
<p>Let‚Äôs begin by installing the library:</p>
<div id="47bd2412" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>pip install sentence_transformers</span></code></pre></div></div>
</div>
<p>The second step is to load an existing model. We‚Äôll start using <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">all-MiniLM-L6-v2</a>. It‚Äôs not the best open-source embedding model, but it‚Äôs quite popular and very small (23 million parameters), which means we can get started with it very quickly.</p>
<div id="822aee06" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sentence_transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SentenceTransformer</span>
<span id="cb2-2"></span>
<span id="cb2-3">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SentenceTransformer(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sentence-transformers/all-MiniLM-L6-v2"</span>)</span></code></pre></div></div>
</div>
<p>Now that we loaded a model, let‚Äôs use it to encode some sentences. We can use the <code>encode</code> method to obtain the embeddings of a list of sentences. Let‚Äôs try it out!</p>
<div id="51e050ec" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sentence_transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> util</span>
<span id="cb3-2"></span>
<span id="cb3-3">sentences <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The weather today is beautiful"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"It's raining!"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Dogs are awesome"</span>]</span>
<span id="cb3-4">embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.encode(sentences)</span>
<span id="cb3-5">embeddings.shape</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>(3, 384)</code></pre>
</div>
</div>
<p>all-MiniLM-L6-v2 creates embeddings of 384 values. We obtain three embeddings, one for each sentence. Think of <code>embeddings</code> as a ‚Äúdatabase‚Äù of embeddings. Given a new sentence, how can we find the most similar sentence? We can use the <code>util.pytorch_cos_sim</code> method to compute the cosine similarity (we‚Äôll talk more about it soon) between the new sentence embedding and all the embeddings in the database. The cosine similarity is a number between 0 and 1 that indicates how similar two embeddings are. A value of 1 means that the embeddings are identical, while 0 means that the embeddings are entirely different. Let‚Äôs try it out!</p>
<div id="ee557a54" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">first_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.encode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Today is a sunny day"</span>)</span>
<span id="cb5-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> embedding, sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(embeddings, sentences):</span>
<span id="cb5-3">    similarity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> util.pytorch_cos_sim(first_embedding, embedding)</span>
<span id="cb5-4">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(similarity, sentence)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.7344]]) The weather today is beautiful
tensor([[0.4180]]) It's raining!
tensor([[0.1060]]) Dogs are awesome</code></pre>
</div>
</div>
<p>What can we interpret of this? Although ‚Äútoday is a sunny day‚Äù and ‚Äúthe weather today is beautiful‚Äù don‚Äôt have the same words, the embeddings can capture some semantic meaning, so the cosine similarity is relatively high. On the other hand, ‚ÄúDogs are awesome‚Äù, although true, has nothing to do with the weather or today; hence, the cosine similarity is very low.</p>
<p>To expand on this idea of similar embeddings, let‚Äôs look into how they could be used in a product. Imagine that U.S. Social Security would like to allow users to write Medicare-related questions in an input field. This topic is very sensitive, and we likely don‚Äôt want a model to hallucinate with something unrelated! Instead, we can leverage a database of questions (in this case, there‚Äôs an existing Medicare FAQ). The process is similar to the above‚Äù</p>
<ol type="1">
<li>We have a corpus (collection) of questions and answers.</li>
<li>We compute the embeddings of all the questions.</li>
<li>Given a new question, we compute its embedding.</li>
<li>We compute the cosine similarity between the new question embedding and all the embeddings in the database.</li>
<li>We return the most similar question (which is associated with the most similar embedding).</li>
</ol>
<p>Steps 1 and 2 can be done offline (that is, we compute the embeddings only once and store them). The rest of the steps can be done at search time (each time a user asks a question). Let‚Äôs see what this would look like in code.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://huggingface.co/spaces/sentence-transformers/embeddings-semantic-search"><img src="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/embedding.png" class="img-fluid figure-img"></a></p>
<figcaption>Representation of embeddings in two dimensions</figcaption>
</figure>
</div>
<p>Let‚Äôs first create our map of frequently asked questions.</p>
<div id="93ff43ec" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Data from https://faq.ssa.gov/en-US/topic/?id=CAT-01092</span></span>
<span id="cb7-2"></span>
<span id="cb7-3">faq <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb7-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"How do I get a replacement Medicare card?"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"If your Medicare card was lost, stolen, or destroyed, you can request a replacement online at Medicare.gov."</span>,</span>
<span id="cb7-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"How do I sign up for Medicare?"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"If you already get Social Security benefits, you do not need to sign up for Medicare. We will automatically enroll you in Original Medicare (Part A and Part B) when you become eligible. We will mail you the information a few months before you become eligible."</span>,</span>
<span id="cb7-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"What are Medicare late enrollment penalties?"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"In most cases, if you don‚Äôt sign up for Medicare when you‚Äôre first eligible, you may have to pay a higher monthly premium. Find more information at https://faq.ssa.gov/en-us/Topic/article/KA-02995"</span>,</span>
<span id="cb7-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Will my Medicare premiums be higher because of my higher income?"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Some people with higher income may pay a larger percentage of their monthly Medicare Part B and prescription drug costs based on their income. We call the additional amount the income-related monthly adjustment amount."</span>,</span>
<span id="cb7-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"What is Medicare and who can get it?"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Medicare is a health insurance program for people age 65 or older. Some younger people are eligible for Medicare including people with disabilities, permanent kidney failure and amyotrophic lateral sclerosis (Lou Gehrig‚Äôs disease or ALS). Medicare helps with the cost of health care, but it does not cover all medical expenses or the cost of most long-term care."</span>,</span>
<span id="cb7-9">}</span></code></pre></div></div>
</div>
<p>Once again, we use the <code>encode</code> method to obtain the embeddings of all the questions.</p>
<div id="21a6455c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">corpus_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.encode(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(faq.keys()))</span>
<span id="cb8-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(corpus_embeddings.shape)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>(5, 384)</code></pre>
</div>
</div>
<p>Once a user asks a question, we obtain its embedding. We usually refer to this embedding as the query embedding.</p>
<div id="aff8ba08" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">user_question <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Do I need to pay more after a raise?"</span></span>
<span id="cb10-2">query_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.encode(user_question)</span>
<span id="cb10-3">query_embedding.shape</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>(384,)</code></pre>
</div>
</div>
<p>We can now compute the similarity between the corpus embeddings and the query embedding. We could have a loop and use <code>util.pytorch.cos_sim</code> as we did before, but Sentence Transformers provides an even friendlier method called <code>semantic_search</code> that does all the work for us. It returns the top-k most similar embeddings and their similarity score. Let‚Äôs try it out!</p>
<div id="c099a300" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">similarities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> util.semantic_search(query_embedding, corpus_embeddings, top_k<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb12-2">similarities</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>[[{'corpus_id': 3, 'score': 0.35796287655830383},
  {'corpus_id': 2, 'score': 0.2787758708000183},
  {'corpus_id': 1, 'score': 0.15840476751327515}]]</code></pre>
</div>
</div>
<p>Let‚Äôs now look at which questions and answers this corresponds to:</p>
<div id="c107948c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, result <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(similarities[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]):</span>
<span id="cb14-2">    corpus_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> result[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"corpus_id"</span>]</span>
<span id="cb14-3">    score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> result[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"score"</span>]</span>
<span id="cb14-4">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Top </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> question (p=</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">): </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(faq.keys())[corpus_id]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb14-5">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Answer: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(faq.values())[corpus_id]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Top 1 question (p=0.35796287655830383): Will my Medicare premiums be higher because of my higher income?
Answer: Some people with higher income may pay a larger percentage of their monthly Medicare Part B and prescription drug costs based on their income. We call the additional amount the income-related monthly adjustment amount.
Top 2 question (p=0.2787758708000183): What are Medicare late enrollment penalties?
Answer: In most cases, if you don‚Äôt sign up for Medicare when you‚Äôre first eligible, you may have to pay a higher monthly premium. Find more information at https://faq.ssa.gov/en-us/Topic/article/KA-02995
Top 3 question (p=0.15840476751327515): How do I sign up for Medicare?
Answer: If you already get Social Security benefits, you do not need to sign up for Medicare. We will automatically enroll you in Original Medicare (Part A and Part B) when you become eligible. We will mail you the information a few months before you become eligible.</code></pre>
</div>
</div>
<p>Great, so given the question ‚ÄúDo I need to pay more after a raise?‚Äù, we know that the most similar question is ‚ÄúWill my Medicare premiums be higher because of my higher income?‚Äù and hence we can return the provided answer. In practice, you would likely have thousands to millions of embeddings, but this was a simple yet powerful example of how embeddings can be used to find similar questions.</p>
<p>Now that we better understand what embeddings are and how they can be used, let‚Äôs do a deeper dive into them!</p>
</section>
<section id="from-word-embeddings-to-sentence-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="from-word-embeddings-to-sentence-embeddings">From word embeddings to sentence embeddings</h2>
<section id="word2vec-and-glove" class="level3">
<h3 class="anchored" data-anchor-id="word2vec-and-glove">Word2Vec and GloVe</h3>
<p>It‚Äôs time to take a step back and learn more about embeddings and why they are needed. Neural networks, such as BERT, are not able to process words directly; they need numbers. And the way to provide words is to represent them as vectors, also called word embeddings.</p>
<p>In the traditional setup, you define a vocabulary (which words are allowed), and then each word in this vocabulary has an assigned embedding. Words not in the vocabulary are mapped to a special token, usually called <unk> (a standard placeholder for words not found during training). For example, let‚Äôs say we have a vocabulary of three words, and we assign each word a vector of size five. We could have the following embeddings:</unk></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Word</th>
<th>Embedding</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>king</td>
<td>[0.15, 0.2, 0.2, 0.3, 0.5]</td>
</tr>
<tr class="even">
<td>queen</td>
<td>[0.12, 0.1, 0.19, 0.3, 0.47]</td>
</tr>
<tr class="odd">
<td>potato</td>
<td>[0.13, 0.4, 0.1, 0.15, 0.01]</td>
</tr>
<tr class="even">
<td><code>&lt;UNK&gt;</code></td>
<td>[0.01, 0.02, 0.01, 0.4, 0.11]</td>
</tr>
</tbody>
</table>
<p>The embedding I wrote above are numbers that I wrote somewhat randomly. In practice, <strong>the embeddings are learned</strong>. This is the main idea of methods such as <a href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec</a> and <a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe</a>. They learn the embeddings of the words in a corpus in such a way that words that appear in similar contexts have similar embeddings. For example, the embeddings of ‚Äúking‚Äù and ‚Äúqueen‚Äù are similar because they appear in similar contexts.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://huggingface.co/spaces/sentence-transformers/embeddings-semantic-search"><img src="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/embedding.png" class="img-fluid figure-img"></a></p>
<figcaption>Word embeddings</figcaption>
</figure>
</div>
<p>Some open-source libraries, such as Gensim and fastText, allow you to obtain pre-trained Word2Vec and GloVe embeddings quickly. In the good ol‚Äô days of NLP (2013), people used these models to compute word embeddings, which were helpful as inputs to other models. For example, you can compute the word embeddings of each word in a sentence and then pass that as input to a sci-kit learn classifier to classify the sentiment of the sentence.</p>
<p>Glove and Word2Vec have fixed representations. Once they are trained, each word is assigned a fixed vector representation, regardless of their context (so ‚Äúbank‚Äù in ‚Äúriver bank‚Äù and ‚Äúsavings bank‚Äù would have the same embedding). <strong>Word2vec and GloVe will struggle with words that have multiple meanings.</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/word2vec_meme.jpeg" class="img-fluid figure-img"></p>
<figcaption>The good ol‚Äô days of NLP</figcaption>
</figure>
</div>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>Understanding the details of word2vec and GloVe is unnecessary to understand the rest of the blog post and sentence embeddings, so I‚Äôll skip them. I recommend reading this <a href="https://lena-voita.github.io/nlp_course/word_embeddings.html">chapter from the excellent interactive NLP course</a> if you‚Äôre interested.</p>
<p>As a TL;DR</p>
<ul>
<li>Word2Vec is trained by passing a very large corpus and training a shallow neural network to predict the surrounding words. Later alternatives predict the center word given the surrounding words.</li>
<li>GloVe is trained by looking at the co-occurrence matrix of words (how often words appear together within a certain distance) and then using that matrix to obtain the embeddings.</li>
</ul>
<p>Word2Vec and GloVe are trained with objectives that ensure that words appearing in similar contexts have similar embeddings.</p>
</div>
</div>
</div>
</section>
<section id="word-embeddings-with-transformers" class="level3">
<h3 class="anchored" data-anchor-id="word-embeddings-with-transformers">Word Embeddings with Transformers</h3>
<p>More recently, with the advent of transformers, we have new ways to compute embeddings. The embedding is also learned, but instead of training an embedding model and then another model for the specific task, transformers learn useful embeddings in the context of their task. For example, BERT, a popular transformer model, learns word embeddings in the context of masked language modeling (predicting which word to fill in the blank) and next sentence prediction (whether sentence B follows sentence A).</p>
<p>Transformers are state-of-the-art in many NLP tasks and can capture contextual information that word2vec and GloVe cannot capture, thanks to a mechanism called attention. Attention allows the model to weigh other words‚Äô importance and capture contextual information. For example, in the sentence ‚ÄúI went to the bank to deposit money‚Äù, the word ‚Äúbank‚Äù is ambiguous. Is it a river bank or a savings bank? The model can use the word ‚Äúdeposit‚Äù to understand that it‚Äôs a savings bank. These are <strong>contextualized embeddings</strong> - their word embedding can differ based on their surrounding words.</p>
<p>Ok‚Ä¶we talked a lot about word embeddings; time to run some code. Let‚Äôs use a pre-trained transformer model, <a href="https://huggingface.co/bert-base-uncased">bert-base-uncased</a>, and obtain some word embeddings. We‚Äôll use the <code>transformers</code> library for this. Let‚Äôs begin by loading the model and its tokenizer</p>
<div id="ee696c78" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AutoModel, AutoTokenizer</span>
<span id="cb16-2"></span>
<span id="cb16-3">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bert-base-uncased"</span>)</span>
<span id="cb16-4">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModel.from_pretrained(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bert-base-uncased"</span>)</span></code></pre></div></div>
</div>
<p>We haven‚Äôt talked about tokenization so far. Until now, we‚Äôve assumed we split data into words. When using transformers, we divided text into tokens. For example, the word ‚Äúbanking‚Äù could be split into two tokens, ‚Äúbank‚Äù and ‚Äúing‚Äù. The tokenizer is responsible for breaking the data into tokens, and the way it splits the data is model-specific and is a deterministic learning process, which means that the same word will always be split into the same tokens. Let‚Äôs see what this looks like in code:</p>
<div id="ea42bd65" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The king and the queen are happy."</span></span>
<span id="cb17-2">tokenizer.tokenize(text, add_special_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>['[CLS]', 'the', 'king', 'and', 'the', 'queen', 'are', 'happy', '.', '[SEP]']</code></pre>
</div>
</div>
<p>Alright, in this example, each word was a token! (this is not always the case, as we‚Äôll soon see). But we also see two things that might be unexpected: <code>[CLS]</code> and <code>[SEP]</code>. These are special tokens added to the sentence‚Äôs beginning and end. These are used because BERT was trained with that format. One of BERT‚Äôs training objectives is next-sentence prediction, which means that it was trained to predict whether two sentences are consecutive. The <code>[CLS]</code> token represents the entire sentence, and the <code>[SEP]</code> token separates sentences. This will be interesting later when we talk about sentence embeddings.</p>
<p>Let‚Äôs now obtain the embeddings of each token.</p>
<div id="f0ba5cf4" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">encoded_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(text, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb19-2">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>encoded_input)</span>
<span id="cb19-3">output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>].shape</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1, 10, 768])</code></pre>
</div>
</div>
<p>Great! BERT is giving us an embedding of 768 values for each token. Each of these tokens has semantic information - <strong>they capture the meaning of the word in the context of the sentence</strong>. Let‚Äôs see if the embedding corresponding to the word ‚Äúking‚Äù in this context is similar to the one in ‚Äúqueen‚Äù.</p>
<div id="311a561e" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">king_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 2 is the position of king</span></span>
<span id="cb21-2">queen_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 5 is the position of queen</span></span>
<span id="cb21-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Shape of embedding </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>king_embedding<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>shape<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb21-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(</span>
<span id="cb21-5">    <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Similarity between king and queen embedding </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>util<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>pytorch_cos_sim(king_embedding, queen_embedding)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb21-6">)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Shape of embedding torch.Size([768])
Similarity between king and queen embedding 0.7920711040496826</code></pre>
</div>
</div>
<p>Ok, it seems they are quite similar in this context! Let‚Äôs now look at the word ‚Äúhappy‚Äù.</p>
<div id="f95cbfca" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">happy_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> output.last_hidden_state[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># happy</span></span>
<span id="cb23-2">util.pytorch_cos_sim(king_embedding, happy_embedding)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[0.5239]], grad_fn=&lt;MmBackward0&gt;)</code></pre>
</div>
</div>
<p>This makes sense; the queen embedding is more similar to the king than the happy embedding.</p>
<p>Let‚Äôs now look at how the same word can have different values depending on the context:</p>
<div id="13151595" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The angry and unhappy king"</span></span>
<span id="cb25-2">encoded_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(text, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb25-3">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>encoded_input)</span>
<span id="cb25-4">output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>].shape</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1, 7, 768])</code></pre>
</div>
</div>
<div id="c22354ba" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">tokenizer.tokenize(text, add_special_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>['[CLS]', 'the', 'angry', 'and', 'unhappy', 'king', '[SEP]']</code></pre>
</div>
</div>
<div id="25940625" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1">king_embedding_2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]</span>
<span id="cb29-2">util.pytorch_cos_sim(king_embedding, king_embedding_2)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[0.5740]], grad_fn=&lt;MmBackward0&gt;)</code></pre>
</div>
</div>
<p>Wow! Although both embeddings seem to correspond to the ‚Äúking‚Äù embedding, they are pretty different in the vector space. What is going on? Remember that these are contextual embeddings. The context of the first sentence is quite positive, while the second sentence is quite negative. Hence, the embeddings are different.</p>
<p>Previously, we discussed how the tokenizer might split a word into multiple tokens. A valid question is how we would obtain the word embedding in such a case. Let‚Äôs look at an example with the long word ‚Äútokenization.‚Äù</p>
<div id="f77c3394" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">tokenizer.tokenize(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tokenization"</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>['token', '##ization']</code></pre>
</div>
</div>
<p>The word ‚Äútokenization‚Äù was split into two tokens, but we care about the embedding of ‚Äútokenization‚Äù! What can we do? We can do a <strong>pooling strategy</strong> in which we obtain the embedding of each token and then average them to obtain the word embedding. Let‚Äôs try it out!</p>
<p>As before, we get started by tokenizing the test and running the token IDs through the model.</p>
<div id="db8675d0" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"this is about tokenization"</span></span>
<span id="cb33-2"></span>
<span id="cb33-3">encoded_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(text, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb33-4">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>encoded_input)</span></code></pre></div></div>
</div>
<p>Let‚Äôs look at the tokenization of the sentence:</p>
<div id="6321f902" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">tokenizer.tokenize(text, add_special_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>['[CLS]', 'this', 'is', 'about', 'token', '##ization', '[SEP]']</code></pre>
</div>
</div>
<p>So we want to pool the embeddings of the tokens 4 and 5 by averaging them. Let‚Äôs first obtain the embeddings of the tokens.</p>
<div id="e13fd5e1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">word_token_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]</span>
<span id="cb36-2">word_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, word_token_indices]</span>
<span id="cb36-3">word_embeddings.shape</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([2, 768])</code></pre>
</div>
</div>
<p>And now let‚Äôs average them using <code>torch.mean</code>.</p>
<div id="d179cd71" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb38-2"></span>
<span id="cb38-3">torch.mean(word_embeddings, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>).shape</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([768])</code></pre>
</div>
</div>
<p>Let‚Äôs wrap all of it in a function so we can easily use it later.</p>
<div id="5aef9630" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_word_embedding(text, word):</span>
<span id="cb40-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Encode the text and do a forward pass through the model to get the hidden states</span></span>
<span id="cb40-3">    encoded_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(text, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb40-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We don't need gradients for embedding extraction</span></span>
<span id="cb40-5">        output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>encoded_input)</span>
<span id="cb40-6"></span>
<span id="cb40-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Find the indices for the word</span></span>
<span id="cb40-8">    word_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer.encode(</span>
<span id="cb40-9">        word, add_special_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span></span>
<span id="cb40-10">    )  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># No special tokens anymore</span></span>
<span id="cb40-11">    word_token_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb40-12">        i</span>
<span id="cb40-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, token_id <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(encoded_input[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"input_ids"</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb40-14">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> token_id <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> word_ids</span>
<span id="cb40-15">    ]</span>
<span id="cb40-16"></span>
<span id="cb40-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Pool the embeddings for the word</span></span>
<span id="cb40-18">    word_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, word_token_indices]</span>
<span id="cb40-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.mean(word_embeddings, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span></code></pre></div></div>
</div>
<p><strong>Example 1.</strong> Similarity between king and queen embeddings in the context of both being angry.</p>
<div id="eb120b8b" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">util.pytorch_cos_sim(</span>
<span id="cb41-2">    get_word_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The king is angry"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"king"</span>),</span>
<span id="cb41-3">    get_word_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The queen is angry"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"queen"</span>),</span>
<span id="cb41-4">)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[0.8564]])</code></pre>
</div>
</div>
<p><strong>Example 2.</strong> Similarity between king and queen embeddings in the context of the king being happy and the queen angry. Notice how they are less similar than in the previous example.</p>
<div id="d915e800" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1">util.pytorch_cos_sim(</span>
<span id="cb43-2">    get_word_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The king is happy"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"king"</span>),</span>
<span id="cb43-3">    get_word_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The queen is angry"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"queen"</span>),</span>
<span id="cb43-4">)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[0.8273]])</code></pre>
</div>
</div>
<p><strong>Example 3</strong>. Similarity between king embeddings in two very different contexts. Even if they are the same word, the different context of the word makes the embeddings very different.</p>
<div id="88fb50b7" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This is same as before</span></span>
<span id="cb45-2">util.pytorch_cos_sim(</span>
<span id="cb45-3">    get_word_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The king and the queen are happy."</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"king"</span>),</span>
<span id="cb45-4">    get_word_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The angry and unhappy king"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"king"</span>),</span>
<span id="cb45-5">)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[0.5740]])</code></pre>
</div>
</div>
<p><strong>Example 4.</strong> Similarity between a word that has two different meanings. The word ‚Äúbank‚Äù is ambiguous, it can be a river bank or a savings bank. The embeddings are different depending on the context.</p>
<div id="daf5635c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb47" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1">util.pytorch_cos_sim(</span>
<span id="cb47-2">    get_word_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The river bank"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bank"</span>),</span>
<span id="cb47-3">    get_word_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"The savings bank"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bank"</span>),</span>
<span id="cb47-4">)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[0.7587]])</code></pre>
</div>
</div>
<p>I hope this gave an idea about what word embeddings are. Now that we understand word embeddings let‚Äôs look into sentence embeddings!</p>
</section>
<section id="sentence-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="sentence-embeddings">Sentence Embeddings</h3>
<p>Just as word embeddings are vector representations of words, sentence embeddings are vector representations of a sentence. We can also compute embeddings of paragraphs and documents! Let‚Äôs look into it.</p>
<p>There are three approaches we can take: <code>[CLS]</code> pooling, max pooling and mean pooling.</p>
<ul>
<li>Mean pooling means averaging all the word embeddings of the sentence.</li>
<li>Max pooling means taking the maximum value of each dimension of the word embeddings.</li>
<li><code>[CLS]</code> pooling means using the embedding corresponding to the <code>[CLS]</code> token as the sentence embedding. Let‚Äôs look deeper into this last one, which is the least intuitive.</li>
</ul>
<section id="cls-pooling" class="level4">
<h4 class="anchored" data-anchor-id="cls-pooling">[CLS] Pooling</h4>
<p>As we saw before, BERT adds a special token <code>[CLS]</code> at the beginning of the sentence. This token is used to represent the entire sentence. For example, when someone wants to fine-tune a BERT model to perform text classification, a common approach is to add a linear layer on top of the <code>[CLS]</code> embedding. The idea is that the <code>[CLS]</code> token will capture the meaning of the entire sentence.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/classification.png" class="img-fluid figure-img"></p>
<figcaption>The hidden state/embedding corresponding to the <code>CLS</code> token can be used to fine-tune a classification model.</figcaption>
</figure>
</div>
<p>We can take the same approach and use the embedding of the [CLS] token as the sentence embedding. Let‚Äôs see how this works in code. We‚Äôll use the same sentence as before.</p>
<div id="41854372" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1">encoded_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"This is an example sentence"</span>, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb49-2">model_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>encoded_input)</span>
<span id="cb49-3">sentence_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>][:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, :]</span>
<span id="cb49-4">sentence_embedding.shape</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1, 768])</code></pre>
</div>
</div>
<p>Great! We obtained the model output‚Äôs first embedding, corresponding to the [CLS] token. Let‚Äôs wrap this code into a function.</p>
<div id="e4c53797" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> cls_pooling(model_output):</span>
<span id="cb51-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> model_output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>][:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, :]</span>
<span id="cb51-3"></span>
<span id="cb51-4"></span>
<span id="cb51-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_sentence_embedding(text):</span>
<span id="cb51-6">    encoded_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(text, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb51-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb51-8">        model_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>encoded_input)</span>
<span id="cb51-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> cls_pooling(model_output)</span></code></pre></div></div>
</div>
<div id="0e8ec816" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb52" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1">embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [get_sentence_embedding(sentence) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sentences]</span>
<span id="cb52-2">query_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_sentence_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Today is a sunny day"</span>)</span>
<span id="cb52-3"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> embedding, sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(embeddings, sentences):</span>
<span id="cb52-4">    similarity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> util.pytorch_cos_sim(query_embedding, embedding)</span>
<span id="cb52-5">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(similarity, sentence)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.9261]]) The weather today is beautiful
tensor([[0.8903]]) It's raining!
tensor([[0.9317]]) Dogs are awesome</code></pre>
</div>
</div>
<p>Hmm‚Ä¶something looks off here ü§î One would have expected this to work out of the box.</p>
<p>Well, it turns out BERT has an additional trick. As mentioned before, when BERT was trained, the CLS token was used to predict whether two sentences were consecutive. To do so, BERT processes the [CLS]-corresponding embedding and passes it through a linear layer and a tanh activation function (see <a href="https://github.com/huggingface/transformers/blob/95754b47a6d4fbdad3440a45762531e8c471c528/src/transformers/models/bert/modeling_bert.py#L652C7-L665">code here</a>). The idea is that the linear layer and the tanh activation function will learn a better representation of the <code>[CLS]</code> token. This is the <code>pooler</code> component of the BERT model and is used to obtain the <code>model_output.pooler_output</code>.</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>This might sound confusing, so let‚Äôs repeat what‚Äôs happening here.</p>
<ol type="1">
<li>BERT outputs the embeddings of each token.</li>
<li>The first embedding corresponds to the <code>[CLS]</code> token.</li>
<li>The <code>[CLS]</code> token is processed through a linear layer and a tanh activation function to obtain the <code>pooler_output</code>.</li>
</ol>
<p>During training, the pooler_output is used to predict whether two sentences are consecutive (one of the pre-training tasks of BERT). This makes processing the [CLS] token more meaningful than the raw [CLS] embedding.</p>
</div>
</div>
</div>
<p>To show that there is no magic going on here, we can either pass the list of word embeddings to <code>model.pooler</code> or simply get the <code>pooler_output</code> from the model output. Let‚Äôs try it out!</p>
<div id="142e73c2" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb54" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1">model.pooler(model_output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>])[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.9302, -0.4884, -0.4387,  0.8024,  0.3668, -0.3349,  0.9438,  0.3593,
        -0.3216, -1.0000], grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<div id="a0309609" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb56" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1">model_output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pooler_output"</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.9302, -0.4884, -0.4387,  0.8024,  0.3668, -0.3349,  0.9438,  0.3593,
        -0.3216, -1.0000], grad_fn=&lt;SliceBackward0&gt;)</code></pre>
</div>
</div>
<p>Yay! As you can see, the first ten elements of the embedding are identical! Let‚Äôs now re-compute the distances using this new embedding technique:</p>
<div id="03e4d03d" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> cls_pooling(model_output):</span>
<span id="cb58-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> model.pooler(model_output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>])  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># we changed this</span></span>
<span id="cb58-3"></span>
<span id="cb58-4"></span>
<span id="cb58-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This stays the same</span></span>
<span id="cb58-6">embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [get_sentence_embedding(sentence) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sentences]</span>
<span id="cb58-7">query_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_sentence_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Today is a sunny day"</span>)</span>
<span id="cb58-8"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> embedding, sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(embeddings, sentences):</span>
<span id="cb58-9">    similarity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> util.pytorch_cos_sim(query_embedding, embedding)</span>
<span id="cb58-10">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(similarity, sentence)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.9673]], grad_fn=&lt;MmBackward0&gt;) The weather today is beautiful
tensor([[0.9029]], grad_fn=&lt;MmBackward0&gt;) It's raining!
tensor([[0.8930]], grad_fn=&lt;MmBackward0&gt;) Dogs are awesome</code></pre>
</div>
</div>
<p>Much, much better! We just obtained the closest sentences to ‚ÄúToday is a sunny day‚Äù.</p>
</section>
</section>
</section>
<section id="sentence-transformers" class="level2">
<h2 class="anchored" data-anchor-id="sentence-transformers">Sentence Transformers</h2>
<section id="using-the-transformers-library" class="level3">
<h3 class="anchored" data-anchor-id="using-the-transformers-library">Using the transformers library</h3>
<p>This yields some decent results, but in practice, this was not much better than using Word2Vec or GloVe word embeddings and averaging them. The reason is that the [CLS] token is not trained to be a good sentence embedding. It‚Äôs trained to be a good sentence embedding for next-sentence prediction!</p>
<p>Introducing ü•Åü•Åü•Å Sentence Transformers! Sentence Sentence Transformers (also known as SBERT) have a special training technique focusing on yielding high-quality sentence embeddings. Just as in the TL;DR section of this blog post, let‚Äôs use the <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">all-MiniLM-L6-v2</a> model. In the beginning, we used the <code>sentence-transformers</code> library, which is a high-level wrapper library around <code>transformers</code>. Let‚Äôs try to go the hard way first! The process is as follows:</p>
<ol type="1">
<li>We tokenize the input sentence.</li>
<li>We process the tokens through the model.</li>
<li>We calculate the mean of the token embeddings.</li>
<li>We normalize the embeddings to ensure the embedding vector has a unit length.</li>
</ol>
<p>Just as before, we can load the model and the tokenizer, tokenize the sentence and pass it to the model</p>
<div id="9e849c1f" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sentence-transformers/all-MiniLM-L6-v2"</span>)</span>
<span id="cb60-2">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModel.from_pretrained(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sentence-transformers/all-MiniLM-L6-v2"</span>)</span>
<span id="cb60-3">encoded_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Today is a sunny day"</span>, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb60-4">model_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>encoded_input)</span></code></pre></div></div>
</div>
<p>What we‚Äôve done until now is very similar to what we did before, except that we are using a different model. The next step is to do pooling. While previously we did [CLS] pooling, sentence transformers usually use mean or max pooling. Let‚Äôs try it out!</p>
<div id="bb460daa" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb61" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1">token_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>]</span>
<span id="cb61-2">token_embeddings.shape</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1, 7, 384])</code></pre>
</div>
</div>
<p>Note how, with this model, each embedding is smaller (384 values rather than 768). We can now compute the mean of the embeddings to obtain the sentence embedding.</p>
<div id="d10efd64" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb63" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1">mean_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.mean(token_embeddings, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb63-2">mean_embedding.shape</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1, 384])</code></pre>
</div>
</div>
<p>The last step is to perform normalization. Normalization ensures that the embedding vector has a unit length, which means its length (or magnitude) is 1.</p>
<div class="callout callout-style-default callout-note callout-titled" title="What is normalization?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>What is normalization?
</div>
</div>
<div class="callout-body-container callout-body">
<p>To understand why we do normalization, revisiting some vector math is helpful. For a vector v with components (v1, v2, ‚Ä¶, vn), it‚Äôs length is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%20%5Cmathbf%7Bv%7D%20%5C%7C%20=%20%5Csqrt%7Bv_1%5E2%20+%20v_2%5E2%20+%20%5Cldots%20+%20v_n%5E2%7D%0A"></p>
<p>When normalizing a vector, we scale the values so that the vector length is 1. This is done by dividing each vector element by the vector‚Äôs magnitude.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7Bu%7D%20=%20%5Cfrac%7B%5Cmathbf%7Bv%7D%7D%7B%5C%7C%20%5Cmathbf%7Bv%7D%20%5C%7C%7D%0A"></p>
</div>
</div>
<p>This is particularly helpful when we want to compare vectors. For example, if we want to compute the cosine similarity between two vectors, we usually compare their direction rather than their magnitude. Normalizing the vectors ensures that each vector contributes equally to the similarity. We‚Äôll talk more about embedding comparisons soon! Let‚Äôs try it out!</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Actually, we are using cosine similarity to compute the similarity between embeddings. As we‚Äôll see later in the blog post, the magnitude of the embeddings is not relevant when computing the cosine similarity, but it‚Äôs still a good think to normalize them in case we want to experiment with other ways to measure distances.</p>
</div>
</div>
<div id="a79d304a" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.nn.functional <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> F</span>
<span id="cb65-2"></span>
<span id="cb65-3">normalized_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> F.normalize(mean_embedding)</span>
<span id="cb65-4">normalized_embedding.shape</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([1, 384])</code></pre>
</div>
</div>
<p>Let‚Äôs wrap this in a function!</p>
<div id="4c1ffd4c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> mean_pooling(model_output):</span>
<span id="cb67-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.mean(model_output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>], dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb67-3"></span>
<span id="cb67-4"></span>
<span id="cb67-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_sentence_embedding(text):</span>
<span id="cb67-6">    encoded_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(text, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>)</span>
<span id="cb67-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb67-8">        model_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>encoded_input)</span>
<span id="cb67-9">    sentence_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean_pooling(model_output)</span>
<span id="cb67-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> F.normalize(sentence_embeddings)</span>
<span id="cb67-11"></span>
<span id="cb67-12"></span>
<span id="cb67-13">get_sentence_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Today is a sunny day"</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.0926,  0.5913,  0.5535,  0.4214,  0.2129])</code></pre>
</div>
</div>
<p>In practice, you‚Äôll likely be encoding batches of sentences, so we need to make some changes</p>
<ul>
<li>Modify the tokenization so we apply <code>truncation</code> (cutting the sentence if it‚Äôs longer than the maximum length) and <code>padding</code> (adding <code>[PAD]</code> tokens to the end of the sentence).</li>
<li>Modify the pooling so we take the attention mask into account. The attention mask is a vector of 0s and 1s that indicates which tokens are real and which are padding. We want to ignore the padding tokens when computing the mean!</li>
</ul>
<div id="4c7abec0" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb69" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> mean_pooling(model_output, attention_mask):</span>
<span id="cb69-2">    token_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_output[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"last_hidden_state"</span>]</span>
<span id="cb69-3">    input_mask_expanded <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb69-4">        attention_mask.unsqueeze(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).expand(token_embeddings.size()).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>()</span>
<span id="cb69-5">    )</span>
<span id="cb69-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> torch.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(token_embeddings, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> torch.clamp(</span>
<span id="cb69-7">        input_mask_expanded.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>), <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-9</span></span>
<span id="cb69-8">    )</span>
<span id="cb69-9"></span>
<span id="cb69-10"></span>
<span id="cb69-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This now receives a list of sentences</span></span>
<span id="cb69-12"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_sentence_embedding(sentences):</span>
<span id="cb69-13">    encoded_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(</span>
<span id="cb69-14">        sentences, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span></span>
<span id="cb69-15">    )</span>
<span id="cb69-16">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb69-17">        model_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>encoded_input)</span>
<span id="cb69-18">    sentence_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> mean_pooling(model_output, encoded_input[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"attention_mask"</span>])</span>
<span id="cb69-19">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> F.normalize(sentence_embeddings)</span></code></pre></div></div>
</div>
<div id="fe620161" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb70" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1">query_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_sentence_embedding(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Today is a sunny day"</span>)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb70-2">query_embedding[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([-0.0163,  0.1041,  0.0974,  0.0742,  0.0375])</code></pre>
</div>
</div>
<p>We got the same result, great! Let‚Äôs now repeat our search example from before.</p>
<div id="690905ce" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb72" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1">embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [get_sentence_embedding(sentence) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> sentences]</span>
<span id="cb72-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> embedding, sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(embeddings, sentences):</span>
<span id="cb72-3">    similarity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> util.pytorch_cos_sim(query_embedding, embedding)</span>
<span id="cb72-4">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(similarity, sentence)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.7344]]) The weather today is beautiful
tensor([[0.4180]]) It's raining!
tensor([[0.1060]]) Dogs are awesome</code></pre>
</div>
</div>
<p>Nice! Compared to the vanilla BERT [CLS]-pooled embeddings, the sentence transformer embeddings are more meaningful and have a larger difference between the unrelated vectors!</p>
<div class="callout callout-style-simple callout-none no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p>When to use each pooling strategy? It depends on the task.</p>
<ul>
<li><code>[CLS]</code> pooling is usually used when the transformer model has been fine-tuned on a specific downstream task that makes the <code>[CLS]</code> token very useful.</li>
<li>Mean pooling is usually more effective on models that have not been fine-tuned on a downstream task. It ensures that all parts of the sentence are represented equally in the embedding and can work for long sentences where the influence of all tokens should be captured.</li>
<li>Max pooling can be useful to capture the most important features in a sentence. This can be very useful if particular keywords are very informative, but it might miss the subtler context.</li>
</ul>
<p>In practice, a pooling method will be stored with the model, and you won‚Äôt have to worry about it. If there‚Äôs no method specified, mean pooling is usually a good default.</p>
</div>
</div>
</div>
</section>
<section id="using-the-sentence-transformers-library" class="level3">
<h3 class="anchored" data-anchor-id="using-the-sentence-transformers-library">Using the sentence-transformers library</h3>
<p>This was relatively easy, but the <code>sentence-transformers</code> library makes it even easier for us to do all of this! Here is the same code as in the TL;DR section.</p>
<div id="21e13451" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> sentence_transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SentenceTransformer</span>
<span id="cb74-2"></span>
<span id="cb74-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We load the model</span></span>
<span id="cb74-4">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SentenceTransformer(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sentence-transformers/all-MiniLM-L6-v2"</span>)</span>
<span id="cb74-5"></span>
<span id="cb74-6">query_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.encode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Today is a sunny day"</span>)</span>
<span id="cb74-7">embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.encode(sentences)</span>
<span id="cb74-8"></span>
<span id="cb74-9"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> embedding, sentence <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(embeddings, sentences):</span>
<span id="cb74-10">    similarity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> util.pytorch_cos_sim(query_embedding, embedding)</span>
<span id="cb74-11">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(similarity, sentence)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[0.7344]]) The weather today is beautiful
tensor([[0.4180]]) It's raining!
tensor([[0.1060]]) Dogs are awesome</code></pre>
</div>
</div>
<p>This is quite powerful! If you had to implement a feature to identify duplicate questions without using ML, you would likely have to implement a lexical search system (which looks at exact matches of the input question), a fuzzy search system (which looks at approximate matches of the input question), or a statistical search system (which looks at the frequency of words in the input question).</p>
<p>With embeddings, we can easily find similar questions without implementing any of these systems and having excellent results!</p>
<p>The following image is a good example of how embeddings can be used to find code that would answer a user‚Äôs question.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/search.png" class="img-fluid figure-img"></p>
<figcaption>Image of code search</figcaption>
</figure>
</div>
</section>
<section id="embedding-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="embedding-dimensions">Embedding dimensions</h3>
<p>As you saw before, the model we used, all-MiniLM-L6-v2, generates sentence embeddings of 384 values. This is a hyperparameter of the model and can be changed. The larger the embedding size, the more information the embedding can capture. However, larger embeddings are more expensive to compute and store.</p>
<p>The embeddings of popular open-source models go from 384 to 1024. The best current model, as of the time of writing, has embedding dimensions of 4096 values, but the model is much larger (7 billion parameters) compared to other models. In the closed-sourced world, Cohere has APIs that go from 384 to 4096 dimensions, OpenAI has embeddings of 1536, and so on. <strong>Embedding dimension is a trade-off</strong>. If you use very large embeddings, you will potentially get better results, but you will also have to pay more for hosting and inference. If you use vector databases, you will also have to pay more for storage.</p>
</section>
<section id="sequence-length" class="level3">
<h3 class="anchored" data-anchor-id="sequence-length">Sequence length</h3>
<p>One of the limitations of transformer models is that they have a maximum sequence length. This means that they can only process a certain number of tokens. For example, BERT has a maximum context length of 512 tokens. This means that if you want to encode a sentence with more than 512 tokens, you will have to find ways to work around this limitation. For example, you could split the sentence into multiple sentences of 512 tokens and then average the embeddings. This is not ideal because the model will not be able to capture the context of the entire sentence.</p>
<p>This is not a problem for most use cases, but it can be a problem for long documents. For example, if you want to encode a 1000-word document, you will have to split it into multiple sentences of 512 tokens. This is not ideal because the model will not be able to capture the context of the entire document. Another approach can be to first generate a summary of the text and then encode the summary. This is a good approach if you want to encode long documents, but will require a good summarization model that might be too slow. Alternatively, you might know if a specific part of the document is good (such as abstracts, introductions, conclusions, etc.) and only encode that part if that‚Äôs the most meaningful part for your task.</p>
</section>
</section>
<section id="application-1.-finding-most-similar-quora-duplicate" class="level2">
<h2 class="anchored" data-anchor-id="application-1.-finding-most-similar-quora-duplicate">Application 1. Finding most similar Quora duplicate</h2>
<p>We‚Äôre going to use the open-source <a href="https://huggingface.co/datasets/quora">Quora dataset</a>, which contains 400,000 pairs of questions from Quora. We will not train a model (yet!) and rather just use the embeddings to find similar questions given a new question. Let‚Äôs get started!</p>
<p>Our first step will be to load the data - to do this, we‚Äôll use the <code>datasets</code> library.</p>
<div id="2bfdbf70" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb76" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!</span>pip install datasets</span></code></pre></div></div>
</div>
<div id="422811e5" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb77" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_dataset</span>
<span id="cb77-2"></span>
<span id="cb77-3">dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"quora"</span>)[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>]</span>
<span id="cb77-4">dataset</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>Dataset({
    features: ['questions', 'is_duplicate'],
    num_rows: 404290
})</code></pre>
</div>
</div>
<p>To take a quick look at the data within the <code>Dataset</code> object, we can convert it to a Pandas <code>DataFrame</code> and look at the first rows.</p>
<div id="b78d9700" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb79" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1">dataset.to_pandas().head()</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">questions</th>
<th data-quarto-table-cell-role="th">is_duplicate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>{'id': [1, 2], 'text': ['What is the step by s...</td>
<td>False</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>{'id': [3, 4], 'text': ['What is the story of ...</td>
<td>False</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>{'id': [5, 6], 'text': ['How can I increase th...</td>
<td>False</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>{'id': [7, 8], 'text': ['Why am I mentally ver...</td>
<td>False</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>{'id': [9, 10], 'text': ['Which one dissolve i...</td>
<td>False</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Ok, so each sample is a dictionary. We do not care about the <code>is_duplicate</code> column here. Our goal is to find if any question in this dataset is similar to a new question. Let‚Äôs process the dataset so we only have a list of questions.</p>
<div id="8cbb0afe" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb80" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1">corpus_questions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb80-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> d <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> dataset:</span>
<span id="cb80-3">    corpus_questions.append(d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"questions"</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span>
<span id="cb80-4">    corpus_questions.append(d[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"questions"</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>][<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb80-5">corpus_questions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">set</span>(corpus_questions))  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Remove duplicates</span></span>
<span id="cb80-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(corpus_questions)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>537362</code></pre>
</div>
</div>
<p>The next step is to embed all the questions. We‚Äôll use the <code>sentence-transformers</code> library for this. We‚Äôll use the <a href="https://huggingface.co/sentence-transformers/quora-distilbert-multilingual"><code>quora-distilbert-multilingual</code> model</a>, which is a model trained for 100 languages and is trained specifically for Quora-style questions. This is a larger model, and hence will be slightly slower. It will also generate larger embeddings of 768 values.</p>
<p>To get some quick results without having to wait five minutes for the model to process all the questions, we‚Äôll only process the first 100000 questions. In practice, you would process all the questions or shuffle the questions and process a random subset of them when experimenting.</p>
<div id="4ed7f45e" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb82" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SentenceTransformer(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"quora-distilbert-multilingual"</span>)</span>
<span id="cb82-2">questions_to_embed <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span></span>
<span id="cb82-3">corpus_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.encode(</span>
<span id="cb82-4">    corpus_questions[:questions_to_embed],</span>
<span id="cb82-5">    show_progress_bar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb82-6">    convert_to_tensor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb82-7">)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"30cc6aa1ede9402ea9cd36fb9aed9376","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<div id="bdb3c885" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb83" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1">corpus_embeddings.shape</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([100000, 768])</code></pre>
</div>
</div>
<p>We just obtained 100,000 embddings in 20 seconds, even when this Sentence Transformer model is not tiny and I‚Äôm running this on my GPU-Poor computer. Unlike generative models, which are autoregressive and usually much slower, BERT-based models are super fast!</p>
<p>Let‚Äôs now write a function that searches the corpus for the most similar question.</p>
<div id="749b3c2e" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb85" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> time</span>
<span id="cb85-2"></span>
<span id="cb85-3"></span>
<span id="cb85-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> search(query):</span>
<span id="cb85-5">    start_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb85-6">    query_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.encode(query, convert_to_tensor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb85-7">    results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> util.semantic_search(query_embedding, corpus_embeddings)</span>
<span id="cb85-8">    end_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb85-9"></span>
<span id="cb85-10">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Results (after </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{:.3f}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> seconds):"</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(end_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> start_time))</span>
<span id="cb85-11">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We look at top 5 results</span></span>
<span id="cb85-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> result <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> results[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>][:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]:</span>
<span id="cb85-13">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(</span>
<span id="cb85-14">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{:.3f}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\t</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(result[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"score"</span>], corpus_questions[result[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"corpus_id"</span>]])</span>
<span id="cb85-15">        )</span></code></pre></div></div>
</div>
<div id="6eb8fb77" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb86" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1">search(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"How can I learn Python online?"</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Results (after 0.612 seconds):
0.982   What is the best online resource to learn Python?
0.980   Where I should learn Python?
0.980   What's the best way to learn Python?
0.980   How do I learn Python in easy way?
0.979   How do I learn Python systematically?</code></pre>
</div>
</div>
<p>Let‚Äôs try in Spanish!</p>
<div id="d9ca38cb" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb88" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1">search(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Como puedo aprender Python online?"</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Results (after 0.016 seconds):
0.980   What are the best websites to learn Python?
0.980   How can I start learning the developing of websites using Python?
0.979   How do I learn Python in easy way?
0.976   How can I learn Python faster and effectively?
0.976   How can I learn advanced Python?</code></pre>
</div>
</div>
<p>It seems to be working quite well! Note that although our model can process queries in other languages, such as Spanish in the example above, the embeddings were generated for English questions. This means that the model will not be able to find similar questions in other languages.</p>
</section>
<section id="distance-between-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="distance-between-embeddings">Distance between embeddings</h2>
<section id="cosine-similarity" class="level3">
<h3 class="anchored" data-anchor-id="cosine-similarity">Cosine similarity</h3>
<p>Until now we‚Äôve been computing the cosine similarity between embeddings. This is a number between 0 and 1 that indicates how similar two embeddings are. A value of 1 means that the embeddings are identical, while 0 means that the embeddings are entirely different. So far we‚Äôve used it as a black-box, so let‚Äôs look into it a bit more.</p>
<p>The cosine similarity allows us to compare how similar two vectors are regardless of their magnitude. For example, if we have two vectors, [1, 2, 3] and [2, 4, 6], they are very similar in terms of direction, but their magnitude is different. The cosine similarity will be close to 1, indicating that they are very similar.</p>
<div id="d01074f9" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb90" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1">a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.FloatTensor([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>])</span>
<span id="cb90-2">b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.FloatTensor([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>])</span>
<span id="cb90-3">util.cos_sim(a, b)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[0.9926]])</code></pre>
</div>
</div>
<p>Let‚Äôs plot both vectors. As you can see, they are very similar in terms of direction, but their magnitude is different.</p>
<div id="9cb2f58a" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb92" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb92-1">a</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([1., 2., 3.])</code></pre>
</div>
</div>
<div id="eec404a4" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb94" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb94-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb94-3"></span>
<span id="cb94-4">V <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([a.tolist(), b.tolist()])</span>
<span id="cb94-5">origin <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># origin point</span></span>
<span id="cb94-6"></span>
<span id="cb94-7">plt.quiver(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>origin, V[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], V[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"r"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"b"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"g"</span>], scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)</span>
<span id="cb94-8">plt.show()</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/index_files/figure-html/cell-55-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let‚Äôs dive into its math. Cosine similarity is defined as the dot product of the vectors divided by the product of their magnitudes:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bcosine%20similarity%7D(%5Cmathbf%7BA%7D,%20%5Cmathbf%7BB%7D)%20=%20%5Cfrac%7B%5Cmathbf%7BA%7D%20%5Ccdot%20%5Cmathbf%7BB%7D%7D%7B%5C%7C%5Cmathbf%7BA%7D%5C%7C%20%5C%7C%5Cmathbf%7BB%7D%5C%7C%7D%0A"></p>
<p>We already discussed magnitudes at the beginning of the blog post. We need to compute the square root of the sum of the squares of a vector component</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%5Cmathbf%7BA%7D%5C%7C%20=%20%5Csqrt%7B1%5E2%20+%202%5E2%20+%203%5E2%7D%20=%20%5Csqrt%7B14%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5C%7C%5Cmathbf%7BB%7D%5C%7C%20=%20%5Csqrt%7B2%5E2%20+%203%5E2%20+%204%5E2%7D%20=%20%5Csqrt%7B29%7D%0A"></p>
<p>We also need to compute the dot product of the vectors. The dot product is defined as the sum of the products of the corresponding vector components</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20%5Ccdot%20%5Cmathbf%7BB%7D%20=%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20A_i%20B_i%0A"></p>
<p>In this case, the dot product for A and B would look as follows</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20%5Ccdot%20%5Cmathbf%7BB%7D%20=%201%20%5Ctimes%202%20+%202%20%5Ctimes%203%20+%203%20%5Ctimes%204%20=%202%20+%206%20+%2012%20=%2020%0A"></p>
<p>Finally, we can compute the cosine similarity by doing</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bcosine%20similarity%7D(%5Cmathbf%7BA%7D,%20%5Cmathbf%7BB%7D)%20=%20%5Cfrac%7B20%7D%7B%5Csqrt%7B14%7D%20%5Csqrt%7B29%7D%7D%20=%200.992583%0A"></p>
<p>which matches our result above.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Can you think of two vectors with cosine similarity of 1? Think of vectors with same direction but different magnitude.</p>
</div>
</div>
</section>
<section id="dot-product" class="level3">
<h3 class="anchored" data-anchor-id="dot-product">Dot product</h3>
<p>Cosine similarity does not take magnitude into account, but there might be use cases where the magnitude is meaningful. In those cases, <strong>dot product</strong> is a better metric. This means that longer or more verbose sentences with similar content could have a higher similarity score than shorter sentences with similar content due to their magnitude.</p>
<p>The dot product is defined as the sum of the products of the corresponding vector components (it‚Äôs what we did before!)</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BA%7D%20%5Ccdot%20%5Cmathbf%7BB%7D%20=%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20A_i%20B_i%0A"></p>
<p>If you look at the cosine similarity formula, if you assume the vectors are normalized (that is, their magnitude is 1), the cosine similarity is equivalent to the dot product. This means that the cosine similarity is a normalized dot product.</p>
<p>Let‚Äôs create a new vector, [4, 6, 8]. This vector has the same direction as [2, 3, 4], but it‚Äôs twice as long. Let‚Äôs compute the dot product of [1, 2, 3] with [2, 3, 4] and [4, 6, 8].</p>
<div id="ce57221c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb95" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1">c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.FloatTensor([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>])</span>
<span id="cb95-2"></span>
<span id="cb95-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Cosine Similarity between a and b: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>util<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>cos_sim(a, b)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb95-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Cosine Similarity between a and c: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>util<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>cos_sim(a, c)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb95-5"></span>
<span id="cb95-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Dot product between a and b: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>torch<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>dot(a, b)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb95-7"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Dot product between a and c: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>torch<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>dot(a, c)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cosine Similarity between a and b: tensor([[0.9926]])
Cosine Similarity between a and c: tensor([[0.9926]])
Dot product between a and b: 20.0
Dot product between a and c: 40.0</code></pre>
</div>
</div>
<p>This makes sense! As b and c have the same angle, the cosine similarity is the same between a and b and a and c.&nbsp;However, the dot product is higher for a and c because c is longer than b.</p>
<div id="c0305545" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb97" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1">V <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([a.tolist(), b.tolist(), c.tolist()])</span>
<span id="cb97-2">origin <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># origin point</span></span>
<span id="cb97-3"></span>
<span id="cb97-4">plt.quiver(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>origin, V[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], V[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"r"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"b"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"g"</span>], scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20</span>)</span>
<span id="cb97-5">plt.show()</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/index_files/figure-html/cell-57-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="euclidean-distance" class="level3">
<h3 class="anchored" data-anchor-id="euclidean-distance">Euclidean Distance</h3>
<p>The Euclidean Distance is the distance between two vectors by measuring a straight line between them. Just as the dot product, the Euclidean distance takes magnitude into account. I won‚Äôt dive too much into interpreting both metrics, but the main idea is that the Dot Product measures how much one vector extends into the direction of another vector, while the Euclidean Distance measures the straight-line distance between two vectors. It is defined as the square root of the sum of the squared differences between the vector components. It‚Äôs defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BEuclidean%20Distance%7D(%5Cmathbf%7BA%7D,%20%5Cmathbf%7BB%7D)%20=%20%5Csqrt%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(A_i%20-%20B_i)%5E2%7D%0A"></p>
<p>In practice, you can use the Squared Euclidean (L2-Squared)</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BSquared%20Euclidean%7D(%5Cmathbf%7BA%7D,%20%5Cmathbf%7BB%7D)%20=%20%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(A_i%20-%20B_i)%5E2%0A"></p>
</section>
<section id="picking-a-score-function" class="level3">
<h3 class="anchored" data-anchor-id="picking-a-score-function">Picking a score function</h3>
<p>We just learned about dot-product, cosine similarity, and euclidean distance. When to use which?</p>
<p>It depends on the model! Some models will be trained in a way that they produce normalized embeddings. In this case, dot-product, cosine similarity and euclidean distance will all produce the same results.</p>
<p>Other models are not trained in a way that they produce normalized embeddings - they are tuned for dot-product. In this case, dot-product will be the best function to find the closest items in a vector space. Even then, if the magnitude is not important, we can normalize as we did in the previous sections. <strong>You can use different distance functions depending on your use case</strong>. Models with normalized embeddings will prefer shorter sentences, while models with non-normalized embeddings will prefer longer sentences. This is because the magnitude of the embeddings will be larger for longer sentences.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Distance function</th>
<th>Values</th>
<th>When to use</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Cosine similarity</td>
<td>[-1, 1]</td>
<td>When the magnitude is not important</td>
</tr>
<tr class="even">
<td>Dot product</td>
<td>[-inf, inf]</td>
<td>When the magnitude is important</td>
</tr>
<tr class="odd">
<td>Euclidean distance</td>
<td>[0, inf]</td>
<td>When the magnitude is important</td>
</tr>
</tbody>
</table>
<p>To recap:</p>
<ul>
<li><strong>Cosine similarity</strong> focuses on the angle between vectors. It‚Äôs a normalized dot product.</li>
<li><strong>Dot product</strong> focused on both magnitude and angle.</li>
<li><strong>Euclidean distance</strong> measures spatial distance between vectors.</li>
</ul>
<p>There are other distance functions, such as Manhattan distance, but these are common ones and useful for our use cases!</p>
</section>
</section>
<section id="scaling-up" class="level2">
<h2 class="anchored" data-anchor-id="scaling-up">Scaling Up</h2>
<p>Until now we‚Äôve been working with just a couple of sentences. In practice, you might have to deal with millions of embeddings, and we cannot always compute the distance to all of them (this is called brute-force search).</p>
<p>One approach is to use an approximate nearest neighbor algorithm. These algorithms partition the data into buckets of similar embeddings. This allows us to quickly find the closest embeddings without having to compute the distance to all of them. This is not exact, as some vectors with high similarity might still be missed. There are different libraries you can use to do this, such as Spotify‚Äôs <a href="https://github.com/spotify/annoy">Annoy</a> and Facebook‚Äôs <a href="https://github.com/facebookresearch/faiss">Faiss</a>. Vector databases such as Pinecone and Weaviate also use nearest neighbor techniques to be able to search millions of objects in milliseconds.</p>
<p>For now, let‚Äôs look at an interesting application where the scaling issues become more apparent.</p>
<section id="application-2.-paraphrase-mining" class="level3">
<h3 class="anchored" data-anchor-id="application-2.-paraphrase-mining">Application 2. Paraphrase Mining</h3>
<p>Until now, with semantic search, we‚Äôve been looking for the sentence most similar to a query sentence. In <strong>paraphrase mining</strong>, the goal is to find texts with similar meaning in a very large corpus. Let‚Äôs take our Quora dataset and see if we can find similar questions.</p>
<div id="ce3865fd" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb98" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1">questions_to_embed <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span></span>
<span id="cb98-2">short_corpus_questions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> corpus_questions[:questions_to_embed]</span>
<span id="cb98-3">short_corpus_questions</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>['',
 'What are the Nostradamus Predictions for the 2017?',
 'Is it expensive to take music lessons?',
 'what are the differences between first world and third world countries? Are there any second world countries?',
 'How much is a 1963 2 dollar bill with a red seal worth?',
 'What is the capital of Finland?',
 'Which is the best project management app for accounting companies?',
 "What is Dire Straits' best album ever?",
 'How does Weapon Silencers work?',
 'How should we study in medical school?']</code></pre>
</div>
</div>
<div id="e9eb9fd0" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb100" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SentenceTransformer(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"quora-distilbert-multilingual"</span>)</span>
<span id="cb100-2">embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.encode(short_corpus_questions, convert_to_tensor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb100-3"></span>
<span id="cb100-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute distance btween all embeddings</span></span>
<span id="cb100-5">start_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb100-6">distances <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> util.pytorch_cos_sim(embeddings, embeddings)</span>
<span id="cb100-7">end_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb100-8"></span>
<span id="cb100-9"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Results (after </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{:.3f}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> seconds):"</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(end_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> start_time))</span>
<span id="cb100-10">distances</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Results (after 0.000 seconds):</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[1.0000, 0.7863, 0.6348, 0.7524, 0.7128, 0.7620, 0.6928, 0.7316, 0.6973,
         0.6602],
        [0.7863, 1.0000, 0.7001, 0.8369, 0.8229, 0.8093, 0.7694, 0.8111, 0.7849,
         0.7157],
        [0.6348, 0.7001, 1.0000, 0.6682, 0.7346, 0.7228, 0.7257, 0.7434, 0.7529,
         0.7616],
        [0.7524, 0.8369, 0.6682, 1.0000, 0.7484, 0.8042, 0.6713, 0.7560, 0.7336,
         0.6901],
        [0.7128, 0.8229, 0.7346, 0.7484, 1.0000, 0.7222, 0.7419, 0.7603, 0.8080,
         0.7145],
        [0.7620, 0.8093, 0.7228, 0.8042, 0.7222, 1.0000, 0.7327, 0.7542, 0.7349,
         0.6992],
        [0.6928, 0.7694, 0.7257, 0.6713, 0.7419, 0.7327, 1.0000, 0.7820, 0.7270,
         0.7513],
        [0.7316, 0.8111, 0.7434, 0.7560, 0.7603, 0.7542, 0.7820, 1.0000, 0.7432,
         0.7151],
        [0.6973, 0.7849, 0.7529, 0.7336, 0.8080, 0.7349, 0.7270, 0.7432, 1.0000,
         0.7243],
        [0.6602, 0.7157, 0.7616, 0.6901, 0.7145, 0.6992, 0.7513, 0.7151, 0.7243,
         1.0000]], device='cuda:0')</code></pre>
</div>
</div>
<p>Awesome! We just computed the distances of 10 embeddings vs 10 embeddings. It was quite fast. Let‚Äôs try now with 1000 queries.</p>
<div id="c947ffb1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb103" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compute_embeddings_slow(questions, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>):</span>
<span id="cb103-2">    embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model.encode(</span>
<span id="cb103-3">        questions[:n], show_progress_bar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, convert_to_tensor<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb103-4">    )</span>
<span id="cb103-5"></span>
<span id="cb103-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute distance btween all embeddings</span></span>
<span id="cb103-7">    start_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb103-8">    distances <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> util.pytorch_cos_sim(embeddings, embeddings)</span>
<span id="cb103-9">    end_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb103-10"></span>
<span id="cb103-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> distances, end_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> start_time</span>
<span id="cb103-12"></span>
<span id="cb103-13"></span>
<span id="cb103-14">_, s <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_embeddings_slow(corpus_questions, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20000</span>)</span>
<span id="cb103-15"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Results (after </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{:.3f}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> seconds):"</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(s))</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"90f929bf62b04c5180c9573320320d4e","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Results (after 0.000 seconds):</code></pre>
</div>
</div>
<p>Ok, that‚Äôs still fast! Let‚Äôs look at some other values</p>
<div id="e27d651f" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb105" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb105-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb105-2"></span>
<span id="cb105-3">n_queries <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10001</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">20001</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30001</span>]  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># If I keep going my computer explodes</span></span>
<span id="cb105-4">times <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb105-5"></span>
<span id="cb105-6"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> n <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> n_queries:</span>
<span id="cb105-7">    _, s <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_embeddings_slow(corpus_questions, n)</span>
<span id="cb105-8">    times.append(s)</span>
<span id="cb105-9">    torch.cuda.empty_cache()  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Clear GPU cache</span></span>
<span id="cb105-10"></span>
<span id="cb105-11">plt.plot(n_queries, times)</span>
<span id="cb105-12">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Number of queries"</span>)</span>
<span id="cb105-13">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Time (seconds)"</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"bc00ef68a4dd45ec822cb736f62f0c6d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"7ca7ff71b91f48aa9c5776803dabc406","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e6de0384bb7e4c5eb497cc1d847f364d","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"1bcf39b41a744a2da167afeab7359ae7","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<pre><code>Text(0, 0.5, 'Time (seconds)')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/index_files/figure-html/cell-61-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The algorithm above has a quadratic runtime, so it won‚Äôt scale up well if we keep increasing the number of queries. For larger collections, we can use the <a href="https://www.sbert.net/examples/applications/paraphrase-mining/README.html">paraphrase mining technique</a>, which is more complex and efficient.</p>
<div id="b28bb006" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb107" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb107-1">start_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb107-2">paraphrases <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> util.paraphrase_mining(</span>
<span id="cb107-3">    model, corpus_questions[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100000</span>], show_progress_bar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb107-4">)</span>
<span id="cb107-5">end_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"207c7cf4d03543b9ab3f5edc276cb7fd","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<div id="0a994549" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb108" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(paraphrases)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>250976</code></pre>
</div>
</div>
<div id="4015fd06" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb110" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1">paraphrases[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>[[0.999999463558197, 18862, 24292],
 [0.9999779462814331, 10915, 61354],
 [0.9999630451202393, 60527, 86890]]</code></pre>
</div>
</div>
<p>The first value is the score, the second is the index of a corpus question, and the third is another index to a corpus question. The score indicates how similar the two questions are.</p>
<p>Nice! We just 1. Computed the embeddings of 100,000 questions 2. Obtained the most similar sentences, and 3. Sorted them</p>
<p>All of this in 20 seconds! Let‚Äôs look at the 5 matches with the highest similariy</p>
<div id="592f7249" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb112" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> score, i, j <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> paraphrases[:<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]:</span>
<span id="cb112-2">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{:.3f}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\t</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> and </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(score, corpus_questions[i], corpus_questions[j]))</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>1.000   How do I  increase traffic on my site? and How do I increase traffic on my site?
1.000   who is the best rapper of all time? and Who is the best rapper of all time?
1.000   How can I become an automobile engineer? and How can I become a automobile engineer?
1.000   I made a plasma vortex at my home, but why doesn't it produce a zapping sound like at time when we see sparks and does the air nearby it ionizes? and I made a plasma vortex at my home, but why doesn't it produce a zapping sound like at time when we see sparks and does the air nearby it, ionizes?
1.000   Why was Cyrus Mistry removed as the chairman of Tata Sons? and Why was Cyrus Mistry removed as the Chairman of Tata Sons?</code></pre>
</div>
</div>
<p>How does this method work? The corpus is divided into smaller chunks, which allows us to manage the memory and compute usage. There are two ways in which the chunking happens:</p>
<ul>
<li><strong>Query Chunk Size:</strong> Determines how many sentences are considered as potential paraphrases. This is the number of sentences that are compared to the query sentence and controlled with <code>query_chunk_size</code> (5000 by default).</li>
<li><strong>Corpus Chunk Size:</strong> Determines how many chunks of the corpus are being compared simultaneously. This is controlled with <code>corpus_chunk_size</code> (100000 by default).</li>
</ul>
<p>For example, with the default parameters, the algorithm processes 5000 sentences at a time, comparing each of these against chunks of 100000 sentences from the rest of the corpus. The algorithm is focused on getting the <strong>top matches</strong> - using <code>top_k</code>, for each sentence in a query chunk, the algorithm just selects the top k matches from the corpus chunk. This means that the algorithm will not find all the matches, but it will find the top matches. This is a good trade-off as we usually don‚Äôt need all the matches, but just the top ones.</p>
<p>Both parameters make the process more efficient as it‚Äôs computationally easier to handle smaller subsets of the data. It also helps use less memory as we don‚Äôt have to load the entire corpus into memory to compute the similarity. Finding the right values for these parameters is a trade-off between speed and accuracy. The larger the values, the more accurate the results, but the slower the algorithm.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>You can use <code>max_pairs</code> to limit the number of pairs returned.</p>
</div>
</div>
<p>Here is some pseudocode of the algorithm:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb114" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Initialize an empty list to store the results</span></span>
<span id="cb114-2">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb114-3"></span>
<span id="cb114-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> query_chunk <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> query_chunks:</span>
<span id="cb114-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> corpus_chunk <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> corpus_chunks:</span>
<span id="cb114-6">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Compute the similarity between the query chunk and the corpus chunk</span></span>
<span id="cb114-7">        similarity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> compute_similarity(query_chunk, corpus_chunk)</span>
<span id="cb114-8">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get the top k matches in the other chunk</span></span>
<span id="cb114-9">        top_k_matches <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> similarity.top_k(top_k)</span>
<span id="cb114-10">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add the top k matches to the results</span></span>
<span id="cb114-11">        results.add(top_k_matches)</span></code></pre></div></div>
</section>
</section>
<section id="selecting-and-evaluating-models" class="level2">
<h2 class="anchored" data-anchor-id="selecting-and-evaluating-models">Selecting and evaluating models</h2>
<p>You should have a pretty good understanding of sentence embeddings and what we can do with them. Today, we used two different models, <code>all-MiniLM-L6-v2</code> and <code>quora-distilbert-multilingual</code>. How do we know which one to use? How do we know if a model is good or not?</p>
<p>The first step is to know where to discover sentence embedding models. If you‚Äôre using open-source ones, the Hugging Face Hub allows you to <a href="https://huggingface.co/models?library=sentence-transformers">filter for them</a>. The community has shared over 4000 models! Although looking at the trending models on Hugging Face is a good indicator (e.g., I can see the Microsoft Multilingual 5 Large model, a decent one), we need more information to pick a model.</p>
<p><a href="https://huggingface.co/spaces/mteb/leaderboard">MTEB</a> has us covered. This leaderboard contains multiple evaluation datasets for various tasks. Let‚Äôs quickly look at some criteria we‚Äôre interested in when picking a model.</p>
<ul>
<li><strong>Sequence length.</strong> As discussed before, you might need to encode longer sequences depending on the expected user inputs. For example, if you‚Äôre encoding long documents, you might need to use a model with a larger sequence length. Another alternative is to split the document into multiple sentences and encode each sentence separately.</li>
<li><strong>Language.</strong> The leaderboard contains mostly English or multilingual models, but you can also find models for other languages such as Chinese, Polish, Danish, Swedish, German, etc.</li>
<li><strong>Embedding dimension.</strong> As discussed before, the larger the embedding dimension, the more information the embedding can capture. However, larger embeddings are more expensive to compute and store.</li>
<li><strong>Average metrics across tasks.</strong> The leaderboard contains multiple tasks, such as clustering, re-ranking, and retrieval. You can look at the average performance across all tasks to get a sense of how good the model is.</li>
<li><strong>Task-specific metrics.</strong> You can also look at the model‚Äôs performance in specific tasks. For example, if you‚Äôre interested in clustering, you can look at the model‚Äôs performance in the clustering task.</li>
</ul>
<p>Knowing the purpose of the model is also essential. Some models will be generalist models. Others, such as <a href="https://huggingface.co/allenai/specter2">Specter 2</a>, are focused on specific tasks, such as scientific papers. I won‚Äôt dive too much into all the tasks in the leaderboard, but you can look at the <a href="https://arxiv.org/abs/2210.07316">MTEB paper</a> for more information. Let me give a brief summary of MTEB.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://arxiv.org/abs/2210.07316"><img src="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/mteb.png" class="img-fluid figure-img"></a></p>
<figcaption>MTEB tasks image from the paper</figcaption>
</figure>
</div>
<p>MTEB provides a benchmark of 56 datasets across eight tasks and contains 112 languages. It‚Äôs easily extensible to add your datasets and models to the leaderboard. Overall, it‚Äôs a straightforward tool to find the suitable speed-accuracy trade-off for your use case.</p>
<p>Today‚Äôs (Jan 7th, 2024) top model is a large model, E5-Mistral-7B-instruct, which is 14.22Gb in size and an average of 66.63 over the 56 datasets. One of the next best open-source models is BGE-Large-en-v1.5, which is just 1.34Gb and performs an average of 64.23. And the base model for BGE, which is even smaller (0.44Gb), has a quality of 63.55! As a comparison, text-embedding-ada-002, even if it provides larger embeddings of 1536 dimensions, performs with a quality of 60.99. That‚Äôs number 23 in the MTEB benchmark! Cohere provides better embeddings, with a quality of 64.47 and embeddings of 1024 dimensions.</p>
<p>I recommend looking at this <a href="https://twitter.com/Nils_Reimers/status/1487014195568775173">Twitter thread from 2022</a>, in which OpenAI embeddings were compared against other embeddings. The results are quite interesting! The costs were many orders of magnitude higher, and the quality was considerably lower than smaller models.</p>
<p><strong>All of this said, don‚Äôt overfixate on a single number. You should always look at the specific metrics of your task and the particular resource and speed requirements</strong></p>
<p>It‚Äôs interesting to look at the different tasks covered in MTEB to understand potential sentence embedding applications better.</p>
<ul>
<li><strong>Bitext Mining.</strong> This task involves finding the most similar sentences in two sets of sentences, each in a different language. It is essential for machine translation and cross-lingual search.</li>
<li><strong>Classification.</strong> In this application, a logistic regression classifier is trained using sentence embeddings for text classification tasks.</li>
<li><strong>Clustering.</strong> Here, a k-means model is trained on sentence embeddings to group similar sentences together, useful in unsupervised learning tasks.</li>
<li><strong>Pair Classification.</strong> This task entails predicting whether a pair of sentences are similar, such as determining if they are duplicates or paraphrases, aiding in paraphrase detection.</li>
<li><strong>Re-ranking.</strong> In this scenario, a list of reference texts is re-ranked based on their similarity to a query sentence, improving search and recommendation systems.</li>
<li><strong>Retrieval.</strong> This application involves embedding queries and associated documents to find the most similar documents to a given query, crucial in search-related tasks.</li>
<li><strong>Semantic Similarity.</strong> This task focuses on determining the similarity between a pair of sentences, outputting a continuous similarity score, useful in paraphrase detection and related tasks.</li>
<li><strong>Summarization.</strong> This involves scoring a set of summaries by computing the similarity between them and a reference (human-written) summary, important in summarization evaluation.</li>
</ul>
</section>
<section id="showcase-application-real-time-embeddings-in-your-browser" class="level2">
<h2 class="anchored" data-anchor-id="showcase-application-real-time-embeddings-in-your-browser">Showcase Application: Real-time Embeddings in your browser</h2>
<p>We won‚Äôt do the hands-on for this one, but I wanted to show you a cool application of embeddings. Lee Butterman built a <a href="https://leebutterman.com/wikipedia-search-by-vibes/">cool app</a> where users can search among millions of Wikipedia articles by using embeddings. <strong>What is extra nice here is that this is offline: the embeddings are stored in the browser and the model is running directly in your browser as well - nothing is being sent to a server!</strong> ü§Ø</p>
<p>Preparing the data</p>
<ul>
<li>We first pre-compute an embedding database. The author used a small yet effective model, all-minilm-l6-v2.</li>
<li>The database of 6 million pages * 384 dimensions * 4 bytes per float = 9.2 GB. This is quite large to have users download that.</li>
<li>The author used a technique called <a href="https://en.wikipedia.org/wiki/Vector_quantization">product quantization</a> to reduce the size of the database.</li>
<li>The data is then exported to a format called Arrow, which is very compact!</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Do not worry too much about the specifics here. Our main goal is to understand the high-level idea of this project; so don‚Äôt be scared if this is the first time you hear the word ‚Äúquantization‚Äù!</p>
</div>
</div>
<p>At inference time</p>
<ul>
<li>Lee used <a href="https://github.com/xenova/transformers.js">transformers.js</a>, a library that allows to run transformers models in the browser with JavaScript. This requires having quantized models. Here is an example</li>
</ul>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb115" style="background: #f1f3f5;"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript"><span id="cb115-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> extractor <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">await</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pipeline</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'feature-extraction'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Xenova/all-MiniLM-L6-v2'</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb115-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">const</span> output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">await</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">extractor</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'This is a simple test.'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> { <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">pooling</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'mean'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">normalize</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">true</span> })<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb115-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// Tensor {</span></span>
<span id="cb115-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//   type: 'float32',</span></span>
<span id="cb115-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//   data: Float32Array [0.09094982594251633, -0.014774246141314507, ...],</span></span>
<span id="cb115-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//   dims: [1, 384]</span></span>
<span id="cb115-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">// }</span></span></code></pre></div></div>
<ul>
<li><code>transformers.js</code> downloads the all-MiniLM-L6-v2 model to the browser and is used to compute the embeddings in the browser.</li>
<li>The distance is then computed using <a href="https://github.com/lsb/pq.js">pq.js</a>.</li>
</ul>
<p>Read more about this project in <a href="https://www.leebutterman.com/2023/06/01/offline-realtime-embedding-search.html">Lee‚Äôs blog post</a>.This is a great example of how embeddings can be used in the browser!</p>
</section>
<section id="the-state-of-the-ecosystem" class="level2">
<h2 class="anchored" data-anchor-id="the-state-of-the-ecosystem">The State of the Ecosystem</h2>
<p>The ecosystem around embeddings is quite large.</p>
<section id="building-on-top-of-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="building-on-top-of-embeddings">Building on top of embeddings:</h3>
<ul>
<li>There are cool tools such as <code>top2vec</code> and <code>bertopic</code> designed for buildimg topic embeddings.</li>
<li><code>keybert</code> is a library that allows extracting keywords and keyphrases similar to a document using BERT embeddings.</li>
<li><code>setfit</code> is a library that allows doing efficient few-shot fine-tuning of Sentence Transformers to use them for text classification.</li>
</ul>
</section>
<section id="embedding-databases" class="level3">
<h3 class="anchored" data-anchor-id="embedding-databases">Embedding databases</h3>
<p>2023 has been the year of embedding databases. <a href="https://integrations.langchain.com/vectorstores">LangChain Integrations Section</a> show 65 vector stores. From Weaviate, Pinecone, and Chroma to Redis, ElasticSearch, and Postgres. Embedding databases are specialized to accelerate similarity search on embeddings, usually using approximate search algorithms. The new wave of embedding database startups has lead to a big amount of money being invested in it. At the same time, classical existing database companies have integrated vector indexes into their products, such as Cassandra and MongoDB.</p>
</section>
<section id="research" class="level3">
<h3 class="anchored" data-anchor-id="research">Research</h3>
<p>The research around embeddings is also quite active. If you follow the MTEB benchmark, it changes every few weeks. Some of the players in this are are Microsoft (E5 models), Cohere, BAAI (BGE), Alibaba (GTE), NLP Group of The University of Hong Kong (Instructor), and Jina, among many others.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>What a journey! We just went from 0 to 1 in sentence embeddings. We learned about what they are, how to compute them, how to compare them, and how to scale them. We also saw some cool applications of embeddings, such as semantic search and paraphrase mining. I hope this blog post gave you a good understanding of what sentence embeddings are and how to use them. This is the first part of a series. What‚Äôs left to learn?</p>
<ul>
<li>The role of vector databases</li>
<li>How to use embeddings for more complex ranking systems</li>
<li>Topic modeling</li>
<li>Multimodality</li>
<li>How to train your own embedding models</li>
<li>All about RAGs</li>
</ul>
<p>There will be a time for each of those! For now, I suggest to take a break to check your knowledge. Don‚Äôt hesitate to change the code and play with it! If you like this blog post, don‚Äôt hesitate to <a href="https://github.com/osanseviero/hackerllama">leave a GitHub Star</a> or share it!</p>
</section>
<section id="knowledge-check" class="level2">
<h2 class="anchored" data-anchor-id="knowledge-check">Knowledge Check</h2>
<ol type="1">
<li>What make transformer models more useful than GloVe or Word2Vec for computing embeddings?</li>
<li>What is the role of the <code>[CLS]</code> token in BERT and how does it help for computing sentence embeddings?</li>
<li>What‚Äôs the difference between <code>pooler_output</code> and the <code>[CLS]</code> token embedding?</li>
<li>What‚Äôs the difference between <code>[CLS]</code> pooling, max pooling, and mean pooling?</li>
<li>What is the sequence length limitation of transformer models and how can we work around it?</li>
<li>When do we need to normalize the embeddings?</li>
<li>Which two vectors would give a cosine similarity of -1? What about 0?</li>
<li>Explain the different parameters of the <code>paraphrase_mining</code> function.</li>
<li>How would you choose the best model for your use case?</li>
</ol>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p>Here are some useful resources:</p>
<ul>
<li><a href="https://www.sbert.net/">Sentence Transformers</a></li>
<li><a href="https://huggingface.co/models?library=sentence-transformers">Hugging Face Hub</a></li>
<li><a href="https://huggingface.co/blog/mteb">MTEB Leaderboard</a></li>
</ul>


</section>

 ]]></description>
  <guid>https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/</guid>
  <pubDate>Sun, 07 Jan 2024 00:00:00 GMT</pubDate>
  <media:content url="https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/embedding.png" medium="image" type="image/png" height="67" width="144"/>
</item>
<item>
  <title>The Random Transformer</title>
  <link>https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/</link>
  <description><![CDATA[ 





<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p>In this blog post, we‚Äôll do an end-to-end example of the math within a transformer model. The goal is to get a good understanding of how the model works. To make this manageable, we‚Äôll do lots of simplification. As we‚Äôll be doing quite a bit of the math by hand, we‚Äôll reduce the dimensions of the model. For example, rather than using embeddings of 512 values, we‚Äôll use embeddings of 4 values. This will make the math easier to follow! We‚Äôll use random vectors and matrices, but you can use your own values if you want to follow along.</p>
<p>As you‚Äôll see, the math is not that complicated. The complexity comes from the number of steps and the number of parameters. <strong>I recommend you to read the <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> blog before reading this blog post (or reading in parallel)</strong>. It‚Äôs a great blog post that explains the transformer model in a very intuitive (and illustrative!) way and I don‚Äôt intend to explain what it‚Äôs already explained there. My goal is to explain the ‚Äúhow‚Äù of the transformer model, not the ‚Äúwhat‚Äù. If you want to dive even deeper, check out the famous original paper: <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>.</p>
<p><strong>Prerequisites</strong></p>
<p>A basic understanding of linear algebra is required - we‚Äôll mostly do simple matrix multiplications, so no need to be an expert. Apart from that, basic understanding of Machine Learning and Deep Learning will be useful.</p>
<p><strong>What is covered here?</strong></p>
<ul>
<li>An end-to-end example of the math within a transformer model during inference</li>
<li>An explanation of attention mechanisms</li>
<li>An explanation of residual connections and layer normalization</li>
<li>Some code to scale it up!</li>
</ul>
<p>Without further ado, let‚Äôs get started! Our goal will be to use the transformer model as a translation tool, so we‚Äôll pass an input to the model expecting it to generate the translation. For example, we could pass ‚ÄúHello World‚Äù in English and expect ‚ÄúHola Mundo‚Äù in Spanish.</p>
<p>Let‚Äôs take a look at the diagram of the transformer beast (don‚Äôt be intimidatd by it, you‚Äôll soon understand it!):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://arxiv.org/abs/1706.03762"><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png" class="img-fluid figure-img"></a></p>
<figcaption>Transformer model from the original ‚Äúattention is all you need‚Äù paper</figcaption>
</figure>
</div>
<p>The original transformer model has two parts: encoder and decoder. The encoder focus is in ‚Äúunderstanding‚Äù or ‚Äúcapturing the meaning‚Äù of the input text, while the decoder focus is in generating the output text. We‚Äôll first focus on the encoder part.</p>
<section id="encoder" class="level2">
<h2 class="anchored" data-anchor-id="encoder">Encoder</h2>
<p>The whole goal of the encoder is to generate a rich embedding representation of the input text. This embedding will capture semantic information about the input, and will then be passed to the decoder to generate the output text. The encoder is composed of a stack of N layers. Before we jump into the layers, we need to see how to pass the words (or tokens) into the model.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Embeddings are a somewhat overused term. We‚Äôll first create an embedding that will be the input to the encoder. The encoder also outputs an embedding (also called hidden states sometimes). The decoder will also receive an embedding! üòÖ The whole point of an embedding is to represent a token as a vector.</p>
</div>
</div>
<section id="tokenization" class="level3">
<h3 class="anchored" data-anchor-id="tokenization">0. Tokenization</h3>
<p>ML models can process numbers, not text. soo we need to turn our input text into numbers. That‚Äôs what <strong>tokenization</strong> does! This is the process of splitting the input text into tokens, each with an associated ID. For example, we could split the text ‚ÄúHello World‚Äù into two tokens: ‚ÄúHello‚Äù and ‚ÄúWorld‚Äù. We could also split it into characters: ‚ÄúH‚Äù, ‚Äúe‚Äù, ‚Äúl‚Äù, ‚Äúl‚Äù, ‚Äúo‚Äù, ‚Äù ‚Äú,‚ÄùW‚Äù, ‚Äúo‚Äù, ‚Äúr‚Äù, ‚Äúl‚Äù, ‚Äúd‚Äù. The choice of tokenization is up to us and depends on the data we‚Äôre working with.</p>
<p>Word-based tokenization (splitting the text into words) will require a very large <strong>vocabulary</strong> (all possible tokens). It will also represent words like ‚Äúdog‚Äù and ‚Äúdogs‚Äù or ‚Äúrun‚Äù and ‚Äúrunning‚Äù as different tokens. Character-based vocabulary will require a smaller vocabulary, but will provide less meaning (in can be useful for languages such as Chinese where each character carries more information).</p>
<p>The field has moved towards subword tokenization. This is a middle ground between word-based and character-based tokenization. We‚Äôll split the words into subwords. For example, we could split ‚Äútokenization‚Äù into ‚Äútoken‚Äù and ‚Äúization‚Äù. How do we decide how to split the words? This is part of training a tokenizer through a statistical process that tries to identify which subwords are the best to pick given a dataset. It‚Äôs a deterministic process (unlike training a ML model).</p>
<p>For this blog post, let‚Äôs go with word tokenization for simplicity. Our goal will be to translate ‚ÄúHello World‚Äù from English to Spanish. Given an example ‚ÄúHello World‚Äù, we‚Äôll split into tokens: ‚ÄúHello‚Äù and ‚ÄúWorld‚Äù. Each token has an associated ID defined in the model‚Äôs vocabulary. For example, ‚ÄúHello‚Äù could be token 1 and ‚ÄúWorld‚Äù could be token 2.</p>
</section>
<section id="embedding-the-text" class="level3">
<h3 class="anchored" data-anchor-id="embedding-the-text">1. Embedding the text</h3>
<p>Although we could pass the token IDs to the model (e.g.&nbsp;1 and 2), these numbers don‚Äôt carry any meaning. We need to turn them into vectors (list of numbers). This is what <strong>embedding</strong> does! The token embeddings map a token ID to a fixed-size vector with some <strong>semantic meaning</strong> of the tokens**. This brings some interesting properties: similar tokens will have a similar embedding (in other words, calculating the cosine similarity between two embeddings will give us a good idea of how similar the tokens are).</p>
<p>Note that the mapping from a token to an embedding is learned. Although we could use a pre-trained embedding such as word2vec or GloVe, transformers models learn these embeddings as part of their training. This is a big advantage as the model can learn the best representation of the tokens for the task at hand. For example, the model could learn that ‚Äúdog‚Äù and ‚Äúdogs‚Äù should have similar embeddings.</p>
<p>All embeddings in a single model have the same size. The original transformer used a size of 512, but let‚Äôs do 4 for our example so we can keep the maths manageable. I‚Äôll assign some random values to each token (as mentioned, this mapping is usually learned by the model).</p>
<p>Hello -&gt; [1,2,3,4]</p>
<p>World -&gt; [2,3,4,5]</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>After releasing this blog post, multiple persons raised questions about the embeddings above. I was a bit lazy and just wrote down some numbers that will make for some nice math below. In practice, these numbers would be learned by the model. I‚Äôve updated the blog post to make this clearer. Thanks to everyone who raised this question!</p>
<p>We can estimate how similar these vectors are using cosine similarity, which would be too high for the vectors above. In practice, a vector would likely look something like [-0.071, 0.344, -0.12, 0.026, ‚Ä¶, -0.008].</p>
</div>
</div>
<p>We can represent our input as a single matrix</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%202%20&amp;%203%20&amp;%204%20%5C%5C%0A2%20&amp;%203%20&amp;%204%20&amp;%205%0A%5Cend%7Bbmatrix%7D%0A"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Although we could manage the two embeddings as separate vectors, it‚Äôs easier to manage them as a single matrix. This is because we‚Äôll be doing matrix multiplications as we move forward!</p>
</div>
</div>
</section>
<section id="positional-encoding" class="level3">
<h3 class="anchored" data-anchor-id="positional-encoding">2 Positional encoding</h3>
<p>The individual embeddings in the matrix contain no information about the position of the words in the sentence‚Äù, so we need to feed some positional information. The way we do this is by adding a positional encoding to the embedding.</p>
<p>There are different choices on how to obtain these - we could use a learned embedding or a fixed vector. The original paper uses a fixed vector as they see almost no difference between the two approaches (see section 3.5 of the original paper). We‚Äôll use a fixed vector as well. Sine and cosine functions have a wave-like pattern, and they repeat over time. By using these functions, <strong>each position in the sentence gets a unique</strong> yet consistent positional encoding. Given they repeat over time, it can help the model more easily learn patterns like proximity and distance between elements. These are the functions they use in the paper (section 3.5):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0APE(pos,%202i)%20=%20%5Csin%5Cleft(%5Cfrac%7Bpos%7D%7B10000%5E%7B2i/d_%7B%5Ctext%7Bmodel%7D%7D%7D%7D%5Cright)%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0APE(pos,%202i+1)%20=%20%5Ccos%5Cleft(%5Cfrac%7Bpos%7D%7B10000%5E%7B2i/d_%7B%5Ctext%7Bmodel%7D%7D%7D%7D%5Cright)%0A"></p>
<p>The idea is to interpolate between sine and cosine for each value in the embedding (even indices will use sine, odd indices will use cosine). Let‚Äôs calculate them for our example!</p>
<p>For ‚ÄúHello‚Äù</p>
<ul>
<li>i = 0 (even): PE(0,0) = sin(0 / 10000^(0 / 4)) = sin(0) = 0</li>
<li>i = 1 (odd): PE(0,1) = cos(0 / 10000^(2*1 / 4)) = cos(0) = 1</li>
<li>i = 2 (even): PE(0,2) = sin(0 / 10000^(2*2 / 4)) = sin(0) = 0</li>
<li>i = 3 (odd): PE(0,3) = cos(0 / 10000^(2*3 / 4)) = cos(0) = 1</li>
</ul>
<p>For ‚ÄúWorld‚Äù</p>
<ul>
<li>i = 0 (even): PE(1,0) = sin(1 / 10000^(0 / 4)) = sin(1 / 10000^0) = sin(1) ‚âà 0.84</li>
<li>i = 1 (odd): PE(1,1) = cos(1 / 10000^(2*1 / 4)) = cos(1 / 10000^0.5) ‚âà cos(0.01) ‚âà 0.99</li>
<li>i = 2 (even): PE(1,2) = sin(1 / 10000^(2*2 / 4)) = sin(1 / 10000^1) ‚âà 0</li>
<li>i = 3 (odd): PE(1,3) = cos(1 / 10000^(2*3 / 4)) = cos(1 / 10000^1.5) ‚âà 1</li>
</ul>
<p>So concluding</p>
<ul>
<li>‚ÄúHello‚Äù -&gt; [0, 1, 0, 1]</li>
<li>‚ÄúWorld‚Äù -&gt; [0.84, 0.99, 0, 1]</li>
</ul>
<p>Note that these encodings have the same dimension as the original embedding.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>While we use sine and cosine as the original paper, there are other ways to do this. BERT, a very popular transformer, use trainable positional embeddings.</p>
</div>
</div>
</section>
<section id="add-positional-encoding-and-embedding" class="level3">
<h3 class="anchored" data-anchor-id="add-positional-encoding-and-embedding">3. Add positional encoding and embedding</h3>
<p>We now add the positional encoding to the embedding. This is done by adding the two vectors together.</p>
<p>‚ÄúHello‚Äù = [1,2,3,4] + [0, 1, 0, 1] = [1, 3, 3, 5] ‚ÄúWorld‚Äù = [2,3,4,5] + [0.84, 0.99, 0, 1] = [2.84, 3.99, 4, 6]</p>
<p>So our new matrix, which will be the input to the encoder, is:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%203%20&amp;%203%20&amp;%205%20%5C%5C%0A2.84%20&amp;%203.99%20&amp;%204%20&amp;%206%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>If you look at the original paper‚Äôs image, what we just did is the bottom left part of the image (the embedding + positional encoding).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://arxiv.org/abs/1706.03762"><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png" class="img-fluid figure-img"></a></p>
<figcaption>Transformer model from the original ‚Äúattention is all you need‚Äù paper</figcaption>
</figure>
</div>
</section>
<section id="self-attention" class="level3">
<h3 class="anchored" data-anchor-id="self-attention">4. Self-attention</h3>
<section id="matrices-definition" class="level4">
<h4 class="anchored" data-anchor-id="matrices-definition">4.1 Matrices Definition</h4>
<p>We‚Äôll now introduce the concept of multi-head attention. Attention is a mechanism that allows the model to focus on certain parts of the input. Multi-head attention is a way to allow the model to jointly attend to information from different representation subspaces. This is done by using multiple attention heads. Each attention head will have its own K, V, and Q matrices.</p>
<p>Let‚Äôs use 2 attention heads for our example. We‚Äôll use random values for these matrices. Each matrix will be a 4x3 matrix. With this, each matrix will transform the 4-dimensional embeddings into 3-dimensional keys, values, and queries. This reduces the dimensionality for attention mechanism, which helps in managing the computational complexity. Note that using a too small attention size will hurt the performance of the model. Let‚Äôs use the following values (just random values):</p>
<p><strong>For the first head</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AWK1%20&amp;=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20&amp;%201%20%5C%5C%0A0%20&amp;%201%20&amp;%200%20%5C%5C%0A1%20&amp;%200%20&amp;%201%20%5C%5C%0A0%20&amp;%201%20&amp;%200%0A%5Cend%7Bbmatrix%7D,%20%5Cquad%0AWV1%20&amp;=%20%5Cbegin%7Bbmatrix%7D%0A0%20&amp;%201%20&amp;%201%20%5C%5C%0A1%20%20&amp;%200%20&amp;%200%20%5C%5C%0A1%20&amp;%200%20&amp;%201%20%5C%5C%0A0%20&amp;%201%20&amp;%200%0A%5Cend%7Bbmatrix%7D,%20%5Cquad%0AWQ1%20&amp;=%20%5Cbegin%7Bbmatrix%7D%0A0%20&amp;%200%20&amp;%200%20%5C%5C%0A1%20&amp;%201%20&amp;%200%20%5C%5C%0A0%20&amp;%200%20&amp;%201%20%5C%5C%0A1%20&amp;%201%20&amp;%200%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p><strong>For the second head</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AWK2%20&amp;=%20%5Cbegin%7Bbmatrix%7D%0A0%20&amp;%201%20&amp;%201%20%5C%5C%0A1%20&amp;%200%20&amp;%201%20%5C%5C%0A1%20&amp;%200%20&amp;%201%20%5C%5C%0A0%20&amp;%201%20&amp;%200%0A%5Cend%7Bbmatrix%7D,%20%5Cquad%0AWV2%20&amp;=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20&amp;%200%20%5C%5C%0A0%20&amp;%201%20&amp;%201%20%5C%5C%0A0%20&amp;%200%20&amp;%201%20%5C%5C%0A1%20&amp;%200%20&amp;%200%0A%5Cend%7Bbmatrix%7D,%20%5Cquad%0AWQ2%20&amp;=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20&amp;%201%20%5C%5C%0A0%20&amp;%201%20&amp;%200%20%5C%5C%0A1%20&amp;%200%20&amp;%200%20%5C%5C%0A0%20&amp;%201%20&amp;%201%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D%0A"></p>
</section>
<section id="keys-queries-and-values-calculation" class="level4">
<h4 class="anchored" data-anchor-id="keys-queries-and-values-calculation">4.2 Keys, queries, and values calculation</h4>
<p>We now need to multiply our input embeddings with the weight matrices to obtain the keys, queries, and values.</p>
<p><strong>Key calculation</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0AE%20%5Ctimes%20WK1%20&amp;=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%203%20&amp;%203%20&amp;%205%20%5C%5C%0A2.84%20&amp;%203.99%20&amp;%204%20&amp;%206%0A%5Cend%7Bbmatrix%7D%0A%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20&amp;%201%20%5C%5C%0A0%20&amp;%201%20&amp;%200%20%5C%5C%0A1%20&amp;%200%20&amp;%201%20%5C%5C%0A0%20&amp;%201%20&amp;%200%0A%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D%0A(1%20%5Ctimes%201)%20+%20(3%20%5Ctimes%200)%20+%20(3%20%5Ctimes%201)%20+%20(5%20%5Ctimes%200)%20&amp;%20(1%20%5Ctimes%200)%20+%20(3%20%5Ctimes%201)%20+%20(3%20%5Ctimes%200)%20+%20(5%20%5Ctimes%201)%20&amp;%20(1%20%5Ctimes%201)%20+%20(3%20%5Ctimes%200)%20+%20(3%20%5Ctimes%201)%20+%20(5%20%5Ctimes%200)%20%5C%5C%0A(2.84%20%5Ctimes%201)%20+%20(3.99%20%5Ctimes%200)%20+%20(4%20%5Ctimes%201)%20+%20(6%20%5Ctimes%200)%20&amp;%20(2.84%20%5Ctimes%200)%20+%20(4%20%5Ctimes%201)%20+%20(4%20%5Ctimes%200)%20+%20(6%20%5Ctimes%201)%20&amp;%20(2.84%20%5Ctimes%201)%20+%20(4%20%5Ctimes%200)%20+%20(4%20%5Ctimes%201)%20+%20(6%20%5Ctimes%200)%0A%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D%0A4%20&amp;%208%20&amp;%204%20%5C%5C%0A6.84%20&amp;%209.99%20&amp;%206.84%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>Ok, I actually do not want to do the math by hand for all of these - it gets a bit repetitive plus it breaks the site. So let‚Äôs cheat and use NumPy to do the calculations for us.</p>
<p>We first define the matrices</p>
<div id="2d86c87b" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"></span>
<span id="cb1-3">WK1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])</span>
<span id="cb1-4">WV1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])</span>
<span id="cb1-5">WQ1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])</span>
<span id="cb1-6"></span>
<span id="cb1-7">WK2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])</span>
<span id="cb1-8">WV2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]])</span>
<span id="cb1-9">WQ2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]])</span></code></pre></div></div>
</div>
<p>And let‚Äôs confirm that I didn‚Äôt make any mistakes in the calculations above.</p>
<div id="4df0e85b" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>], [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.84</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">3.99</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>]])</span>
<span id="cb2-2">K1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WK1</span>
<span id="cb2-3">K1</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[4.  , 8.  , 4.  ],
       [6.84, 9.99, 6.84]])</code></pre>
</div>
</div>
<p>Phew! Let‚Äôs now get the values and queries</p>
<p><strong>Value calculations</strong></p>
<div id="51507ce9" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">V1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WV1</span>
<span id="cb4-2">V1</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[6.  , 6.  , 4.  ],
       [7.99, 8.84, 6.84]])</code></pre>
</div>
</div>
<p><strong>Query calculations</strong></p>
<div id="9f4e95bc" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">Q1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WQ1</span>
<span id="cb6-2">Q1</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[8.  , 3.  , 3.  ],
       [9.99, 3.99, 4.  ]])</code></pre>
</div>
</div>
<p>Let‚Äôs skip the second head for now and focus on the first head final score. We‚Äôll come back to the second head later.</p>
</section>
<section id="attention-calculation" class="level4">
<h4 class="anchored" data-anchor-id="attention-calculation">4.3 Attention calculation</h4>
<p>Calculating the attention score requires a couple of steps:</p>
<ol type="1">
<li>Calculate the dot product of the query with each key</li>
<li>Divide the result by the square root of the dimension of the key vector</li>
<li>Apply a softmax function to obtain the attention weights</li>
<li>Multiply each value vector by the attention weights</li>
</ol>
<section id="dot-product-of-query-with-each-key" class="level5">
<h5 class="anchored" data-anchor-id="dot-product-of-query-with-each-key">4.3.1 Dot product of query with each key</h5>
<p>The score for ‚ÄúHello‚Äù requires calculating the dot product of q1 with each key vector (k1 and k2)</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0Aq1%20%5Ccdot%20k1%20&amp;=%20%5Cbegin%7Bbmatrix%7D%208%20&amp;%203%20&amp;%203%20%5Cend%7Bbmatrix%7D%20%5Ccdot%20%5Cbegin%7Bbmatrix%7D%204%20%5C%5C%208%20%5C%5C%204%20%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%208%20%5Ccdot%204%20+%203%20%5Ccdot%208%20+%203%20%5Ccdot%204%20%5C%5C%0A&amp;=%2068%0A%5Cend%7Balign*%7D%0A"></p>
<p>In matrix world, that would be Q1 multiplied by the transpose of K1</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0AQ1%20%5Ctimes%20K1%5E%5Ctop%20&amp;=%20%5Cbegin%7Bbmatrix%7D%208%20&amp;%203%20&amp;%203%20%5C%5C%209.99%20&amp;%203.99%20&amp;%204%20%5Cend%7Bbmatrix%7D%20%5Ctimes%20%5Cbegin%7Bbmatrix%7D%204%20&amp;%206.84%20%5C%5C%208%20&amp;%209.99%20%5C%5C%204%20&amp;%206.84%20%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%208%20%5Ccdot%204%20+%203%20%5Ccdot%208%20+%203%20%5Ccdot%204%20&amp;%208%20%5Ccdot%206.84%20+%203%20%5Ccdot%209.99%20+%203%20%5Ccdot%206.84%20%5C%5C%0A%20%20%20%209.99%20%5Ccdot%204%20+%203.99%20%5Ccdot%208%20+%204%20%5Ccdot%204%20&amp;%209.99%20%5Ccdot%206.84%20+%203.99%20%5Ccdot%209.99%20+%204%20%5Ccdot%206.84%0A%20%20%20%20%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D%0A%20%20%20%2068%20&amp;%20105.21%20%5C%5C%0A%20%20%20%2087.88%20&amp;%20135.5517%0A%20%20%20%20%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D"></p>
<p>I‚Äôm prone to do mistakes, so let‚Äôs confirm with Python once again</p>
<div id="a32f968f" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">scores1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> K1.T</span>
<span id="cb8-2">scores1</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[ 68.    , 105.21  ],
       [ 87.88  , 135.5517]])</code></pre>
</div>
</div>
</section>
<section id="divide-by-square-root-of-dimension-of-key-vector" class="level5">
<h5 class="anchored" data-anchor-id="divide-by-square-root-of-dimension-of-key-vector">4.3.2 Divide by square root of dimension of key vector</h5>
<p>We then divide the scores by the square root of the dimension (d) of the keys (3 in this case, but 64 in the original paper). Why? For large values of d, the dot product grows too large (we‚Äôre adding the multiplication of a bunch of numbers, after all, leading to high values). And large values are bad! We‚Äôll discuss soon more about this.</p>
<div id="1122634f" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">scores1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scores1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.sqrt(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb10-2">scores1</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[39.2598183 , 60.74302182],
       [50.73754166, 78.26081048]])</code></pre>
</div>
</div>
</section>
<section id="apply-softmax-function" class="level5">
<h5 class="anchored" data-anchor-id="apply-softmax-function">4.3.3 Apply softmax function</h5>
<p>We then softmax to normalize so they are all positive and add up to 1.</p>
<div class="callout callout-style-default callout-note callout-titled" title="What is softmax?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>What is softmax?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Softmax is a function that takes a vector of values and returns a vector of values between 0 and 1, where the sum of the values is 1. It‚Äôs a nice way of obtaining probabilities. It‚Äôs defined as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bsoftmax%7D(x_i)%20=%20%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum_%7Bj=1%7D%5En%20e%5E%7Bx_j%7D%7D%0A"></p>
<p>Don‚Äôt be intimidated by the formula - it‚Äôs actually quite simple. Let‚Äôs say we have the following vector:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ax%20=%20%5Cbegin%7Bbmatrix%7D%201%20&amp;%202%20&amp;%203%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>The softmax of this vector would be:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bsoftmax%7D(x)%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cfrac%7Be%5E1%7D%7Be%5E1%20+%20e%5E2%20+%20e%5E3%7D%20&amp;%20%5Cfrac%7Be%5E2%7D%7Be%5E1%20+%20e%5E2%20+%20e%5E3%7D%20&amp;%20%5Cfrac%7Be%5E3%7D%7Be%5E1%20+%20e%5E2%20+%20e%5E3%7D%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%200.09%20&amp;%200.24%20&amp;%200.67%20%5Cend%7Bbmatrix%7D%0A"></p>
<p>As you can see, the values are all positive and add up to 1.</p>
</div>
</div>
<div id="a5344a87" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> softmax(x):</span>
<span id="cb12-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.exp(x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(np.exp(x), axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb12-3"></span>
<span id="cb12-4"></span>
<span id="cb12-5">scores1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> softmax(scores1)</span>
<span id="cb12-6">scores1</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[4.67695573e-10, 1.00000000e+00],
       [1.11377182e-12, 1.00000000e+00]])</code></pre>
</div>
</div>
</section>
<section id="multiply-value-matrix-by-attention-weights" class="level5">
<h5 class="anchored" data-anchor-id="multiply-value-matrix-by-attention-weights">4.3.4 Multiply value matrix by attention weights</h5>
<p>We then multiply times the value matrix</p>
<div id="d9e2b597" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">attention1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scores1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> V1</span>
<span id="cb14-2">attention1</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[7.99, 8.84, 6.84],
       [7.99, 8.84, 6.84]])</code></pre>
</div>
</div>
<p>Let‚Äôs combine 4.3.1, 4.3.2, 4.3.3, and 4.3.4 into a single formula using matrices (this is from section 3.2.1 of the original paper):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AAttention(Q,K,V)%20=%20%5Ctext%7Bsoftmax%7D%5Cleft(%5Cfrac%7BQK%5E%5Ctop%7D%7B%5Csqrt%7Bd%7D%7D%5Cright)V%0A"></p>
<p>Yes, that‚Äôs it! All the math we just did can easily be encapsulated in the attention formula above! Let‚Äôs now translate this to code!</p>
<div id="1279f4fe" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> attention(x, WQ, WK, WV):</span>
<span id="cb16-2">    K <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WK</span>
<span id="cb16-3">    V <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WV</span>
<span id="cb16-4">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WQ</span>
<span id="cb16-5"></span>
<span id="cb16-6">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> K.T</span>
<span id="cb16-7">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.sqrt(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb16-8">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> softmax(scores)</span>
<span id="cb16-9">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> V</span>
<span id="cb16-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> scores</span></code></pre></div></div>
</div>
<div id="8fd743f2" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">attention(embedding, WQ1, WK1, WV1)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[7.99, 8.84, 6.84],
       [7.99, 8.84, 6.84]])</code></pre>
</div>
</div>
<p>We confirm we got same values as above. Let‚Äôs chear and use this to obtain the attention scores the second attention head:</p>
<div id="77d1d39e" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">attention2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attention(embedding, WQ2, WK2, WV2)</span>
<span id="cb19-2">attention2</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[8.84, 3.99, 7.99],
       [8.84, 3.99, 7.99]])</code></pre>
</div>
</div>
<p>If you‚Äôre wondering how come the attention is the same for the two embeddings, it‚Äôs because the softmax is taking our scores to 0 and 1. See this:</p>
<div id="86bfce07" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">softmax(((embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WQ2) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> (embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WK2).T) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.sqrt(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>))</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[1.10613872e-14, 1.00000000e+00],
       [4.95934510e-20, 1.00000000e+00]])</code></pre>
</div>
</div>
<p>This is due to bad initialization of the matrices and small vector sizes. Large differences in the scores before applying softmax will just be amplified with softmax, leading to one value being close to 1 and others close to 0. In practice, our initial embedding matrices‚Äô values were maybe too high, leading to high values for the keys, values, and queries, which just grew larger as we multiplied them.</p>
<p>Remember when we were dividing by the square root of the dimension of the keys? This is why we do that. If we don‚Äôt do that, the values of the dot product will be too large, leading to large values after the softmax. In this case, though, it seems it wasn‚Äôt enough given our small values! As a short-term hack, we can scale down the values by a larger amount than the square root of 3. Let‚Äôs redefine the attention function but scaling down by 30. This is not a good long-term solution, but it will help us get different values for the attention scores. We‚Äôll get back to a better solution later.</p>
<div id="5752a1b7" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> attention(x, WQ, WK, WV):</span>
<span id="cb23-2">    K <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WK</span>
<span id="cb23-3">    V <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WV</span>
<span id="cb23-4">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WQ</span>
<span id="cb23-5"></span>
<span id="cb23-6">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> K.T</span>
<span id="cb23-7">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># we just changed this</span></span>
<span id="cb23-8">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> softmax(scores)</span>
<span id="cb23-9">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> V</span>
<span id="cb23-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> scores</span></code></pre></div></div>
</div>
<div id="07a1403b" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1">attention1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attention(embedding, WQ1, WK1, WV1)</span>
<span id="cb24-2">attention1</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[7.54348784, 8.20276657, 6.20276657],
       [7.65266185, 8.35857269, 6.35857269]])</code></pre>
</div>
</div>
<div id="76fcc4bd" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1">attention2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attention(embedding, WQ2, WK2, WV2)</span>
<span id="cb26-2">attention2</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[8.45589591, 3.85610456, 7.72085664],
       [8.63740591, 3.91937741, 7.84804146]])</code></pre>
</div>
</div>
</section>
<section id="heads-attention-output" class="level5">
<h5 class="anchored" data-anchor-id="heads-attention-output">4.3.5 Heads‚Äô attention output</h5>
<p>The next layer of the encoder will expect a single matrix, not two. The first step will be to concatenate the two heads‚Äô outputs (section 3.2.2 of the original paper)</p>
<div id="6e07ccbb" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1">attentions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.concatenate([attention1, attention2], axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb28-2">attentions</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[7.54348784, 8.20276657, 6.20276657, 8.45589591, 3.85610456,
        7.72085664],
       [7.65266185, 8.35857269, 6.35857269, 8.63740591, 3.91937741,
        7.84804146]])</code></pre>
</div>
</div>
<p>We finally multiply this concatenated matrix by a weight matrix to obtain the final output of the attention layer. This weight matrix is also learned! The dimension of the matrix ensures we go back to the same dimension as the embedding (4 in our case).</p>
<div id="953bf008" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Just some random values</span></span>
<span id="cb30-2">W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array(</span>
<span id="cb30-3">    [</span>
<span id="cb30-4">        [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.79445237</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1081456</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.27411536</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.78394531</span>],</span>
<span id="cb30-5">        [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.29081936</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.36187258</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.32312791</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.48530339</span>],</span>
<span id="cb30-6">        [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.36702934</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.76471963</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.88058366</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.73713022</span>],</span>
<span id="cb30-7">        [<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.02305587</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.64315981</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.68306653</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.25393866</span>],</span>
<span id="cb30-8">        [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.29077448</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.04121674</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01509932</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.13149906</span>],</span>
<span id="cb30-9">        [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.57451867</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.08895355</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.02190485</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.24535932</span>],</span>
<span id="cb30-10">    ]</span>
<span id="cb30-11">)</span>
<span id="cb30-12">Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attentions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> W</span>
<span id="cb30-13">Z</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[ 11.46394285, -13.18016471, -11.59340253, -17.04387829],
       [ 11.62608573, -13.47454936, -11.87126395, -17.4926367 ]])</code></pre>
</div>
</div>
<p>The image from <a href="https://jalammar.github.io/illustrated-transformer/">The Ilustrated Transformer</a> encapsulates all of this in a single image <img src="http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" class="img-fluid" alt="Attention"></p>
</section>
</section>
</section>
<section id="feed-forward-layer" class="level3">
<h3 class="anchored" data-anchor-id="feed-forward-layer">5. Feed-forward layer</h3>
<section id="basic-feed-forward-layer" class="level4">
<h4 class="anchored" data-anchor-id="basic-feed-forward-layer">5.1 Basic feed-forward layer</h4>
<p>After the self-attention layer, the encoder has a feed-forward neural network (FFN). This is a simple network with two linear transformations and a ReLU activation in between. The Illustrated Transformer blog post does not dive into it, so let me briefly explain a bit more. The goal of the FFN is to process and transformer the representation produced by the attention mechanism. The flow is usually as follows (see section 3.3 of the original paper):</p>
<ol type="1">
<li><strong>First linear layer:</strong> this usually expands the dimensionality of the input. For example, if the input dimension is 512, the output dimension might be 2048. This is done to allow the model to learn more complex functions. In our simple of example with dimension of 4, we‚Äôll expand to 8.</li>
<li><strong>ReLU activation:</strong> This is a non-linear activation function. It‚Äôs a simple function that returns 0 if the input is negative, and the input if it‚Äôs positive. This allows the model to learn non-linear functions. The math is as follows:</li>
</ol>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BReLU%7D(x)%20=%20%5Cmax(0,%20x)%0A"></p>
<ol start="3" type="1">
<li><strong>Second linear layer:</strong> This is the opposite of the first linear layer. It reduces the dimensionality back to the original dimension. In our example, we‚Äôll reduce from 8 to 4.</li>
</ol>
<p>We can represent all of this as follows</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BFFN%7D(x)%20=%20%5Ctext%7BReLU%7D(xW_1%20+%20b_1)W_2%20+%20b_2%0A"></p>
<p>Just as a reminder, the input for this layer is the Z we calculated in the self-attention above. Here are the values as a reminder</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AZ%20=%0A%5Cbegin%7Bbmatrix%7D%0A11.46394281%20&amp;%20-13.18016469%20&amp;%20-11.59340253%20&amp;%20-17.04387833%20%5C%5C%0A11.62608569%20&amp;%20-13.47454934%20&amp;%20-11.87126395%20&amp;%20-17.49263674%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>Let‚Äôs now define some random values for the weight matrices and bias vectors. I‚Äôll do it with code, but you can do it by hand if you feel patient!</p>
<div id="1fee4bcb" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1">W1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>)</span>
<span id="cb32-2">W2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span>
<span id="cb32-3">b1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>)</span>
<span id="cb32-4">b2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span></code></pre></div></div>
</div>
<p>And now let‚Äôs write the forward pass function</p>
<div id="d001d38c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> relu(x):</span>
<span id="cb33-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.maximum(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, x)</span>
<span id="cb33-3"></span>
<span id="cb33-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> feed_forward(Z, W1, b1, W2, b2):</span>
<span id="cb33-5">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> relu(Z.dot(W1) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b1).dot(W2) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b2</span></code></pre></div></div>
</div>
<div id="55976bde" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1">output_encoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> feed_forward(Z, W1, b1, W2, b2)</span>
<span id="cb34-2">output_encoder</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[ -3.24115016,  -9.7901049 , -29.42555675, -19.93135286],
       [ -3.40199463,  -9.87245924, -30.05715408, -20.05271018]])</code></pre>
</div>
</div>
</section>
<section id="encapsulating-everything-the-random-encoder" class="level4">
<h4 class="anchored" data-anchor-id="encapsulating-everything-the-random-encoder">5.2 Encapsulating everything: The Random Encoder</h4>
<p>Let‚Äôs now write some code to have the multi-head attention and the feed-forward, all together in the encoder block.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The code optimizes for understanding and educational purposes, not for performance! Don‚Äôt judge too hard!</p>
</div>
</div>
<div id="b903fbc3" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1">d_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb36-2">d_key <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> d_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> d_query <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb36-3">d_feed_forward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span></span>
<span id="cb36-4">n_attention_heads <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb36-5"></span>
<span id="cb36-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> attention(x, WQ, WK, WV):</span>
<span id="cb36-7">    K <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WK</span>
<span id="cb36-8">    V <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WV</span>
<span id="cb36-9">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WQ</span>
<span id="cb36-10"></span>
<span id="cb36-11">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> K.T</span>
<span id="cb36-12">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.sqrt(d_key)</span>
<span id="cb36-13">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> softmax(scores)</span>
<span id="cb36-14">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> V</span>
<span id="cb36-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> scores</span>
<span id="cb36-16"></span>
<span id="cb36-17"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> multi_head_attention(x, WQs, WKs, WVs):</span>
<span id="cb36-18">    attentions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.concatenate(</span>
<span id="cb36-19">        [attention(x, WQ, WK, WV) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> WQ, WK, WV <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(WQs, WKs, WVs)], axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb36-20">    )</span>
<span id="cb36-21">    W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(n_attention_heads <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> d_value, d_embedding)</span>
<span id="cb36-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> attentions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> W</span>
<span id="cb36-23"></span>
<span id="cb36-24"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> feed_forward(Z, W1, b1, W2, b2):</span>
<span id="cb36-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> relu(Z.dot(W1) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b1).dot(W2) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b2</span>
<span id="cb36-26"></span>
<span id="cb36-27"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):</span>
<span id="cb36-28">    Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> multi_head_attention(x, WQs, WKs, WVs)</span>
<span id="cb36-29">    Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> feed_forward(Z, W1, b1, W2, b2)</span>
<span id="cb36-30">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> Z</span>
<span id="cb36-31"></span>
<span id="cb36-32"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> random_encoder_block(x):</span>
<span id="cb36-33">    WQs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb36-34">        np.random.randn(d_embedding, d_query) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)</span>
<span id="cb36-35">    ]</span>
<span id="cb36-36">    WKs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb36-37">        np.random.randn(d_embedding, d_key) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)</span>
<span id="cb36-38">    ]</span>
<span id="cb36-39">    WVs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb36-40">        np.random.randn(d_embedding, d_value) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)</span>
<span id="cb36-41">    ]</span>
<span id="cb36-42">    W1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(d_embedding, d_feed_forward)</span>
<span id="cb36-43">    b1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(d_feed_forward)</span>
<span id="cb36-44">    W2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(d_feed_forward, d_embedding)</span>
<span id="cb36-45">    b2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(d_embedding)</span>
<span id="cb36-46">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2)</span></code></pre></div></div>
</div>
<p>Recall that our input is the matrix E which has the positional encoding and the embedding.</p>
<div id="63520e3e" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1">embedding</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[1.  , 3.  , 3.  , 5.  ],
       [2.84, 3.99, 4.  , 6.  ]])</code></pre>
</div>
</div>
<p>Let‚Äôs now pass this to our <code>random_encoder_block</code> function</p>
<div id="11f33901" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">random_encoder_block(embedding)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[ -71.76537515, -131.43316885,   13.2938131 ,   -4.26831998],
       [ -72.04253781, -131.84091347,   13.3385937 ,   -4.32872015]])</code></pre>
</div>
</div>
<p>Nice! This was just one encoder block. The original paper uses 6 encoders. The output of one encoder goes to the next, and so on:</p>
<div id="bcd9d5f2" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> encoder(x, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>):</span>
<span id="cb41-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n):</span>
<span id="cb41-3">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> random_encoder_block(x)</span>
<span id="cb41-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x</span>
<span id="cb41-5"></span>
<span id="cb41-6"></span>
<span id="cb41-7">encoder(embedding)</span></code></pre></div></div>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_11906/1045810361.py:2: RuntimeWarning: overflow encountered in exp
  return np.exp(x)/np.sum(np.exp(x),axis=1, keepdims=True)
/tmp/ipykernel_11906/1045810361.py:2: RuntimeWarning: invalid value encountered in divide
  return np.exp(x)/np.sum(np.exp(x),axis=1, keepdims=True)</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>array([[nan, nan, nan, nan],
       [nan, nan, nan, nan]])</code></pre>
</div>
</div>
</section>
<section id="residual-and-layer-normalization" class="level4">
<h4 class="anchored" data-anchor-id="residual-and-layer-normalization">5.3 Residual and Layer Normalization</h4>
<p>Uh oh! We‚Äôre getting NaNs! It seems our values are too high, and when being passed to the next encoder, they end up being too high and exploding! This issue of having values that are too high is a common issue when training models. For example, when doing the backpropagation (the technique through which the models learn), the gradients can become too large and end up exploding; this is called <strong>gradient explosion</strong>. Without any kind of normalization, small changes in the input of early layers end up being amplified in later layers. This is a common problem in deep neural networks. There are two common techniques to mitigate this problem: residual connections and layer normalization (section 3.1 of the paper, barely mentioned).</p>
<ul>
<li><strong>Residual connections:</strong> Residual connections are simply adding the input of the layer to it output. For example, we add the initial embedding to the output of the attention. Residual connections mitigate the vanishing gradient problem. The intuition is that if the gradient is too small, we can just add the input to the output and the gradient will be larger. The math is very simple:</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BResidual%7D(x)%20=%20x%20+%20%5Ctext%7BLayer%7D(x)%0A"></p>
<p>That‚Äôs it! We‚Äôll do this to the output of the attention and the output of the feed-forward layer.</p>
<ul>
<li><strong>Layer normalization</strong> Layer normalization is a technique to normalize the inputs of a layer. It normalizes across the embedding dimension. The intuition is that we want to normalize the inputs of a layer so that they have a mean of 0 and a standard deviation of 1. This helps with the gradient flow. The math does not look so simple at a first glance.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BLayerNorm%7D(x)%20=%20%5Cfrac%7Bx%20-%20%5Cmu%7D%7B%5Csqrt%7B%5Csigma%5E2%20+%20%5Cepsilon%7D%7D%20%5Ctimes%20%5Cgamma%20+%20%5Cbeta%0A"></p>
<p>Let‚Äôs explain each parameter:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmu"> is the mean of the embedding</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Csigma"> is the standard deviation of the embedding</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> is a small number to avoid division by zero. In case the standard deviation is 0, this small epsilon saves the day!</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cgamma"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> are learned parameters that control scaling and shifting steps.</li>
</ul>
<p>Unlike batch normalization (no worries if you don‚Äôt know what it is), layer normalization normalizes across the embedding dimension - that means that each embedding will not be affected by other samples in the batch. The intuition is that we want to normalize the inputs of a layer so that they have a mean of 0 and a standard deviation of 1.</p>
<p>Why do we add the learnable parameters <img src="https://latex.codecogs.com/png.latex?%5Cgamma"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta">? The reason is that we don‚Äôt want to lose the representational power of the layer. If we just normalize the inputs, we might lose some information. By adding the learnable parameters, we can learn to scale and shift the normalized values.</p>
<p>Combining the equations, the equation for the whole encoder could look like this</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BZ%7D(x)%20=%20%5Ctext%7BLayerNorm%7D(x%20+%20%5Ctext%7BAttention%7D(x))%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BFFN%7D(x)%20=%20%5Ctext%7BReLU%7D(xW_1%20+%20b_1)W_2%20+%20b_2%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BEncoder%7D(x)%20=%20%5Ctext%7BLayerNorm%7D(Z(x)%20+%20%5Ctext%7BFFN%7D(Z(x)%20+%20x))%0A"></p>
<p>Let‚Äôs try with our example! Let‚Äôs go with E and Z values from before</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Ctext%7BE%7D%20+%20%5Ctext%7BAttention(E)%7D%20&amp;=%20%5Cbegin%7Bbmatrix%7D%0A1.0%20&amp;%203.0%20&amp;%203.0%20&amp;%205.0%20%5C%5C%0A2.84%20&amp;%203.99%20&amp;%204.0%20&amp;%206.0%0A%5Cend%7Bbmatrix%7D%20+%20%5Cbegin%7Bbmatrix%7D%0A11.46394281%20&amp;%20-13.18016469%20&amp;%20-11.59340253%20&amp;%20-17.04387833%20%5C%5C%0A11.62608569%20&amp;%20-13.47454934%20&amp;%20-11.87126395%20&amp;%20-17.49263674%0A%5Cend%7Bbmatrix%7D%20%5C%5C%0A&amp;=%20%5Cbegin%7Bbmatrix%7D%0A12.46394281%20&amp;%20-10.18016469%20&amp;%20-8.59340253%20&amp;%20-12.04387833%20%5C%5C%0A14.46608569%20&amp;%20-9.48454934%20&amp;%20-7.87126395%20&amp;%20-11.49263674%0A%5Cend%7Bbmatrix%7D%0A%5Cend%7Balign*%7D%0A"></p>
<p>Let‚Äôs now calculate the layer normalization, we can divide it into three steps:</p>
<ol type="1">
<li>Compute mean and variance for each embedding.</li>
<li>Normalize by substracting the mean of its row and dividing by the square root of its row variance (plus a small number to avoid division by zero).</li>
<li>Scale and shift by multiplying by gamma and adding beta.</li>
</ol>
<section id="mean-and-variance" class="level5">
<h5 class="anchored" data-anchor-id="mean-and-variance">5.3.1 Mean and variance</h5>
<p>For the first embedding</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmu_1%20&amp;=%20%5Cfrac%7B12.46394281-10.18016469-8.59340253-12.04387833%7D%7B4%7D%20=%20-4.58837568%20%5C%5C%0A%5Csigma%5E2%20&amp;=%20%5Cfrac%7B%5Csum%20(x_i%20-%20%5Cmu)%5E2%7D%7BN%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B(12.46394281%20-%20(-4.588375685))%5E2%20+%20%5Cldots%20+%20(-12.04387833%20-%20(-4.588375685))%5E2%7D%7B4%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B393.67443005013%7D%7B4%7D%20%5C%5C%0A&amp;=%2098.418607512533%20%5C%5C%0A%5Csigma%20&amp;=%20%5Csqrt%7B98.418607512533%7D%20%5C%5C%0A&amp;=%209.9206152789297%0A%5Cend%7Balign*%7D%0A"></p>
<p>We can do the same for the second embedding. We‚Äôll skip the calculations but you get the hang of it.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign*%7D%0A%5Cmu_2%20&amp;=%20-3.59559109%20%5C%5C%0A%5Csigma_2%20&amp;=%2010.50653018%0A%5Cend%7Balign*%7D%0A"></p>
<p>Let‚Äôs confirm with Python</p>
<div id="e75572f6" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb44" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1">(embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Z).mean(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[-4.58837567],
       [-3.59559107]])</code></pre>
</div>
</div>
<div id="091881c7" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb46" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1">(embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Z).std(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[ 9.92061529],
       [10.50653019]])</code></pre>
</div>
</div>
<p>Amazing! Let‚Äôs now normalize</p>
</section>
<section id="normalize" class="level5">
<h5 class="anchored" data-anchor-id="normalize">5.3.2 Normalize</h5>
<p>For normalization, for each value in the embedding, we subsctract the mean and divide by the standard deviation. Epsilon is a very small value, such as 0.00001. We‚Äôll assume <img src="https://latex.codecogs.com/png.latex?%5Cgamma=1"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta=0">, it simplifies things.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%5Ctext%7Bnormalized%7D_1%20&amp;=%20%5Cfrac%7B12.46394281%20-%20(-4.58837568)%7D%7B%5Csqrt%7B98.418607512533%20+%20%5Cepsilon%7D%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B17.05231849%7D%7B9.9206152789297%7D%20%5C%5C%0A&amp;=%201.718%20%5C%5C%0A%5Ctext%7Bnormalized%7D_2%20&amp;=%20%5Cfrac%7B-10.18016469%20-%20(-4.58837568)%7D%7B%5Csqrt%7B98.418607512533%20+%20%5Cepsilon%7D%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B-5.59178901%7D%7B9.9206152789297%7D%20%5C%5C%0A&amp;=%20-0.564%20%5C%5C%0A%5Ctext%7Bnormalized%7D_3%20&amp;=%20%5Cfrac%7B-8.59340253%20-%20(-4.58837568)%7D%7B%5Csqrt%7B98.418607512533%20+%20%5Cepsilon%7D%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B-4.00502685%7D%7B9.9206152789297%7D%20%5C%5C%0A&amp;=%20-0.404%20%5C%5C%0A%5Ctext%7Bnormalized%7D_4%20&amp;=%20%5Cfrac%7B-12.04387833%20-%20(-4.58837568)%7D%7B%5Csqrt%7B98.418607512533%20+%20%5Cepsilon%7D%7D%20%5C%5C%0A&amp;=%20%5Cfrac%7B-7.45550265%7D%7B9.9206152789297%7D%20%5C%5C%0A&amp;=%20-0.752%0A%5Cend%7Balign*%7D"></p>
<p>We‚Äôll skip the calculations by hand for the second embedding. Let‚Äôs confirm with code! Let‚Äôs re-define our <code>encoder_block</code> function with this change</p>
<div id="af03350d" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb48" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> layer_norm(x, epsilon<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1e-6</span>):</span>
<span id="cb48-2">    mean <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.mean(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb48-3">    std <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> x.std(axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdims<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb48-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> mean) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> (std <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> epsilon)</span>
<span id="cb48-5"></span>
<span id="cb48-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> encoder_block(x, WQs, WKs, WVs, W1, b1, W2, b2):</span>
<span id="cb48-7">    Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> multi_head_attention(x, WQs, WKs, WVs)</span>
<span id="cb48-8">    Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer_norm(Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x)</span>
<span id="cb48-9"></span>
<span id="cb48-10">    output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> feed_forward(Z, W1, b1, W2, b2)</span>
<span id="cb48-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> layer_norm(output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Z)</span></code></pre></div></div>
</div>
<div id="0f2ceeba" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb49" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1">layer_norm(Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> embedding)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[ 1.71887693, -0.56365339, -0.40370747, -0.75151608],
       [ 1.71909039, -0.56050453, -0.40695381, -0.75163205]])</code></pre>
</div>
</div>
<p>It works! Let‚Äôs retry to pass the embedding through the six encoders.</p>
<div id="f2a1f920" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb51" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> encoder(x, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>):</span>
<span id="cb51-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n):</span>
<span id="cb51-3">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> random_encoder_block(x)</span>
<span id="cb51-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x</span>
<span id="cb51-5"></span>
<span id="cb51-6"></span>
<span id="cb51-7">encoder(embedding)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[-0.335849  , -1.44504571,  1.21698183,  0.56391289],
       [-0.33583947, -1.44504861,  1.21698606,  0.56390202]])</code></pre>
</div>
</div>
<p>Amazing! These values make sense and we don‚Äôt get NaNs! The idea of the stack of encoders is that they output a continuous representation, z, that captures the meaning of the input sequence. This representation is then passed to the decoder, which will genrate an output sequence of symbols, one element at a time.</p>
<p>Before diving into the decoder, here‚Äôs an image from Jay‚Äôs amazing blog post:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png" class="img-fluid figure-img"></p>
<figcaption>Encoder and decoder</figcaption>
</figure>
</div>
<p>You should be able to explain each component at the left side! Quite impressive, right? Let‚Äôs now move to the decoder.</p>
</section>
</section>
</section>
</section>
<section id="decoder" class="level2">
<h2 class="anchored" data-anchor-id="decoder">Decoder</h2>
<p>Most of the thing we learned for encoders will be used in the decoder as well! The decoder has two self-attention layers, one for the encoder and one for the decoder. The decoder also has a feed-forward layer. Let‚Äôs go through each of these.</p>
<p>The decoder block receives two inputs: the output of the encoder and the generated output sequence. The output of the encoder is the representation of the input sequence. During inference, the generated output sequence starts with a special start-of-sequence token (SOS). During training, the target output sequence is the actual output sequence, shifted by one position. This will be clearer soon!</p>
<p>Given the embedding generated by the encoder and the SOS token, the decoder will then generate the next token of the sequence, e.g.&nbsp;‚Äúhola‚Äù. The decoder is autoregressive, that means that the decoder will take the previously generated tokens and again generate the second token.</p>
<ul>
<li>Iteration 1: Input is SOS, output is ‚Äúhola‚Äù</li>
<li>Iteration 2: Input is SOS + ‚Äúhola‚Äù, output is ‚Äúmundo‚Äù</li>
<li>Iteration 3: Input is SOS + ‚Äúhola‚Äù + ‚Äúmundo‚Äù, output is EOS</li>
</ul>
<p>Here, SOS is the start-of-sequence token and EOS is the end-of-sequence token. The decoder will stop when it generates the EOS token. It generates one token at a time. Note that all iterations use the embedding generated by the encoder.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>This autoregressive design makes decoder slow.</strong> The encoder is able to generate its embedding in a single forward pass while the decoder needs to do many forward passes. This is one of the reasons why architectures that only use the encoder (such as BERT or sentence similarity models) are much faster than decoder-only architectures (such as GPT-2 or BART).</p>
</div>
</div>
<p>Let‚Äôs dive into each step! Just as the encoder, the decoder is composed of a stack of decoder blocks. The decoder block is a bit more complex than the encoder block. The general structure is:</p>
<ol type="1">
<li>(Masked) Self-attention layer</li>
<li>Residual connection and layer normalization</li>
<li>Encoder-decoder attention layer</li>
<li>Residual connection and layer normalization</li>
<li>Feed-forward layer</li>
<li>Residual connection and layer normalization</li>
</ol>
<p>We‚Äôre already familiar with all the math from 1, 2, 3, 5 and 6. See the right side of the image below, you‚Äôll see that all these blocks you already know (the right part):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://arxiv.org/abs/1706.03762"><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png" class="img-fluid figure-img"></a></p>
<figcaption>Transformer model from the original ‚Äúattention is all you need‚Äù paper</figcaption>
</figure>
</div>
<section id="embedding-the-text-1" class="level3">
<h3 class="anchored" data-anchor-id="embedding-the-text-1">1. Embedding the text</h3>
<p>The first text of the decoder is to embed the input tokens. The input token is <code>SOS</code>, so we‚Äôll embed it. We‚Äôll use the same embedding dimension as the encoder. Let‚Äôs assume the embedding vector for <code>SOS</code> is the following:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%200%20&amp;%200%20&amp;%200%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="positional-encoding-1" class="level3">
<h3 class="anchored" data-anchor-id="positional-encoding-1">2. Positional encoding</h3>
<p>We‚Äôll now add the positional encoding to the embedding, just as we did for the encoder. Given it‚Äôs the same position as ‚ÄúHello‚Äù, we‚Äôll have same positional encoding as we did before:</p>
<ul>
<li>i = 0 (even): PE(0,0) = sin(0 / 10000^(0 / 4)) = sin(0) = 0</li>
<li>i = 1 (odd): PE(0,1) = cos(0 / 10000^(2*1 / 4)) = cos(0) = 1</li>
<li>i = 2 (even): PE(0,2) = sin(0 / 10000^(2*2 / 4)) = sin(0) = 0</li>
<li>i = 3 (odd): PE(0,3) = cos(0 / 10000^(2*3 / 4)) = cos(0) = 1</li>
</ul>
</section>
<section id="add-positional-encoding-and-embedding-1" class="level3">
<h3 class="anchored" data-anchor-id="add-positional-encoding-and-embedding-1">3. Add positional encoding and embedding</h3>
<p>Adding the positional encoding to the embedding is done by adding the two vectors together:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%20=%20%5Cbegin%7Bbmatrix%7D%0A1%20&amp;%201%20&amp;%200%20&amp;%201%0A%5Cend%7Bbmatrix%7D%0A"></p>
</section>
<section id="self-attention-1" class="level3">
<h3 class="anchored" data-anchor-id="self-attention-1">4. Self-attention</h3>
<p>The first step within the decoder block is the self-attention mechanism. Luckily, we have some code for this and can just use it!</p>
<div id="14fb8de7" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb53" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1">d_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb53-2">n_attention_heads <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb53-3"></span>
<span id="cb53-4">E <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]])</span>
<span id="cb53-5">WQs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [np.random.randn(d_embedding, d_query) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)]</span>
<span id="cb53-6">WKs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [np.random.randn(d_embedding, d_key) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)]</span>
<span id="cb53-7">WVs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [np.random.randn(d_embedding, d_value) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)]</span>
<span id="cb53-8"></span>
<span id="cb53-9">Z_self_attention <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> multi_head_attention(E, WQs, WKs, WVs)</span>
<span id="cb53-10">Z_self_attention</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[ 2.19334924, 10.61851198, -4.50089666, -2.76366551]])</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Things are quite simple for inference. For training, things are a bit tricky. During training, we use unlabeled data: just a bunch of text data, frequentyl scraped from the web. While the encoder‚Äôs goal is to capture all information of the input, the decoder‚Äôs goal is to predict the most likely next token. This means that the decoder can only use the tokens that have been generated so far (it cannot cheat and see the next tokens).</p>
<p>Because of this, we use masked self-attention: we mask the tokens that have not been generated yet. This is done by setting the attention scores to -inf. This is done in the original paper (section 3.2.3.1). We‚Äôll skip this for now, but it‚Äôs important to keep in mind that the decoder is a bit more complex during training.</p>
</div>
</div>
</section>
<section id="residual-connection-and-layer-normalization" class="level3">
<h3 class="anchored" data-anchor-id="residual-connection-and-layer-normalization">5. Residual connection and layer normalization</h3>
<p>Nothing magical here, we just add the input to the output of the self-attention and apply layer normalization. We‚Äôll use the same code as before.</p>
<div id="888c3adb" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb55" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1">Z_self_attention <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer_norm(Z_self_attention <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> E)</span>
<span id="cb55-2">Z_self_attention</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[ 0.17236212,  1.54684892, -1.0828824 , -0.63632864]])</code></pre>
</div>
</div>
</section>
<section id="encoder-decoder-attention" class="level3">
<h3 class="anchored" data-anchor-id="encoder-decoder-attention">6. Encoder-decoder attention</h3>
<p><strong>This part is the new one!</strong> If you were wondering where do the encoder-generated embeddings come in, this is their moment to shine!</p>
<p>Let‚Äôs assume the output of the encoder is the following matrix</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Bbmatrix%7D%0A-1.5%20&amp;%201.0%20&amp;%20-0.8%20&amp;%201.5%20%5C%5C%0A1.0%20&amp;%20-1.0%20&amp;%20-0.5%20&amp;%201.0%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>In the self-attention mechanism, we calculate the queries, keys, and values from the input embedding.</p>
<p>In the encoder-decoder attention, we calculate the queries from the previous decoder layer and the keys and values from the encoder output! All the math is the same as before; the only difference is what embedding to use for the queries. Let‚Äôs look at some code</p>
<div id="0c8c535d" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb57" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> encoder_decoder_attention(encoder_output, attention_input, WQ, WK, WV):</span>
<span id="cb57-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The next three lines are the key difference!</span></span>
<span id="cb57-3">    K <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> encoder_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WK    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Note that now we pass the previous encoder output!</span></span>
<span id="cb57-4">    V <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> encoder_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WV    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Note that now we pass the previous encoder output!</span></span>
<span id="cb57-5">    Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> attention_input <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> WQ   <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Same as self-attention</span></span>
<span id="cb57-6"></span>
<span id="cb57-7">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This stays the same</span></span>
<span id="cb57-8">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Q <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> K.T</span>
<span id="cb57-9">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> np.sqrt(d_key)</span>
<span id="cb57-10">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> softmax(scores)</span>
<span id="cb57-11">    scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> V</span>
<span id="cb57-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> scores</span>
<span id="cb57-13"></span>
<span id="cb57-14"></span>
<span id="cb57-15"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> multi_head_encoder_decoder_attention(</span>
<span id="cb57-16">    encoder_output, attention_input, WQs, WKs, WVs</span>
<span id="cb57-17">):</span>
<span id="cb57-18">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Note that now we pass the previous encoder output!</span></span>
<span id="cb57-19">    attentions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.concatenate(</span>
<span id="cb57-20">        [</span>
<span id="cb57-21">            encoder_decoder_attention(</span>
<span id="cb57-22">                encoder_output, attention_input, WQ, WK, WV</span>
<span id="cb57-23">            )</span>
<span id="cb57-24">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> WQ, WK, WV <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(WQs, WKs, WVs)</span>
<span id="cb57-25">        ],</span>
<span id="cb57-26">        axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb57-27">    )</span>
<span id="cb57-28">    W <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(n_attention_heads <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> d_value, d_embedding)</span>
<span id="cb57-29">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> attentions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span> W</span></code></pre></div></div>
</div>
<div id="1d3380ae" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb58" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1">WQs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [np.random.randn(d_embedding, d_query) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)]</span>
<span id="cb58-2">WKs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [np.random.randn(d_embedding, d_key) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)]</span>
<span id="cb58-3">WVs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [np.random.randn(d_embedding, d_value) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)]</span>
<span id="cb58-4"></span>
<span id="cb58-5">encoder_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>], [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>]])</span>
<span id="cb58-6"></span>
<span id="cb58-7">Z_encoder_decoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> multi_head_encoder_decoder_attention(</span>
<span id="cb58-8">    encoder_output, Z_self_attention, WQs, WKs, WVs</span>
<span id="cb58-9">)</span>
<span id="cb58-10">Z_encoder_decoder</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[ 1.57651431,  4.92489307, -0.08644448, -0.46776051]])</code></pre>
</div>
</div>
<p>This worked! You might be asking ‚Äúwhy do we do this?‚Äù. The reason is that we want the decoder to focus on the relevant parts of the input text (e.g., ‚Äúhello world‚Äù). The encoder-decoder attention allows each position in the decoder to attend over all positions in the input sequence. This is very helpful for tasks such as translation, where the decoder needs to focus on the relevant parts of the input sequence. The decoder will learn to focus on the relevant parts of the input sequence by learning to generate the correct output tokens. This is a very powerful mechanism!</p>
</section>
<section id="residual-connection-and-layer-normalization-1" class="level3">
<h3 class="anchored" data-anchor-id="residual-connection-and-layer-normalization-1">7. Residual connection and layer normalization</h3>
<p>Same as before!</p>
<div id="a5645dc1" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb60" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1">Z_encoder_decoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer_norm(Z_encoder_decoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Z_self_attention)</span>
<span id="cb60-2">Z_encoder_decoder</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[-0.44406723,  1.6552893 , -0.19984632, -1.01137575]])</code></pre>
</div>
</div>
</section>
<section id="feed-forward-layer-1" class="level3">
<h3 class="anchored" data-anchor-id="feed-forward-layer-1">8. Feed-forward layer</h3>
<p>Once again, same as before! I‚Äôll also do the residual connection and layer normalization after it.</p>
<div id="cd016208" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb62" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1">W1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>)</span>
<span id="cb62-2">W2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span>
<span id="cb62-3">b1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>)</span>
<span id="cb62-4">b2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span>
<span id="cb62-5"></span>
<span id="cb62-6">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer_norm(feed_forward(Z_encoder_decoder, W1, b1, W2, b2) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Z_encoder_decoder)</span>
<span id="cb62-7">output</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[-0.97650182,  0.81470137, -2.79122044, -3.39192873]])</code></pre>
</div>
</div>
</section>
<section id="encapsulating-everything-the-random-decoder" class="level3">
<h3 class="anchored" data-anchor-id="encapsulating-everything-the-random-decoder">9. Encapsulating everything: The Random Decoder</h3>
<p>Let‚Äôs write the code for a single decoder block. The main change is that we now have an additional attention mechanism.</p>
<div id="17f3f866" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb64" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1">d_embedding <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb64-2">d_key <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> d_value <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> d_query <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span></span>
<span id="cb64-3">d_feed_forward <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span></span>
<span id="cb64-4">n_attention_heads <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb64-5">encoder_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.array([[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>], [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>]])</span>
<span id="cb64-6"></span>
<span id="cb64-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> decoder_block(</span>
<span id="cb64-8">    x,</span>
<span id="cb64-9">    encoder_output,</span>
<span id="cb64-10">    WQs_self_attention, WKs_self_attention, WVs_self_attention,</span>
<span id="cb64-11">    WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,</span>
<span id="cb64-12">    W1, b1, W2, b2,</span>
<span id="cb64-13">):</span>
<span id="cb64-14">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Same as before</span></span>
<span id="cb64-15">    Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> multi_head_attention(</span>
<span id="cb64-16">        x, WQs_self_attention, WKs_self_attention, WVs_self_attention</span>
<span id="cb64-17">    )</span>
<span id="cb64-18">    Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer_norm(Z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> x)</span>
<span id="cb64-19"></span>
<span id="cb64-20">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># The next three lines are the key difference!</span></span>
<span id="cb64-21">    Z_encoder_decoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> multi_head_encoder_decoder_attention(</span>
<span id="cb64-22">        encoder_output, Z, WQs_ed_attention, WKs_ed_attention, WVs_ed_attention</span>
<span id="cb64-23">    )</span>
<span id="cb64-24">    Z_encoder_decoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> layer_norm(Z_encoder_decoder <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Z)</span>
<span id="cb64-25"></span>
<span id="cb64-26">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Same as before</span></span>
<span id="cb64-27">    output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> feed_forward(Z_encoder_decoder, W1, b1, W2, b2)</span>
<span id="cb64-28">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> layer_norm(output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> Z_encoder_decoder)</span>
<span id="cb64-29"></span>
<span id="cb64-30"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> random_decoder_block(x, encoder_output):</span>
<span id="cb64-31">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Just a bunch of random initializations</span></span>
<span id="cb64-32">    WQs_self_attention <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb64-33">        np.random.randn(d_embedding, d_query) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)</span>
<span id="cb64-34">    ]</span>
<span id="cb64-35">    WKs_self_attention <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb64-36">        np.random.randn(d_embedding, d_key) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)</span>
<span id="cb64-37">    ]</span>
<span id="cb64-38">    WVs_self_attention <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb64-39">        np.random.randn(d_embedding, d_value) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)</span>
<span id="cb64-40">    ]</span>
<span id="cb64-41"></span>
<span id="cb64-42">    WQs_ed_attention <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb64-43">        np.random.randn(d_embedding, d_query) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)</span>
<span id="cb64-44">    ]</span>
<span id="cb64-45">    WKs_ed_attention <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb64-46">        np.random.randn(d_embedding, d_key) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)</span>
<span id="cb64-47">    ]</span>
<span id="cb64-48">    WVs_ed_attention <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb64-49">        np.random.randn(d_embedding, d_value) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n_attention_heads)</span>
<span id="cb64-50">    ]</span>
<span id="cb64-51"></span>
<span id="cb64-52">    W1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(d_embedding, d_feed_forward)</span>
<span id="cb64-53">    b1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(d_feed_forward)</span>
<span id="cb64-54">    W2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(d_feed_forward, d_embedding)</span>
<span id="cb64-55">    b2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(d_embedding)</span>
<span id="cb64-56"></span>
<span id="cb64-57"></span>
<span id="cb64-58">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> decoder_block(</span>
<span id="cb64-59">        x, encoder_output,</span>
<span id="cb64-60">        WQs_self_attention, WKs_self_attention, WVs_self_attention,</span>
<span id="cb64-61">        WQs_ed_attention, WKs_ed_attention, WVs_ed_attention,</span>
<span id="cb64-62">        W1, b1, W2, b2,</span>
<span id="cb64-63">    )</span></code></pre></div></div>
</div>
<div id="ae5b3375" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb65" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> decoder(x, decoder_embedding, n<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>):</span>
<span id="cb65-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(n):</span>
<span id="cb65-3">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> random_decoder_block(x, decoder_embedding)</span>
<span id="cb65-4">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x</span>
<span id="cb65-5"></span>
<span id="cb65-6">decoder(E, encoder_output)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[ 0.25919176,  1.49913566, -1.14331487, -0.61501256],
       [ 0.25956188,  1.49896896, -1.14336934, -0.61516151]])</code></pre>
</div>
</div>
</section>
</section>
<section id="generating-the-output-sequence" class="level2">
<h2 class="anchored" data-anchor-id="generating-the-output-sequence">Generating the output sequence</h2>
<p>We have all the building blocks! Let‚Äôs now generate the output sequence.</p>
<ul>
<li>We have the <strong>encoder</strong>, which takes the input sequence and generates its rich representation. It‚Äôs composed of a stack of encoder blocks.</li>
<li>We have the <strong>decoder</strong>, which takes the encoder output and generated tokens, and generates the output sequence. It‚Äôs composed of a stack of decoder blocks.</li>
</ul>
<p>How do we go from the decoder‚Äôs output to a word? We need to add a final linear layer and a softmax layer on top of the decoder. The whole algorithm looks like this:</p>
<ol type="1">
<li><strong>Encoder Processing:</strong> The encoder receives the input sequence and generates a contextualized representation of the entire sequence, utilizing a stack of encoder blocks.</li>
<li><strong>Decoder Initiation:</strong> The decoding process begins with the embedding of the SOS (Start of Sequence) token, combined with the encoder‚Äôs output.</li>
<li><strong>Decoder Operation:</strong> The decoder uses the encoder‚Äôs output and the embeddings of all previously generated tokens to produce a new list of embeddings.</li>
<li><strong>Linear Layer for Logits</strong> A linear layer is applied to the latest output embedding from the decoder to generate logits, representing raw predictions for the next token.</li>
<li><strong>Softmax for Probabilities:</strong> These logits are then passed through a softmax layer, which converts them into a probability distribution over potential next tokens.</li>
<li><strong>Iterative Token Generation:</strong> This process is repeated, with each step involving the decoder generating the next token based on the cumulative embeddings of previously generated tokens and the initial encoder output.</li>
<li><strong>Sequence Completion:</strong> The generation continues through these steps until the EOS (End of Sequence) token is produced or a predefined maximum sequence length is reached.</li>
</ol>
<p>This is mentioned in the section 3.4 of the paper.</p>
<section id="linear-layer" class="level3">
<h3 class="anchored" data-anchor-id="linear-layer">1. Linear layer</h3>
<p>The linear layer is a simple linear transformation. It takes the decoder‚Äôs output and transforms it into a vector of size <code>vocab_size</code>. This is the size of the vocabulary. For example, if we have a vocabulary of 10000 words, the linear layer will transform the decoder‚Äôs output into a vector of size 10000. This vector will contain the probability of each word being the next word in the sequence. For simplicity, let‚Äôs go with a vocabulary of 10 words and assume the first decoder output is a very simple vector: [1, 0, 1, 0]. We‚Äôll use random weights and biases matrices of the size <code>vocab_size</code> x <code>decoder_output_size</code>.</p>
<div id="0d2e4026" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb67" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> linear(x, W, b):</span>
<span id="cb67-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> np.dot(x, W) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> b</span>
<span id="cb67-3"></span>
<span id="cb67-4">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> linear([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>), np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>))</span>
<span id="cb67-5">x</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([ 0.06900542, -1.81351091, -1.3122958 , -0.33197364,  2.54767851,
       -1.55188231,  0.82907169,  0.85910931, -0.32982856, -1.26792439])</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>What do we use as input for the linear layer? The decoder will output one embedding for each token in the sequence. The input for the linear layer will be the last generated embedding. The last embedding encapsulates information to the entire sequence up to that point, so it contains all the information needed to generate the next token. This means that each output embedding from the decoder contains information about the entire sequence up to that point.</p>
</div>
</div>
</section>
<section id="softmax" class="level3">
<h3 class="anchored" data-anchor-id="softmax">2. Softmax</h3>
<p>These are called logits but they are not easily interpretable. We need to apply a softmax function to obtain the probabilities.</p>
<div id="5bb5c83c" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb69" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1">softmax(x)</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>array([[0.01602618, 0.06261303, 0.38162024, 0.03087794, 0.0102383 ,
        0.00446011, 0.01777314, 0.00068275, 0.46780959, 0.00789871]])</code></pre>
</div>
</div>
<p>This is giving us probabilities! Let‚Äôa assume the vocabulary is the following:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bvocab%7D%20=%20%5Cbegin%7Bbmatrix%7D%0A%5Ctext%7Bhello%7D%20&amp;%20%5Ctext%7Bmundo%7D%20&amp;%20%5Ctext%7Bworld%7D%20&amp;%20%5Ctext%7Bhow%7D%20&amp;%20%5Ctext%7B?%7D%20&amp;%20%5Ctext%7BEOS%7D%20&amp;%20%5Ctext%7BSOS%7D%20&amp;%20%5Ctext%7Ba%7D%20&amp;%20%5Ctext%7Bhola%7D%20&amp;%20%5Ctext%7Bc%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>The above tells us that the probabilities are</p>
<ul>
<li>hello: 0.01602618</li>
<li>mundo: 0.06261303</li>
<li>world: 0.38162024</li>
<li>how: 0.03087794</li>
<li>?: 0.0102383</li>
<li>EOS: 0.00446011</li>
<li>SOS: 0.01777314</li>
<li>a: 0.00068275</li>
<li>hola: 0.46780959</li>
<li>c: 0.00789871</li>
</ul>
<p>From these, the most likely next token is ‚Äúhola‚Äù. Picking always the most likely token is called greedy decoding. This is not always the best approach, as it might lead to suboptimal results, but we won‚Äôt dive into generation techniques at the moment. If you want to learn more about it, check out this amazing <a href="https://huggingface.co/blog/how-to-generate">blog post</a>.</p>
</section>
<section id="the-random-encoder-decoder-transformer" class="level3">
<h3 class="anchored" data-anchor-id="the-random-encoder-decoder-transformer">3. The Random Encoder-Decoder Transformer</h3>
<p>Let‚Äôs write the whole code for this! Let‚Äôs define a dictionary that maps the words to their initial embeddings. Note that this is also learned during training, but we‚Äôll use random values for now.</p>
<div id="eaff46ac" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb71" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1">vocabulary <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb71-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"hello"</span>,</span>
<span id="cb71-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"mundo"</span>,</span>
<span id="cb71-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"world"</span>,</span>
<span id="cb71-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"how"</span>,</span>
<span id="cb71-6">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"?"</span>,</span>
<span id="cb71-7">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EOS"</span>,</span>
<span id="cb71-8">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"SOS"</span>,</span>
<span id="cb71-9">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a"</span>,</span>
<span id="cb71-10">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"hola"</span>,</span>
<span id="cb71-11">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"c"</span>,</span>
<span id="cb71-12">]</span>
<span id="cb71-13">embedding_reps <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span>
<span id="cb71-14">vocabulary_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb71-15">    word: embedding_reps[i] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, word <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(vocabulary)</span>
<span id="cb71-16">}</span>
<span id="cb71-17">vocabulary_embeddings</span></code></pre></div></div>
<div class="cell-output cell-output-display">
<pre><code>{'hello': array([-0.32106406,  2.09332588, -0.77994069,  0.92639774]),
 'mundo': array([-0.59563791, -0.63389256,  1.70663692, -0.99495115]),
 'world': array([ 1.35581862, -0.0323546 ,  2.76696887,  0.83069982]),
 'how': array([-0.52975474,  0.94439644,  0.80073818, -1.50135518]),
 '?': array([-0.88116833,  0.13995055,  2.01827674, -0.52554391]),
 'EOS': array([1.12207024, 1.40905796, 1.22231714, 0.02267638]),
 'SOS': array([-0.60624082, -0.67560165,  0.77152125,  0.63472247]),
 'a': array([ 1.67622229, -0.20319309, -0.18324905, -0.24258774]),
 'hola': array([ 1.07809402, -0.83846408, -0.33448976,  0.28995976]),
 'c': array([ 0.65643157,  0.24935726, -0.80839751, -1.87156293])}</code></pre>
</div>
</div>
<p>And now let‚Äôs write our random <code>generate</code> method that generates tokens autorergressively.</p>
<div id="bc0cbdbc" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb73" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> generate(input_sequence, max_iters<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>):</span>
<span id="cb73-2">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We first encode the inputs into embeddings</span></span>
<span id="cb73-3">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># This skips the positional encoding step for simplicity</span></span>
<span id="cb73-4">    embedded_inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb73-5">        vocabulary_embeddings[token] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> token <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> input_sequence</span>
<span id="cb73-6">    ]</span>
<span id="cb73-7">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Embedding representation (encoder input)"</span>, embedded_inputs)</span>
<span id="cb73-8"></span>
<span id="cb73-9">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We then generate an embedding representation</span></span>
<span id="cb73-10">    encoder_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> encoder(embedded_inputs)</span>
<span id="cb73-11">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Embedding generated by encoder (encoder output)"</span>, encoder_output)</span>
<span id="cb73-12"></span>
<span id="cb73-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We initialize the decoder output with the embedding of the start token</span></span>
<span id="cb73-14">    sequence_embeddings <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [vocabulary_embeddings[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"SOS"</span>]]</span>
<span id="cb73-15">    output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"SOS"</span></span>
<span id="cb73-16">    </span>
<span id="cb73-17">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Random matrices for the linear layer</span></span>
<span id="cb73-18">    W_linear <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(d_embedding, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(vocabulary))</span>
<span id="cb73-19">    b_linear <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.random.randn(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(vocabulary))</span>
<span id="cb73-20"></span>
<span id="cb73-21">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We limit number of decoding steps to avoid too long sequences without EOS</span></span>
<span id="cb73-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(max_iters):</span>
<span id="cb73-23">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Decoder step</span></span>
<span id="cb73-24">        decoder_output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> decoder(sequence_embeddings, encoder_output)</span>
<span id="cb73-25"></span>
<span id="cb73-26">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Only use the last output for prediction</span></span>
<span id="cb73-27">        logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> linear(decoder_output[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>], W_linear, b_linear)</span>
<span id="cb73-28">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We wrap logits in a list as our softmax expects batches/2D array</span></span>
<span id="cb73-29">        probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> softmax([logits])</span>
<span id="cb73-30"></span>
<span id="cb73-31">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># We get the most likely next token</span></span>
<span id="cb73-32">        next_token <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> vocabulary[np.argmax(probs)]</span>
<span id="cb73-33">        sequence_embeddings.append(vocabulary_embeddings[next_token])</span>
<span id="cb73-34">        output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> next_token</span>
<span id="cb73-35"></span>
<span id="cb73-36">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(</span>
<span id="cb73-37">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Iteration"</span>, i, </span>
<span id="cb73-38">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"next token"</span>, next_token,</span>
<span id="cb73-39">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"with probability of"</span>, np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(probs),</span>
<span id="cb73-40">        )</span>
<span id="cb73-41"></span>
<span id="cb73-42">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># If the next token is the end token, we return the sequence</span></span>
<span id="cb73-43">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> next_token <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EOS"</span>:</span>
<span id="cb73-44">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> output</span>
<span id="cb73-45"></span>
<span id="cb73-46">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> output, sequence_embeddings</span></code></pre></div></div>
</div>
<p>Let‚Äôs run this now!</p>
<div id="c47b03f2" class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb74" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1">generate([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"hello"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"world"</span>])</span></code></pre></div></div>
<div class="cell-output cell-output-stdout">
<pre><code>Embedding representation (encoder input) [array([-0.32106406,  2.09332588, -0.77994069,  0.92639774]), array([ 1.35581862, -0.0323546 ,  2.76696887,  0.83069982])]
Embedding generated by encoder (encoder output) [[ 1.14747807 -1.5941759   0.36847675  0.07822107]
 [ 1.14747705 -1.59417696  0.36847441  0.07822551]]
Iteration 0 next token hola with probability of 0.4327111653266739
Iteration 1 next token mundo with probability of 0.4411354383451089
Iteration 2 next token world with probability of 0.4746898792307499</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>('SOS hola mundo world',
 [array([-0.60624082, -0.67560165,  0.77152125,  0.63472247]),
  array([ 1.07809402, -0.83846408, -0.33448976,  0.28995976]),
  array([-0.59563791, -0.63389256,  1.70663692, -0.99495115]),
  array([ 1.35581862, -0.0323546 ,  2.76696887,  0.83069982])])</code></pre>
</div>
</div>
<p>Ok, so we got the tokens ‚Äúhow‚Äù, ‚Äúa‚Äù, and ‚Äúc‚Äù. This is not a good translation, but it‚Äôs expected! We only used random weights!</p>
<p>I suggest you to look again in detail at the whole encoder-decoder architecture from the original paper:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/transformer.png" class="img-fluid figure-img"></p>
<figcaption>Encoder and decoder</figcaption>
</figure>
</div>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>I hope that was fun and informational! We covered a lot of ground. Wait‚Ä¶was that it? And the answer is, mostly, yes! New transformer architectures add lots of tricks, but the core of the transformer is what we just covered. Depending on what task you want to solve, you can also only the encoder or the decoder. For example, for understanding-heavy tasks such as classification, you can use the encoder stack with a linear layer on top. For generation-heavy tasks such as translation, you can use the encoder and decoder stacks. And finally, for free generation, as in ChatGPT or Mistral, you can use only the decoder stack.</p>
<p>Of course, we also did lots of simplifications. Let‚Äôs briefly check which were the numbers in the original transformer paper:</p>
<ul>
<li>Embedding dimension: 512 (4 in our example)</li>
<li>Number of encoders: 6 (6 in our example)</li>
<li>Number of decoders: 6 (6 in our example)</li>
<li>Feed-forward dimension: 2048 (8 in our example)</li>
<li>Number of attention heads: 8 (2 in our example)</li>
<li>Attention dimension: 64 (3 in our example)</li>
</ul>
<p>We just covered lots of topics, but it‚Äôs quite interesting we can achieve impressive results by scaling up this math and doing smart training. We didn‚Äôt cover training in this blog post as the goal was to understand the math when using an existing model, but I hope this provided strong foundations for jumping into the training part. I hope you enjoyed this blog post!</p>
<p>You can also find a more formal document with the math in <a href="https://johnthickstun.com/docs/transformers.pdf">this PDF</a> (recommended by HackerNews folks).</p>
</section>
<section id="exercises" class="level2">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<p>Here are some exercises to practice your understanding of the transformer.</p>
<ol type="1">
<li>What is the purpose of the positional encoding?</li>
<li>How does self-attention and encoder-decoder attention differ?</li>
<li>What would happen if our attention dimension was too small? What about if it was too large?</li>
<li>Briefly describe the structure of a feed-forward layer.</li>
<li>Why is the decoder slower than the encoder?</li>
<li>What is the purpose of the residual connections and layer normalization?</li>
<li>How do we go from the decoder output to probabilities?</li>
<li>Why is picking the most likely next token every single time problematic?</li>
</ol>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></li>
<li><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></li>
<li><a href="https://huggingface.co/learn/nlp-course/chapter1/1">Hugging Face free NLP course</a></li>
</ul>


</section>

 ]]></description>
  <guid>https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/</guid>
  <pubDate>Mon, 01 Jan 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>The GPU Poor strike back</title>
  <link>https://osanseviero.github.io/hackerllama/blog/posts/gpu-poor-strike-back/</link>
  <description><![CDATA[ 





<p>
Some months ago, SemiAnalysis published a flashy <a href="https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini">article</a> with the premise that organizations with GPUs in the magnitude of tens of thousands had so many resources that the rest of the startups and researchers with <em>few</em> GPUs were wasting their time doing things such as local fine-tuning and over-quantization. According to them, the GPU Poor were not focusing on useful stuff.
</p>
<p>
First of all, I am, proudly, GPU Poor (I have a 3080/12GB GPU and do many things in free Colab). And I couldn‚Äôt be prouder of what the ecosystem has done this year. We‚Äôre in a world in which <a href="https://huggingface.co/TheBloke">TheBloke</a> quantizes models at the accelerating speed of the model releases; a world where the <a href="https://twitter.com/Teknium1">Tekniums</a>, <a href="https://www.reddit.com/r/LocalLLaMA/">local llamas</a>, and aligners and unaligners will fine-tune the models before they are even announced; a world in which Tim Dettmers enables us to do <a href="https://arxiv.org/abs/2305.14314">4-bit fine-tuning</a>. These are exciting days!
</p>
<p>
Yes, most of the community uses the nice Llama, but guess what? We also have options. Microsoft dropped Phi - a 3B model I can <a href="https://huggingface.co/spaces/radames/Candle-phi1-phi2-wasm-demo?model=phi_2_0_q4k">run in my browser</a> without sending anything to a server. Mistral unleashed <a href="https://mistral.ai/news/mixtral-of-experts/">Mixtral</a>, a MoE with the same quality as the largest version of Llama, and running much faster. And we also have Qwen, Yi, Falcon, Deci, Starling, InternML, MPT, and StableLM, plus all their fine tunes and weird merges.
</p>
<div class="captioned-image-container">
<figure class="figure">
<a class="image-link is-viewable-img image2" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8f556b3-9cc0-481c-aaef-4f985c0ffb88_575x288.png" data-component-name="Image2ToDOM">
<div class="image2-inset">
<picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8f556b3-9cc0-481c-aaef-4f985c0ffb88_575x288.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8f556b3-9cc0-481c-aaef-4f985c0ffb88_575x288.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8f556b3-9cc0-481c-aaef-4f985c0ffb88_575x288.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8f556b3-9cc0-481c-aaef-4f985c0ffb88_575x288.png 1456w" sizes="100vw"><img src="https://substack-post-media.s3.amazonaws.com/public/images/d8f556b3-9cc0-481c-aaef-4f985c0ffb88_575x288.png" width="575" height="288" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d8f556b3-9cc0-481c-aaef-4f985c0ffb88_575x288.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:288,&quot;width&quot;:575,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:285380,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null}" class="sizing-normal figure-img" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8f556b3-9cc0-481c-aaef-4f985c0ffb88_575x288.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8f556b3-9cc0-481c-aaef-4f985c0ffb88_575x288.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8f556b3-9cc0-481c-aaef-4f985c0ffb88_575x288.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd8f556b3-9cc0-481c-aaef-4f985c0ffb88_575x288.png 1456w" sizes="100vw" fetchpriority="high"></picture>
<div class="image-link-expand">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewbox="0 0 24 24" fill="none" stroke="#FFFFFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 ">
<polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line>
</svg>
</div>
</div>
</a>
</figure>
</div>
<p>
This year is the one in which we got tools such as <a href="https://lmstudio.ai/">LM Studio</a> and <a href="https://github.com/huggingface/candle">Candle</a> to run the models on-device, not sending any data to external servers. While the GPU Rich focused on somewhat similar user experiences (chatbots, LLM, maybe add some image or audio input here and then), the community can <a href="https://github.com/Vaibhavs10/insanely-fast-whisper">transcribe 2.5 hours of audio in less than 98 seconds</a>, do <a href="https://huggingface.co/spaces/diffusers/unofficial-SDXL-Turbo-i2i-t2i">image generation in real-time</a>, and even video understanding, all running in our good ol‚Äô potatoes.
</p>
<p>
While the Turbo GPU Rich spent weeks preparing their release and waiting to get those L8+ approvals, the tinkerers‚Äô communities of all kinds of disciplines, from artists to healthcare specialists, were combining open-source tools to generate music from images, figuring out how to enable <a href="https://huggingface.co/blog/lora-adapters-dynamic-loading">fast loading of dozens of LoRAs models</a>, or achieving <a href="https://arxiv.org/abs/2310.16795">sub-1-bit quantization</a>.
</p>
<p>
Don‚Äôt get me wrong. We greatly appreciate and love the amazing efforts of the GPU Rich that are releasing in the open their work and sharing with the community. We genuinely want them to succeed in their open and collaborative paths. But to imply that the GPU poor have no moat and are not contributing or doing something useful is naive.
</p>
<p>
The efforts of the GPU Poor and Middle Class are closing the access gap, making high-quality models more accessible than ever to people from different backgrounds, pushing open science forward, and taking hardware to its limits.
</p>
<p>
This was an exciting year for open-source, and we have a wide variety of labs and companies doing open work, GPU Poor, Middle Class, and Rich, all contributing in their own meaningful ways. Shoutouts to <a href="https://kyutai.org/">Kyutai</a>, <a href="http://answer.ai/">Answer.ai</a>, <a href="http://01.ai/">01.ai</a>, <a href="https://www.bigcode-project.org/">BigCode</a>, <a href="https://mistral.ai/">Mistral</a>, <a href="https://stability.ai/">Stability</a>, Alibaba, Meta, and Microsoft. This year, we also got <a href="https://twitter.com/NousResearch">Nous Research</a>, <a href="https://twitter.com/skunkworks_ai">Skunkworks AI</a>, <a href="https://twitter.com/alignment_lab">Alignment Lab</a>, Open Assistant, WizardLM, and so many other amazing communities.
</p>
<p>
So here we are, closing the year with an average of 3 new SOTA models daily, tackling all kinds of modalities, running models as powerful as GPT 3.5 in our computers, exploring AI feedback, building a thriving ecosystem of tools, and more. How can‚Äôt I be excited for next year?
</p>
<p>
What‚Äôs on the wishlist for next year? More collaboration, transparency, and sharing. The vibrant GPU Poor ecosystem, where needs lead to novel research in asynchronous Discord servers and pushing the boundaries of libraries and hardware alike. The GPU Rich sharing research that can only be done at a huge scale and open-sourcing some of their models with licenses that will foster adoption and community. The bridging GPU Middle Class in direct touch with the Poor, understanding the masses‚Äô needs and training high-quality models under intense constraints.
</p>
<p>
The GPU Poor strike back! Vive la r√©volution Open Source!
</p>
<div class="captioned-image-container">
<figure class="figure">
<a class="image-link is-viewable-img image2" target="_blank" href="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F485ddbc2-77ed-4c08-8dd5-4d7e66f67854_512x512.png" data-component-name="Image2ToDOM">
<div class="image2-inset">
<picture><source type="image/webp" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F485ddbc2-77ed-4c08-8dd5-4d7e66f67854_512x512.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F485ddbc2-77ed-4c08-8dd5-4d7e66f67854_512x512.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F485ddbc2-77ed-4c08-8dd5-4d7e66f67854_512x512.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F485ddbc2-77ed-4c08-8dd5-4d7e66f67854_512x512.png 1456w" sizes="100vw"><img src="https://substack-post-media.s3.amazonaws.com/public/images/485ddbc2-77ed-4c08-8dd5-4d7e66f67854_512x512.png" width="512" height="512" data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/485ddbc2-77ed-4c08-8dd5-4d7e66f67854_512x512.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:512,&quot;width&quot;:512,&quot;resizeWidth&quot;:512,&quot;bytes&quot;:340831,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null}" class="sizing-normal figure-img" alt="" srcset="https://substackcdn.com/image/fetch/w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F485ddbc2-77ed-4c08-8dd5-4d7e66f67854_512x512.png 424w, https://substackcdn.com/image/fetch/w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F485ddbc2-77ed-4c08-8dd5-4d7e66f67854_512x512.png 848w, https://substackcdn.com/image/fetch/w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F485ddbc2-77ed-4c08-8dd5-4d7e66f67854_512x512.png 1272w, https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F485ddbc2-77ed-4c08-8dd5-4d7e66f67854_512x512.png 1456w" sizes="100vw" loading="lazy"></picture>
<div class="image-link-expand">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewbox="0 0 24 24" fill="none" stroke="#FFFFFF" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-maximize2 ">
<polyline points="15 3 21 3 21 9"></polyline><polyline points="9 21 3 21 3 15"></polyline><line x1="21" x2="14" y1="3" y2="10"></line><line x1="3" x2="10" y1="21" y2="14"></line>
</svg>
</div>
</div>
</a>
</figure>
</div>
<p>
Image from Harrison Kinsley (<a href="https://twitter.com/Sentdex/status/1735436902759629250">Sentdex</a>)
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>
<p>
</p>



 ]]></description>
  <guid>https://osanseviero.github.io/hackerllama/blog/posts/gpu-poor-strike-back/</guid>
  <pubDate>Fri, 15 Dec 2023 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
